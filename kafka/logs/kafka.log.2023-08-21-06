[2023-08-21 06:43:54,369] INFO [ProducerStateManager partition=telemetry-0] Wrote producer snapshot at offset 72326526 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager)
[2023-08-21 06:43:54,369] INFO [MergedLog partition=telemetry-0, dir=/etc/confluent/kraft-combined-logs] Rolled new log segment at offset 72326526 in 1 ms. (kafka.log.MergedLog)
[2023-08-21 06:47:24,649] INFO [MergedLog partition=telemetry-0, dir=/etc/confluent/kraft-combined-logs] Deleting segment LogSegment(baseOffset=48920560, size=79128804, lastModifiedTime=1692341004369, largestRecordTimestamp=Some(1692341004374)) due to retention time 259200000ms breach based on the largest record timestamp in the segment (kafka.log.MergedLog)
[2023-08-21 06:47:24,650] INFO [MergedLog partition=telemetry-0, dir=/etc/confluent/kraft-combined-logs] Incrementing merged log start offset to 50149492 due to segment deletion (kafka.log.MergedLog)
[2023-08-21 06:48:24,650] INFO [LocalLog partition=telemetry-0, dir=/etc/confluent/kraft-combined-logs] Deleting segment files LogSegment(baseOffset=48920560, size=79128804, lastModifiedTime=1692341004369, largestRecordTimestamp=Some(1692341004374)) (kafka.log.LocalLog$)
[2023-08-21 06:48:24,650] INFO Deleted producer state snapshot /etc/confluent/kraft-combined-logs/telemetry-0/00000000000048920560.snapshot.deleted (kafka.log.SnapshotFile)
[2023-08-21 06:48:24,663] INFO Deleted log /etc/confluent/kraft-combined-logs/telemetry-0/00000000000048920560.log.deleted. (kafka.log.LogSegment)
[2023-08-21 06:48:24,663] INFO Deleted offset index /etc/confluent/kraft-combined-logs/telemetry-0/00000000000048920560.index.deleted. (kafka.log.LogSegment)
[2023-08-21 06:48:24,663] INFO Deleted time index /etc/confluent/kraft-combined-logs/telemetry-0/00000000000048920560.timeindex.deleted. (kafka.log.LogSegment)
[2023-08-21 06:48:45,002] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2023-08-21 06:48:46,342] INFO KafkaConfig values: 
	advertised.listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = JeTT4OO7RcqJ_Sdb0Fjd5w
	client.quota.callback.class = null
	compression.type = producer
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.demotion.support.enabled = false
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = true
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = ANY_UNEVEN_LOAD
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = false
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.resource.utilization.detector.enabled = false
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.partition.maximum.movements = 5
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.suspension.ms = 10800000
	confluent.balancer.topic.replication.factor = 1
	confluent.balancer.triggering.goals = []
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.kraft.mitigation.enabled = false
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.max.topics.per.snapshot = 5000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud
	confluent.cdc.api.keys.topic = 
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 1
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.metadata.load.threads = 32
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.throttle.ms = null
	confluent.metadata.active.encryptor = null
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-01:19092
	confluent.min.acks = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.prefer.tier.fetch.ms = -1
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.dynamic.reporting.min.usage = 102400
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = false
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = http://schema-registry:8081
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.telemetry.enabled = true
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.producer.enable.idempotence = false
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.partition.default.placement = 2
	confluent.topic.replica.assignor.builder.class = 
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id = 
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.valid.broker.rack.set = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-01:29092]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = INTERNAL
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
	listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092,CONTROLLER://kafka-01:29092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dirs = /etc/confluent/kraft-combined-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 2000000
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter, io.confluent.telemetry.reporter.TelemetryReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = [hidden]
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.id.quota.window.num = 11
	producer.id.quota.window.size.seconds = 1
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 2097152
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.acl.change.notification.expiration.ms = 900000
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2023-08-21 06:48:46,372] INFO KafkaConfig values: 
	advertised.listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = JeTT4OO7RcqJ_Sdb0Fjd5w
	client.quota.callback.class = null
	compression.type = producer
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.demotion.support.enabled = false
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = true
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = ANY_UNEVEN_LOAD
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = false
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.resource.utilization.detector.enabled = false
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.partition.maximum.movements = 5
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.suspension.ms = 10800000
	confluent.balancer.topic.replication.factor = 1
	confluent.balancer.triggering.goals = []
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.kraft.mitigation.enabled = false
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.max.topics.per.snapshot = 5000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud
	confluent.cdc.api.keys.topic = 
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 1
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.metadata.load.threads = 32
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.throttle.ms = null
	confluent.metadata.active.encryptor = null
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-01:19092
	confluent.min.acks = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.prefer.tier.fetch.ms = -1
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.dynamic.reporting.min.usage = 102400
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = false
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = http://schema-registry:8081
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.telemetry.enabled = true
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.producer.enable.idempotence = false
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.partition.default.placement = 2
	confluent.topic.replica.assignor.builder.class = 
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id = 
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.valid.broker.rack.set = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-01:29092]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = INTERNAL
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
	listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092,CONTROLLER://kafka-01:29092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dirs = /etc/confluent/kraft-combined-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 2000000
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter, io.confluent.telemetry.reporter.TelemetryReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = [hidden]
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.id.quota.window.num = 11
	producer.id.quota.window.size.seconds = 1
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 2097152
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.acl.change.notification.expiration.ms = 900000
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2023-08-21 06:48:46,382] INFO Starting controller (kafka.server.ControllerServer)
[2023-08-21 06:48:46,700] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas)
[2023-08-21 06:48:46,710] INFO Awaiting socket connections on kafka-01:29092. (kafka.network.DataPlaneAcceptor)
[2023-08-21 06:48:46,758] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)
[2023-08-21 06:48:46,770] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer)
[2023-08-21 06:48:46,923] INFO [MergedLog partition=__cluster_metadata-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:46,923] INFO [MergedLog partition=__cluster_metadata-0, dir=/etc/confluent/kraft-combined-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.MergedLog$)
[2023-08-21 06:48:46,924] INFO [MergedLog partition=__cluster_metadata-0, dir=/etc/confluent/kraft-combined-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.MergedLog$)
[2023-08-21 06:48:46,941] INFO Initialized snapshots with IDs SortedSet() from /etc/confluent/kraft-combined-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2023-08-21 06:48:46,957] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2023-08-21 06:48:47,046] INFO [kafka-raft-outbound-request-thread]: Starting (kafka.raft.RaftSendThread)
[2023-08-21 06:48:47,047] INFO [kafka-raft-io-thread]: Starting (kafka.raft.KafkaRaftManager$RaftIoThread)
[2023-08-21 06:48:47,164] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-21 06:48:47,166] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2023-08-21 06:48:47,167] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-21 06:48:47,178] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2023-08-21 06:48:47,178] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-21 06:48:47,180] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2023-08-21 06:48:47,180] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-21 06:48:47,200] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-21 06:48:47,209] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
[2023-08-21 06:48:47,213] INFO [BrokerServer id=1] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2023-08-21 06:48:47,213] INFO [BrokerServer id=1] Starting broker (kafka.server.BrokerServer)
[2023-08-21 06:48:47,229] INFO [BrokerServer id=1] FIPS mode enabled: false (kafka.server.BrokerServer)
[2023-08-21 06:48:47,240] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2023-08-21 06:48:47,240] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-21 06:48:47,241] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-21 06:48:47,242] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2023-08-21 06:48:47,242] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2023-08-21 06:48:47,242] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-21 06:48:47,242] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-21 06:48:47,279] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread)
[2023-08-21 06:48:47,280] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Recorded new controller, from now on will use node kafka-01:29092 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2023-08-21 06:48:47,320] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas)
[2023-08-21 06:48:47,320] INFO Awaiting socket connections on kafka-01:19092. (kafka.network.DataPlaneAcceptor)
[2023-08-21 06:48:47,328] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(INTERNAL) (kafka.network.SocketServer)
[2023-08-21 06:48:47,328] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas)
[2023-08-21 06:48:47,329] INFO Awaiting socket connections on kafka-01:9092. (kafka.network.DataPlaneAcceptor)
[2023-08-21 06:48:47,334] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2023-08-21 06:48:47,342] INFO [BrokerToControllerChannelManager broker=1 name=alterPartition]: Starting (kafka.server.BrokerToControllerRequestThread)
[2023-08-21 06:48:47,343] INFO [BrokerToControllerChannelManager broker=1 name=alterPartition]: Recorded new controller, from now on will use node kafka-01:29092 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2023-08-21 06:48:47,364] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-21 06:48:47,365] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-21 06:48:47,366] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-21 06:48:47,367] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-21 06:48:47,367] INFO [ExpirationReaper-1-ListOffsets]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-21 06:48:47,389] INFO [BrokerHealthManager]: Starting (kafka.availability.BrokerHealthManager)
[2023-08-21 06:48:47,395] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-21 06:48:47,395] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-21 06:48:49,192] INFO [BrokerToControllerChannelManager broker=1 name=heartbeat]: Starting (kafka.server.BrokerToControllerRequestThread)
[2023-08-21 06:48:49,192] INFO [BrokerToControllerChannelManager broker=1 name=heartbeat]: Recorded new controller, from now on will use node kafka-01:29092 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2023-08-21 06:48:49,196] INFO [BrokerLifecycleManager id=1] Incarnation H8iFrt3UTdGkbu_AIRMjwQ of broker 1 in cluster MkU3OEVBNTcwNTJENDM2Qg is now STARTING. (kafka.server.BrokerLifecycleManager)
[2023-08-21 06:48:49,250] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-21 06:48:49,257] INFO [BrokerServer id=1] Waiting for broker metadata to catch up. (kafka.server.BrokerServer)
[2023-08-21 06:48:49,289] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 6 (kafka.server.BrokerLifecycleManager)
[2023-08-21 06:48:49,296] INFO [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2023-08-21 06:48:49,301] INFO [BrokerMetadataListener id=1] Starting to publish metadata events at offset 6. (kafka.server.metadata.BrokerMetadataListener)
[2023-08-21 06:48:49,306] INFO [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=6, epoch=1) with metadata.version 3.4-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2023-08-21 06:48:49,308] INFO Loading logs from log dirs ArraySeq(/etc/confluent/kraft-combined-logs) (kafka.log.LogManager)
[2023-08-21 06:48:49,308] INFO [BrokerLifecycleManager id=1] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2023-08-21 06:48:49,310] INFO Attempting recovery for all logs in /etc/confluent/kraft-combined-logs since no clean shutdown file was found (kafka.log.LogManager)
[2023-08-21 06:48:49,315] INFO Loaded 0 logs in 8ms. (kafka.log.LogManager)
[2023-08-21 06:48:49,316] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2023-08-21 06:48:49,317] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2023-08-21 06:48:49,335] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2023-08-21 06:48:49,335] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2023-08-21 06:48:49,427] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner)
[2023-08-21 06:48:49,427] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner)
[2023-08-21 06:48:49,432] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2023-08-21 06:48:49,433] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:49,435] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:49,435] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:48:49,439] INFO [Transaction Marker Channel Manager 1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2023-08-21 06:48:49,439] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:48:49,442] INFO [BrokerMetadataPublisher id=1] Updating metadata.version to 3.4-IV0 at offset OffsetAndEpoch(offset=6, epoch=1). (kafka.server.metadata.BrokerMetadataPublisher)
[2023-08-21 06:48:49,458] INFO KafkaConfig values: 
	advertised.listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = JeTT4OO7RcqJ_Sdb0Fjd5w
	client.quota.callback.class = null
	compression.type = producer
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.demotion.support.enabled = false
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = true
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = ANY_UNEVEN_LOAD
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = false
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.resource.utilization.detector.enabled = false
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.partition.maximum.movements = 5
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.suspension.ms = 10800000
	confluent.balancer.topic.replication.factor = 1
	confluent.balancer.triggering.goals = []
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.kraft.mitigation.enabled = false
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.max.topics.per.snapshot = 5000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud
	confluent.cdc.api.keys.topic = 
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 1
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.metadata.load.threads = 32
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.throttle.ms = null
	confluent.metadata.active.encryptor = null
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-01:19092
	confluent.min.acks = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.prefer.tier.fetch.ms = -1
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.dynamic.reporting.min.usage = 102400
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = false
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = http://schema-registry:8081
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.telemetry.enabled = true
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.producer.enable.idempotence = false
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.partition.default.placement = 2
	confluent.topic.replica.assignor.builder.class = 
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id = 
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.valid.broker.rack.set = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-01:29092]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = INTERNAL
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
	listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092,CONTROLLER://kafka-01:29092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dirs = /etc/confluent/kraft-combined-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 2000000
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter, io.confluent.telemetry.reporter.TelemetryReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = [hidden]
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.id.quota.window.num = 11
	producer.id.quota.window.size.seconds = 1
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 2097152
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.acl.change.notification.expiration.ms = 900000
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2023-08-21 06:48:49,467] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
[2023-08-21 06:48:49,515] INFO Sent auto-creation request for Set(connect-offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2023-08-21 06:48:49,515] INFO Sent auto-creation request for Set(_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2023-08-21 06:48:49,515] INFO Sent auto-creation request for Set(_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2023-08-21 06:48:49,515] INFO Sent auto-creation request for Set(connect-configs) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2023-08-21 06:48:49,520] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2023-08-21 06:48:49,522] INFO Sent auto-creation request for Set(_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-monitoring) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2023-08-21 06:48:49,536] INFO Sent auto-creation request for Set(connect-statuses) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2023-08-21 06:48:49,550] INFO Sent auto-creation request for Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2023-08-21 06:48:49,552] INFO Sent auto-creation request for Set(_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2023-08-21 06:48:49,564] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2023-08-21 06:48:49,591] INFO Sent auto-creation request for Set(_confluent-command) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2023-08-21 06:48:49,662] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(connect-offsets-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:48:49,725] INFO [MergedLog partition=connect-offsets-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:49,732] INFO Created log for partition connect-offsets-0 in /etc/confluent/kraft-combined-logs/connect-offsets-0 with properties {} (kafka.log.LogManager)
[2023-08-21 06:48:49,742] INFO [Partition connect-offsets-0 broker=1] No checkpointed highwatermark is found for partition connect-offsets-0 (kafka.cluster.Partition)
[2023-08-21 06:48:49,744] INFO [Partition connect-offsets-0 broker=1] Log loaded for partition connect-offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:49,750] INFO Setting topicIdPartition oovNB-EEQ_KEcQHVU1UyDg:connect-offsets-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:49,753] INFO [MergedLog partition=connect-offsets-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:49,757] INFO Sent auto-creation request for Set(_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2023-08-21 06:48:49,783] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:48:49,813] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:49,816] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 with properties {} (kafka.log.LogManager)
[2023-08-21 06:48:49,816] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:48:49,816] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:49,816] INFO Setting topicIdPartition gYfsXCkRRkWtSDzg1UKBNA:_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:49,817] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:49,820] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(connect-configs-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:48:49,824] INFO KafkaConfig values: 
	advertised.listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = JeTT4OO7RcqJ_Sdb0Fjd5w
	client.quota.callback.class = null
	compression.type = producer
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.demotion.support.enabled = false
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = true
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = ANY_UNEVEN_LOAD
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = false
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.resource.utilization.detector.enabled = false
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.partition.maximum.movements = 5
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.suspension.ms = 10800000
	confluent.balancer.topic.replication.factor = 1
	confluent.balancer.triggering.goals = []
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.kraft.mitigation.enabled = false
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.max.topics.per.snapshot = 5000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud
	confluent.cdc.api.keys.topic = 
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 1
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.metadata.load.threads = 32
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.throttle.ms = null
	confluent.metadata.active.encryptor = null
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-01:19092
	confluent.min.acks = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.prefer.tier.fetch.ms = -1
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.dynamic.reporting.min.usage = 102400
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = false
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = http://schema-registry:8081
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.telemetry.enabled = true
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.producer.enable.idempotence = false
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.partition.default.placement = 2
	confluent.topic.replica.assignor.builder.class = 
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id = 
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.valid.broker.rack.set = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-01:29092]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = INTERNAL
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
	listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092,CONTROLLER://kafka-01:29092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dirs = /etc/confluent/kraft-combined-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 2000000
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter, io.confluent.telemetry.reporter.TelemetryReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = [hidden]
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.id.quota.window.num = 11
	producer.id.quota.window.size.seconds = 1
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 2097152
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.acl.change.notification.expiration.ms = 900000
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2023-08-21 06:48:49,829] INFO [MergedLog partition=connect-configs-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:49,830] INFO Created log for partition connect-configs-0 in /etc/confluent/kraft-combined-logs/connect-configs-0 with properties {} (kafka.log.LogManager)
[2023-08-21 06:48:49,830] INFO [Partition connect-configs-0 broker=1] No checkpointed highwatermark is found for partition connect-configs-0 (kafka.cluster.Partition)
[2023-08-21 06:48:49,830] INFO [Partition connect-configs-0 broker=1] Log loaded for partition connect-configs-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:49,831] INFO Setting topicIdPartition fMFuzj_-TXqT0AHP7ZB0_Q:connect-configs-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:49,832] INFO [MergedLog partition=connect-configs-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-configs-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:49,833] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:48:49,843] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:49,844] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 with properties {} (kafka.log.LogManager)
[2023-08-21 06:48:49,845] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:48:49,848] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:49,848] INFO Setting topicIdPartition lBXW3tleTxyFrz96KEOUQg:_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:49,848] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:49,852] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:49,855] INFO Created log for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 with properties {} (kafka.log.LogManager)
[2023-08-21 06:48:49,855] INFO [Partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 (kafka.cluster.Partition)
[2023-08-21 06:48:49,856] INFO [Partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:49,856] INFO Setting topicIdPartition RNfvmSxiShCJGpSQi6gkYg:_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:49,856] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:49,860] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, _confluent-monitoring-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:48:49,876] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:49,878] INFO Created log for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 with properties {} (kafka.log.LogManager)
[2023-08-21 06:48:49,878] INFO [Partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (kafka.cluster.Partition)
[2023-08-21 06:48:49,878] INFO [Partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:49,878] INFO Setting topicIdPartition Ok2rddlNRGqSneH3ZnCx1w:_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:49,878] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:49,884] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:49,887] INFO Created log for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 with properties {} (kafka.log.LogManager)
[2023-08-21 06:48:49,889] INFO [Partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (kafka.cluster.Partition)
[2023-08-21 06:48:49,889] INFO [Partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:49,889] INFO Setting topicIdPartition mH8Xb10-ReySeBBNAv71lA:_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:49,889] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:49,893] INFO [MergedLog partition=_confluent-monitoring-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:49,896] INFO Created log for partition _confluent-monitoring-0 in /etc/confluent/kraft-combined-logs/_confluent-monitoring-0 with properties {} (kafka.log.LogManager)
[2023-08-21 06:48:49,896] INFO [Partition _confluent-monitoring-0 broker=1] No checkpointed highwatermark is found for partition _confluent-monitoring-0 (kafka.cluster.Partition)
[2023-08-21 06:48:49,896] INFO [Partition _confluent-monitoring-0 broker=1] Log loaded for partition _confluent-monitoring-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:49,896] INFO Setting topicIdPartition V9sy6ULWRp6ASrTCGa2Y5Q:_confluent-monitoring-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:49,897] INFO [MergedLog partition=_confluent-monitoring-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-monitoring-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:49,898] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(connect-statuses-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:48:49,905] INFO [MergedLog partition=connect-statuses-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:49,910] INFO Created log for partition connect-statuses-0 in /etc/confluent/kraft-combined-logs/connect-statuses-0 with properties {} (kafka.log.LogManager)
[2023-08-21 06:48:49,910] INFO [Partition connect-statuses-0 broker=1] No checkpointed highwatermark is found for partition connect-statuses-0 (kafka.cluster.Partition)
[2023-08-21 06:48:49,911] INFO [Partition connect-statuses-0 broker=1] Log loaded for partition connect-statuses-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:49,911] INFO Setting topicIdPartition FqIkCTyHRTqgMckhtbav_w:connect-statuses-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:49,912] INFO [MergedLog partition=connect-statuses-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-statuses-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:49,918] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:48:49,933] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:49,934] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with properties {} (kafka.log.LogManager)
[2023-08-21 06:48:49,935] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:48:49,935] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:49,935] INFO Setting topicIdPartition kKIDUXKRQIOOrfFUm3O6sQ:_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:49,936] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:49,939] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:49,940] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with properties {} (kafka.log.LogManager)
[2023-08-21 06:48:49,941] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:48:49,941] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:49,942] INFO Setting topicIdPartition K4aIj9bcTYewa-yjPZxzAg:_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:49,942] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:49,946] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:49,947] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 with properties {} (kafka.log.LogManager)
[2023-08-21 06:48:49,948] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 (kafka.cluster.Partition)
[2023-08-21 06:48:49,948] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:49,948] INFO Setting topicIdPartition cKlt-ffoRgW3RsBOnF1CMQ:_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:49,949] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:49,955] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:48:49,962] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:49,964] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 with properties {} (kafka.log.LogManager)
[2023-08-21 06:48:49,964] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 (kafka.cluster.Partition)
[2023-08-21 06:48:49,964] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:49,968] INFO Setting topicIdPartition GF5j6eugQpatQnSUNx1pkw:_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:49,968] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,011] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:48:50,218] INFO [MergedLog partition=__consumer_offsets-13, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,221] INFO Created log for partition __consumer_offsets-13 in /etc/confluent/kraft-combined-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,221] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2023-08-21 06:48:50,221] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,221] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-13 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,223] INFO [MergedLog partition=__consumer_offsets-13, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-13 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,227] INFO [MergedLog partition=__consumer_offsets-46, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,228] INFO Created log for partition __consumer_offsets-46 in /etc/confluent/kraft-combined-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,229] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2023-08-21 06:48:50,230] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,230] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-46 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,230] INFO [MergedLog partition=__consumer_offsets-46, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-46 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,237] INFO [MergedLog partition=__consumer_offsets-9, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,238] INFO Created log for partition __consumer_offsets-9 in /etc/confluent/kraft-combined-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,238] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2023-08-21 06:48:50,238] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,239] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-9 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,239] INFO [MergedLog partition=__consumer_offsets-9, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,244] INFO [MergedLog partition=__consumer_offsets-42, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,247] INFO Created log for partition __consumer_offsets-42 in /etc/confluent/kraft-combined-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,247] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2023-08-21 06:48:50,247] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,247] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-42 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,247] INFO [MergedLog partition=__consumer_offsets-42, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-42 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,257] INFO [MergedLog partition=__consumer_offsets-21, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,259] INFO Created log for partition __consumer_offsets-21 in /etc/confluent/kraft-combined-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,259] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2023-08-21 06:48:50,259] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,260] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-21 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,260] INFO [MergedLog partition=__consumer_offsets-21, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-21 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,263] INFO [MergedLog partition=__consumer_offsets-17, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,264] INFO Created log for partition __consumer_offsets-17 in /etc/confluent/kraft-combined-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,265] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2023-08-21 06:48:50,265] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,267] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-17 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,267] INFO [MergedLog partition=__consumer_offsets-17, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-17 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,278] INFO [MergedLog partition=__consumer_offsets-30, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,279] INFO Created log for partition __consumer_offsets-30 in /etc/confluent/kraft-combined-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,280] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2023-08-21 06:48:50,280] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,280] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-30 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,280] INFO [MergedLog partition=__consumer_offsets-30, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-30 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,285] INFO [MergedLog partition=__consumer_offsets-26, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,286] INFO Created log for partition __consumer_offsets-26 in /etc/confluent/kraft-combined-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,287] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2023-08-21 06:48:50,287] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,287] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-26 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,288] INFO [MergedLog partition=__consumer_offsets-26, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-26 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,291] INFO [MergedLog partition=__consumer_offsets-5, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,292] INFO Created log for partition __consumer_offsets-5 in /etc/confluent/kraft-combined-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,294] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2023-08-21 06:48:50,294] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,294] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-5 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,294] INFO [MergedLog partition=__consumer_offsets-5, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,298] INFO [MergedLog partition=__consumer_offsets-38, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,299] INFO Created log for partition __consumer_offsets-38 in /etc/confluent/kraft-combined-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,300] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2023-08-21 06:48:50,300] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,300] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-38 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,301] INFO [MergedLog partition=__consumer_offsets-38, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-38 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,305] INFO [MergedLog partition=__consumer_offsets-1, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,309] INFO Created log for partition __consumer_offsets-1 in /etc/confluent/kraft-combined-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,310] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2023-08-21 06:48:50,310] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,311] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-1 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,312] INFO [MergedLog partition=__consumer_offsets-1, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,341] INFO Connection id 172.20.0.2:19092-172.20.0.8:60338-2 disconnected while trying to send response for request 1700678 (kafka.network.Processor)
[2023-08-21 06:48:50,343] INFO [MergedLog partition=__consumer_offsets-34, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,345] INFO Created log for partition __consumer_offsets-34 in /etc/confluent/kraft-combined-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,346] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2023-08-21 06:48:50,347] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,348] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-34 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,348] INFO [MergedLog partition=__consumer_offsets-34, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-34 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,352] INFO [MergedLog partition=__consumer_offsets-16, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,353] INFO Created log for partition __consumer_offsets-16 in /etc/confluent/kraft-combined-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,353] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2023-08-21 06:48:50,354] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,354] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-16 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,354] INFO [MergedLog partition=__consumer_offsets-16, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-16 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,357] INFO [MergedLog partition=__consumer_offsets-45, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,357] INFO Created log for partition __consumer_offsets-45 in /etc/confluent/kraft-combined-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,357] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2023-08-21 06:48:50,358] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,359] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-45 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,359] INFO [MergedLog partition=__consumer_offsets-45, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-45 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,362] INFO [MergedLog partition=__consumer_offsets-12, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,363] INFO Created log for partition __consumer_offsets-12 in /etc/confluent/kraft-combined-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,363] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2023-08-21 06:48:50,363] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,364] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-12 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,364] INFO [MergedLog partition=__consumer_offsets-12, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-12 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,366] INFO [MergedLog partition=__consumer_offsets-41, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,367] INFO Created log for partition __consumer_offsets-41 in /etc/confluent/kraft-combined-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,367] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2023-08-21 06:48:50,368] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,368] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-41 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,368] INFO [MergedLog partition=__consumer_offsets-41, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-41 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,371] INFO [MergedLog partition=__consumer_offsets-24, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,372] INFO Created log for partition __consumer_offsets-24 in /etc/confluent/kraft-combined-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,372] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2023-08-21 06:48:50,372] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,373] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-24 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,373] INFO [MergedLog partition=__consumer_offsets-24, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-24 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,376] INFO [MergedLog partition=__consumer_offsets-20, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,377] INFO Created log for partition __consumer_offsets-20 in /etc/confluent/kraft-combined-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,378] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2023-08-21 06:48:50,378] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,378] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-20 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,378] INFO [MergedLog partition=__consumer_offsets-20, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-20 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,382] INFO [MergedLog partition=__consumer_offsets-49, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,383] INFO Created log for partition __consumer_offsets-49 in /etc/confluent/kraft-combined-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,384] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2023-08-21 06:48:50,384] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,384] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-49 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,385] INFO [MergedLog partition=__consumer_offsets-49, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-49 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,388] INFO [MergedLog partition=__consumer_offsets-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,390] INFO Created log for partition __consumer_offsets-0 in /etc/confluent/kraft-combined-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,390] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,391] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,391] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,391] INFO [MergedLog partition=__consumer_offsets-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,394] INFO [MergedLog partition=__consumer_offsets-29, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,395] INFO Created log for partition __consumer_offsets-29 in /etc/confluent/kraft-combined-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,396] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2023-08-21 06:48:50,396] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,396] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-29 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,396] INFO [MergedLog partition=__consumer_offsets-29, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-29 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,399] INFO [MergedLog partition=__consumer_offsets-25, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,400] INFO Created log for partition __consumer_offsets-25 in /etc/confluent/kraft-combined-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,400] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2023-08-21 06:48:50,400] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,400] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-25 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,400] INFO [MergedLog partition=__consumer_offsets-25, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-25 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,404] INFO [MergedLog partition=__consumer_offsets-8, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,405] INFO Created log for partition __consumer_offsets-8 in /etc/confluent/kraft-combined-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,405] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2023-08-21 06:48:50,408] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,408] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-8 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,408] INFO [MergedLog partition=__consumer_offsets-8, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,411] INFO [MergedLog partition=__consumer_offsets-37, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,412] INFO Created log for partition __consumer_offsets-37 in /etc/confluent/kraft-combined-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,412] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2023-08-21 06:48:50,413] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,413] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-37 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,413] INFO [MergedLog partition=__consumer_offsets-37, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-37 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,415] INFO [MergedLog partition=__consumer_offsets-4, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,416] INFO Created log for partition __consumer_offsets-4 in /etc/confluent/kraft-combined-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,416] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2023-08-21 06:48:50,417] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,418] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-4 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,418] INFO [MergedLog partition=__consumer_offsets-4, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,421] INFO Connection id 172.20.0.2:19092-172.20.0.8:60390-11 disconnected while trying to send response for request 1701005 (kafka.network.Processor)
[2023-08-21 06:48:50,423] INFO [MergedLog partition=__consumer_offsets-33, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,424] INFO Created log for partition __consumer_offsets-33 in /etc/confluent/kraft-combined-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,428] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2023-08-21 06:48:50,428] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,428] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-33 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,428] INFO [MergedLog partition=__consumer_offsets-33, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-33 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,432] INFO [MergedLog partition=__consumer_offsets-15, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,432] INFO Created log for partition __consumer_offsets-15 in /etc/confluent/kraft-combined-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,434] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2023-08-21 06:48:50,434] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,434] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-15 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,434] INFO [MergedLog partition=__consumer_offsets-15, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-15 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,437] INFO [MergedLog partition=__consumer_offsets-48, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,438] INFO Created log for partition __consumer_offsets-48 in /etc/confluent/kraft-combined-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,438] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2023-08-21 06:48:50,438] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,439] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-48 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,439] INFO [MergedLog partition=__consumer_offsets-48, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-48 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,442] INFO [MergedLog partition=__consumer_offsets-11, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,442] INFO Created log for partition __consumer_offsets-11 in /etc/confluent/kraft-combined-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,443] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2023-08-21 06:48:50,443] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,444] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-11 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,444] INFO [MergedLog partition=__consumer_offsets-11, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,447] INFO [MergedLog partition=__consumer_offsets-44, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,448] INFO Created log for partition __consumer_offsets-44 in /etc/confluent/kraft-combined-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,449] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2023-08-21 06:48:50,449] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,449] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-44 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,449] INFO [MergedLog partition=__consumer_offsets-44, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-44 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,452] INFO [MergedLog partition=__consumer_offsets-23, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,453] INFO Created log for partition __consumer_offsets-23 in /etc/confluent/kraft-combined-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,454] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2023-08-21 06:48:50,454] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,454] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-23 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,455] INFO [MergedLog partition=__consumer_offsets-23, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-23 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,458] INFO [MergedLog partition=__consumer_offsets-19, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,459] INFO Created log for partition __consumer_offsets-19 in /etc/confluent/kraft-combined-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,460] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2023-08-21 06:48:50,460] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,460] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-19 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,460] INFO [MergedLog partition=__consumer_offsets-19, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-19 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,463] INFO [MergedLog partition=__consumer_offsets-32, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,464] INFO Created log for partition __consumer_offsets-32 in /etc/confluent/kraft-combined-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,465] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2023-08-21 06:48:50,465] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,466] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-32 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,466] INFO [MergedLog partition=__consumer_offsets-32, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-32 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,469] INFO [MergedLog partition=__consumer_offsets-28, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,470] INFO Created log for partition __consumer_offsets-28 in /etc/confluent/kraft-combined-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,470] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2023-08-21 06:48:50,470] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,470] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-28 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,471] INFO [MergedLog partition=__consumer_offsets-28, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-28 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,473] INFO [MergedLog partition=__consumer_offsets-7, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,474] INFO Created log for partition __consumer_offsets-7 in /etc/confluent/kraft-combined-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,474] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2023-08-21 06:48:50,474] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,475] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-7 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,475] INFO [MergedLog partition=__consumer_offsets-7, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,477] INFO [MergedLog partition=__consumer_offsets-40, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,478] INFO Created log for partition __consumer_offsets-40 in /etc/confluent/kraft-combined-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,481] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2023-08-21 06:48:50,481] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,481] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-40 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,481] INFO [MergedLog partition=__consumer_offsets-40, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-40 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,484] INFO [MergedLog partition=__consumer_offsets-3, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,485] INFO Created log for partition __consumer_offsets-3 in /etc/confluent/kraft-combined-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,486] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2023-08-21 06:48:50,487] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,489] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-3 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,489] INFO [MergedLog partition=__consumer_offsets-3, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,492] INFO [MergedLog partition=__consumer_offsets-36, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,493] INFO Created log for partition __consumer_offsets-36 in /etc/confluent/kraft-combined-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,494] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2023-08-21 06:48:50,494] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,494] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-36 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,494] INFO [MergedLog partition=__consumer_offsets-36, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-36 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,497] INFO [MergedLog partition=__consumer_offsets-47, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,498] INFO Created log for partition __consumer_offsets-47 in /etc/confluent/kraft-combined-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,498] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2023-08-21 06:48:50,499] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,499] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-47 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,500] INFO [MergedLog partition=__consumer_offsets-47, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-47 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,502] INFO [MergedLog partition=__consumer_offsets-14, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,503] INFO Created log for partition __consumer_offsets-14 in /etc/confluent/kraft-combined-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,504] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2023-08-21 06:48:50,504] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,504] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-14 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,504] INFO [MergedLog partition=__consumer_offsets-14, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-14 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,507] INFO [MergedLog partition=__consumer_offsets-43, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,508] INFO Created log for partition __consumer_offsets-43 in /etc/confluent/kraft-combined-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,508] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2023-08-21 06:48:50,508] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,508] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-43 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,509] INFO [MergedLog partition=__consumer_offsets-43, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-43 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,511] INFO [MergedLog partition=__consumer_offsets-10, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,512] INFO Created log for partition __consumer_offsets-10 in /etc/confluent/kraft-combined-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,512] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2023-08-21 06:48:50,514] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,514] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-10 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,514] INFO [MergedLog partition=__consumer_offsets-10, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,517] INFO [MergedLog partition=__consumer_offsets-22, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,518] INFO Created log for partition __consumer_offsets-22 in /etc/confluent/kraft-combined-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,518] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2023-08-21 06:48:50,519] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,520] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-22 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,520] INFO [MergedLog partition=__consumer_offsets-22, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-22 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,525] INFO [MergedLog partition=__consumer_offsets-18, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,526] INFO Created log for partition __consumer_offsets-18 in /etc/confluent/kraft-combined-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,526] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2023-08-21 06:48:50,526] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,526] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-18 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,526] INFO [MergedLog partition=__consumer_offsets-18, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-18 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,530] INFO [MergedLog partition=__consumer_offsets-31, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,531] INFO Created log for partition __consumer_offsets-31 in /etc/confluent/kraft-combined-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,532] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2023-08-21 06:48:50,532] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,532] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-31 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,532] INFO [MergedLog partition=__consumer_offsets-31, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-31 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,535] INFO [MergedLog partition=__consumer_offsets-27, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,536] INFO Created log for partition __consumer_offsets-27 in /etc/confluent/kraft-combined-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,537] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2023-08-21 06:48:50,538] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,538] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-27 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,538] INFO [MergedLog partition=__consumer_offsets-27, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-27 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,543] INFO [MergedLog partition=__consumer_offsets-39, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,544] INFO Created log for partition __consumer_offsets-39 in /etc/confluent/kraft-combined-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,544] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2023-08-21 06:48:50,544] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,545] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-39 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,545] INFO [MergedLog partition=__consumer_offsets-39, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-39 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,548] INFO [MergedLog partition=__consumer_offsets-6, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,549] INFO Created log for partition __consumer_offsets-6 in /etc/confluent/kraft-combined-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,550] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2023-08-21 06:48:50,550] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,550] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-6 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,550] INFO [MergedLog partition=__consumer_offsets-6, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,553] INFO [MergedLog partition=__consumer_offsets-35, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,555] INFO Created log for partition __consumer_offsets-35 in /etc/confluent/kraft-combined-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,555] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2023-08-21 06:48:50,555] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,555] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-35 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,555] INFO [MergedLog partition=__consumer_offsets-35, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-35 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,558] INFO [MergedLog partition=__consumer_offsets-2, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,559] INFO Created log for partition __consumer_offsets-2 in /etc/confluent/kraft-combined-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:48:50,559] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2023-08-21 06:48:50,559] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,559] INFO Setting topicIdPartition r8b5c8fJTymER3GPJ_Upjg:__consumer_offsets-2 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,559] INFO [MergedLog partition=__consumer_offsets-2, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:50,562] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,565] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,568] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,569] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,569] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,569] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,569] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,569] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,570] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,570] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,571] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,571] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,575] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,575] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,576] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,576] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,576] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,576] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,576] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,576] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,577] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,577] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,577] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,578] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,579] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,579] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,579] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,579] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,579] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,579] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,580] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,580] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,580] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,580] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,580] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,580] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,581] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,581] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,581] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,581] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,581] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,582] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,582] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,582] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,584] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,584] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,584] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,584] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,584] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,585] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,586] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,588] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,590] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,590] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,591] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,589] INFO The cleaning for partition __consumer_offsets-25 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,594] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,589] INFO The cleaning for partition __consumer_offsets-4 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,596] INFO The cleaning for partition __consumer_offsets-15 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,589] INFO The cleaning for partition __consumer_offsets-49 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,589] INFO The cleaning for partition __consumer_offsets-13 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,589] INFO The cleaning for partition __consumer_offsets-21 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,589] INFO The cleaning for partition __consumer_offsets-38 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,589] INFO The cleaning for partition __consumer_offsets-46 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,589] INFO The cleaning for partition __consumer_offsets-42 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,589] INFO The cleaning for partition __consumer_offsets-29 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,602] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,602] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,589] INFO The cleaning for partition __consumer_offsets-26 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,604] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 28 milliseconds for epoch 0, of which 28 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,589] INFO The cleaning for partition __consumer_offsets-30 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,589] INFO The cleaning for partition __consumer_offsets-17 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,605] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,589] INFO The cleaning for partition __consumer_offsets-9 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,605] INFO The cleaning for partition __consumer_offsets-11 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,589] INFO The cleaning for partition __consumer_offsets-5 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,589] INFO The cleaning for partition __consumer_offsets-0 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,589] INFO The cleaning for partition __consumer_offsets-1 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,589] INFO The cleaning for partition __consumer_offsets-34 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,605] INFO The cleaning for partition __consumer_offsets-48 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,589] INFO The cleaning for partition __consumer_offsets-24 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,605] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 10 milliseconds for epoch 0, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,605] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 25 milliseconds for epoch 0, of which 25 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,605] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 27 milliseconds for epoch 0, of which 27 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,606] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 29 milliseconds for epoch 0, of which 28 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,606] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 30 milliseconds for epoch 0, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,606] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,606] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 37 milliseconds for epoch 0, of which 37 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,606] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 25 milliseconds for epoch 0, of which 25 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,606] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,606] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 35 milliseconds for epoch 0, of which 35 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,606] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 31 milliseconds for epoch 0, of which 31 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,607] INFO Cleaning for partition __consumer_offsets-1 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,607] INFO Cleaning for partition __consumer_offsets-17 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,607] INFO Cleaning for partition __consumer_offsets-30 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,607] INFO Cleaning for partition __consumer_offsets-5 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,607] INFO Cleaning for partition __consumer_offsets-11 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,589] INFO The cleaning for partition __consumer_offsets-45 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,607] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 28 milliseconds for epoch 0, of which 28 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,607] INFO Cleaning for partition __consumer_offsets-45 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,589] INFO The cleaning for partition __consumer_offsets-12 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,608] INFO Cleaning for partition __consumer_offsets-0 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,608] INFO Cleaning for partition __consumer_offsets-34 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,608] INFO Cleaning for partition __consumer_offsets-24 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,608] INFO Cleaning for partition __consumer_offsets-48 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,608] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 29 milliseconds for epoch 0, of which 29 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,608] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,608] INFO Cleaning for partition __consumer_offsets-12 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,608] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,608] INFO The cleaning for partition __consumer_offsets-44 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,608] INFO Cleaning for partition __consumer_offsets-26 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,608] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,608] INFO Cleaning for partition __consumer_offsets-9 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,589] INFO The cleaning for partition __consumer_offsets-41 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,590] INFO The cleaning for partition __consumer_offsets-8 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,609] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,609] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 25 milliseconds for epoch 0, of which 25 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,609] INFO Cleaning for partition __consumer_offsets-8 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,609] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 29 milliseconds for epoch 0, of which 29 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,590] INFO The cleaning for partition __consumer_offsets-16 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,590] INFO The cleaning for partition __consumer_offsets-37 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,610] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 26 milliseconds for epoch 0, of which 26 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,610] INFO Cleaning for partition __consumer_offsets-37 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,590] INFO The cleaning for partition __consumer_offsets-20 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,610] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,610] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 30 milliseconds for epoch 0, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,591] INFO The cleaning for partition __consumer_offsets-33 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,601] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 35 milliseconds for epoch 0, of which 34 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,601] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 31 milliseconds for epoch 0, of which 31 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,601] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 19 milliseconds for epoch 0, of which 12 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,602] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 33 milliseconds for epoch 0, of which 32 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,601] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 25 milliseconds for epoch 0, of which 25 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,610] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 10 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,602] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 33 milliseconds for epoch 0, of which 32 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,602] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 20 milliseconds for epoch 0, of which 20 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,609] INFO Cleaning for partition __consumer_offsets-41 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,602] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 17 milliseconds for epoch 0, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,602] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 21 milliseconds for epoch 0, of which 18 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,611] INFO The cleaning for partition __consumer_offsets-23 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,611] INFO Cleaning for partition __consumer_offsets-44 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,613] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,614] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,614] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 35 milliseconds for epoch 0, of which 35 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,614] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,614] INFO Cleaning for partition __consumer_offsets-16 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,614] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,614] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,614] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,614] INFO The cleaning for partition __consumer_offsets-19 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,614] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,614] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,614] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,614] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,615] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,615] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,615] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,615] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,615] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,615] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,615] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,615] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,615] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,615] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,615] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,615] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,615] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,615] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,615] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,615] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,615] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,615] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,615] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,615] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,615] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,615] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,615] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,615] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,615] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,615] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:50,615] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,617] INFO Cleaning for partition __consumer_offsets-49 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,617] INFO Cleaning for partition __consumer_offsets-4 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,617] INFO Cleaning for partition __consumer_offsets-29 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,617] INFO Cleaning for partition __consumer_offsets-42 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,617] INFO Cleaning for partition __consumer_offsets-15 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,617] INFO Cleaning for partition __consumer_offsets-38 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,617] INFO Cleaning for partition __consumer_offsets-46 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,617] INFO Cleaning for partition __consumer_offsets-25 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,618] INFO Cleaning for partition __consumer_offsets-21 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,618] INFO Cleaning for partition __consumer_offsets-13 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,618] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 30 milliseconds for epoch 0, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,618] INFO Cleaning for partition __consumer_offsets-33 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,619] INFO The cleaning for partition __consumer_offsets-28 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,619] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 11 milliseconds for epoch 0, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,619] INFO The cleaning for partition __consumer_offsets-7 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,619] INFO The cleaning for partition __consumer_offsets-32 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,619] INFO Cleaning for partition __consumer_offsets-23 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,619] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,619] INFO Cleaning for partition __consumer_offsets-28 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,619] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,619] INFO The cleaning for partition __consumer_offsets-3 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,619] INFO Cleaning for partition __consumer_offsets-20 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,619] INFO The cleaning for partition __consumer_offsets-36 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,619] INFO The cleaning for partition __consumer_offsets-14 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,619] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,619] INFO Cleaning for partition __consumer_offsets-36 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,619] INFO The cleaning for partition __consumer_offsets-43 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,619] INFO The cleaning for partition __consumer_offsets-10 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,619] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,619] INFO Cleaning for partition __consumer_offsets-3 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,619] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,619] INFO Cleaning for partition __consumer_offsets-10 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,619] INFO The cleaning for partition __consumer_offsets-22 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,620] INFO The cleaning for partition __consumer_offsets-18 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,620] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 5 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,620] INFO The cleaning for partition __consumer_offsets-31 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,620] INFO Cleaning for partition __consumer_offsets-22 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,619] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,620] INFO The cleaning for partition __consumer_offsets-27 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,620] INFO Cleaning for partition __consumer_offsets-32 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,620] INFO The cleaning for partition __consumer_offsets-39 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,620] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,620] INFO The cleaning for partition __consumer_offsets-6 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,620] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,620] INFO Cleaning for partition __consumer_offsets-43 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,620] INFO The cleaning for partition __consumer_offsets-35 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,619] INFO The cleaning for partition __consumer_offsets-40 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,620] INFO The cleaning for partition __consumer_offsets-2 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,620] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,620] INFO Cleaning for partition __consumer_offsets-35 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,620] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,620] INFO Cleaning for partition __consumer_offsets-2 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,620] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,621] INFO Cleaning for partition __consumer_offsets-39 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,621] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,620] INFO Cleaning for partition __consumer_offsets-31 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,620] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,621] INFO Cleaning for partition __consumer_offsets-27 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,620] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,621] INFO Cleaning for partition __consumer_offsets-18 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,619] INFO Cleaning for partition __consumer_offsets-19 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,619] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,619] INFO Cleaning for partition __consumer_offsets-7 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,621] INFO Cleaning for partition __consumer_offsets-14 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,620] INFO The cleaning for partition __consumer_offsets-47 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:48:50,620] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,621] INFO Cleaning for partition __consumer_offsets-6 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,621] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:48:50,621] INFO Cleaning for partition __consumer_offsets-47 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,621] INFO Cleaning for partition __consumer_offsets-40 is resumed (kafka.log.LogManager)
[2023-08-21 06:48:50,624] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600,confluent.placement.constraints ->  (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:48:50,657] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-command-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:48:50,663] INFO [MergedLog partition=_confluent-command-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:50,664] INFO Created log for partition _confluent-command-0 in /etc/confluent/kraft-combined-logs/_confluent-command-0 with properties {} (kafka.log.LogManager)
[2023-08-21 06:48:50,667] INFO [Partition _confluent-command-0 broker=1] No checkpointed highwatermark is found for partition _confluent-command-0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,668] INFO [Partition _confluent-command-0 broker=1] Log loaded for partition _confluent-command-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:50,668] INFO Setting topicIdPartition JccOu-RDTM-4OTF9jbJpkw:_confluent-command-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:50,671] INFO [MergedLog partition=_confluent-command-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-command-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:53,418] INFO [BrokerServer id=1] Skipping durability audit instantiation (kafka.server.BrokerServer)
[2023-08-21 06:48:53,715] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:48:53,721] INFO [MergedLog partition=_schemas-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:48:53,721] INFO Created log for partition _schemas-0 in /etc/confluent/kraft-combined-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:48:53,723] INFO [Partition _schemas-0 broker=1] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2023-08-21 06:48:53,723] INFO [Partition _schemas-0 broker=1] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:48:53,723] INFO Setting topicIdPartition LtP6-uipSoW1_HtU6m9Nbg:_schemas-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:48:53,724] INFO [MergedLog partition=_schemas-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _schemas-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:48:53,724] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:48:54,183] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-7ddcd05d-1d4e-4959-a86a-e760a8d8bdf6 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:54,195] INFO [GroupCoordinator 1]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 0 (__consumer_offsets-29) (reason: Adding new member sr-1-7ddcd05d-1d4e-4959-a86a-e760a8d8bdf6 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:54,203] INFO [GroupCoordinator 1]: Stabilized group schema-registry generation 1 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:54,236] INFO [GroupCoordinator 1]: Assignment received from leader sr-1-7ddcd05d-1d4e-4959-a86a-e760a8d8bdf6 for group schema-registry for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:48:54,406] INFO Starting delay for broker load metric (kafka.metrics.BrokerLoad)
[2023-08-21 06:48:54,408] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2023-08-21 06:48:54,409] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer)
[2023-08-21 06:49:03,198] ERROR [KafkaApi-1] Unexpected error handling request RequestHeader(apiKey=INIT_PRODUCER_ID, apiVersion=4, clientId=confluent-control-center-heartbeat-sender-1, correlationId=2, headerVersion=2) -- InitProducerIdRequestData(transactionalId=null, transactionTimeoutMs=2147483647, producerId=-1, producerEpoch=-1) with context RequestContext(header=RequestHeader(apiKey=INIT_PRODUCER_ID, apiVersion=4, clientId=confluent-control-center-heartbeat-sender-1, correlationId=2, headerVersion=2), connectionId='172.20.0.2:19092-172.20.0.6:33712-31', clientAddress=/172.20.0.6, principal=User:ANONYMOUS, listenerName=ListenerName(INTERNAL), securityProtocol=PLAINTEXT, clientInformation=ClientInformation(softwareName=apache-kafka-java, softwareVersion=7.4.1-ce), fromPrivilegedListener=true, principalSerde=Optional[org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder@1414ee21]) (kafka.server.KafkaApis)
org.apache.kafka.common.errors.CoordinatorLoadInProgressException: Timed out waiting for next producer ID block
[2023-08-21 06:49:04,212] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-metrics-11, _confluent-metrics-9, _confluent-metrics-10, _confluent-metrics-7, _confluent-metrics-8, _confluent-metrics-5, _confluent-metrics-6, _confluent-metrics-3, _confluent-metrics-4, _confluent-metrics-1, _confluent-metrics-2, _confluent-metrics-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:04,238] INFO [MergedLog partition=_confluent-metrics-11, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:04,239] INFO Created log for partition _confluent-metrics-11 in /etc/confluent/kraft-combined-logs/_confluent-metrics-11 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:49:04,248] INFO [Partition _confluent-metrics-11 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-11 (kafka.cluster.Partition)
[2023-08-21 06:49:04,248] INFO [Partition _confluent-metrics-11 broker=1] Log loaded for partition _confluent-metrics-11 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:04,249] INFO Setting topicIdPartition sqZFK0n7T5K2DzqorpR8Pg:_confluent-metrics-11 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:04,249] INFO [MergedLog partition=_confluent-metrics-11, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:04,254] INFO [MergedLog partition=_confluent-metrics-9, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:04,254] INFO Created log for partition _confluent-metrics-9 in /etc/confluent/kraft-combined-logs/_confluent-metrics-9 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:49:04,254] INFO [Partition _confluent-metrics-9 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-9 (kafka.cluster.Partition)
[2023-08-21 06:49:04,254] INFO [Partition _confluent-metrics-9 broker=1] Log loaded for partition _confluent-metrics-9 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:04,256] INFO Setting topicIdPartition sqZFK0n7T5K2DzqorpR8Pg:_confluent-metrics-9 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:04,256] INFO [MergedLog partition=_confluent-metrics-9, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:04,260] INFO [MergedLog partition=_confluent-metrics-10, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:04,262] INFO Created log for partition _confluent-metrics-10 in /etc/confluent/kraft-combined-logs/_confluent-metrics-10 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:49:04,262] INFO [Partition _confluent-metrics-10 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-10 (kafka.cluster.Partition)
[2023-08-21 06:49:04,262] INFO [Partition _confluent-metrics-10 broker=1] Log loaded for partition _confluent-metrics-10 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:04,262] INFO Setting topicIdPartition sqZFK0n7T5K2DzqorpR8Pg:_confluent-metrics-10 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:04,262] INFO [MergedLog partition=_confluent-metrics-10, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:04,286] INFO [MergedLog partition=_confluent-metrics-7, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:04,288] INFO Created log for partition _confluent-metrics-7 in /etc/confluent/kraft-combined-logs/_confluent-metrics-7 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:49:04,288] INFO [Partition _confluent-metrics-7 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-7 (kafka.cluster.Partition)
[2023-08-21 06:49:04,288] INFO [Partition _confluent-metrics-7 broker=1] Log loaded for partition _confluent-metrics-7 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:04,288] INFO Setting topicIdPartition sqZFK0n7T5K2DzqorpR8Pg:_confluent-metrics-7 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:04,288] INFO [MergedLog partition=_confluent-metrics-7, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:04,296] INFO [MergedLog partition=_confluent-metrics-8, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:04,303] INFO Created log for partition _confluent-metrics-8 in /etc/confluent/kraft-combined-logs/_confluent-metrics-8 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:49:04,303] INFO [Partition _confluent-metrics-8 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-8 (kafka.cluster.Partition)
[2023-08-21 06:49:04,303] INFO [Partition _confluent-metrics-8 broker=1] Log loaded for partition _confluent-metrics-8 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:04,304] INFO Setting topicIdPartition sqZFK0n7T5K2DzqorpR8Pg:_confluent-metrics-8 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:04,304] INFO [MergedLog partition=_confluent-metrics-8, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:04,306] INFO [MergedLog partition=_confluent-metrics-5, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:04,317] INFO Created log for partition _confluent-metrics-5 in /etc/confluent/kraft-combined-logs/_confluent-metrics-5 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:49:04,317] INFO [Partition _confluent-metrics-5 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-5 (kafka.cluster.Partition)
[2023-08-21 06:49:04,318] INFO [Partition _confluent-metrics-5 broker=1] Log loaded for partition _confluent-metrics-5 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:04,318] INFO Setting topicIdPartition sqZFK0n7T5K2DzqorpR8Pg:_confluent-metrics-5 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:04,318] INFO [MergedLog partition=_confluent-metrics-5, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:04,321] INFO [MergedLog partition=_confluent-metrics-6, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:04,328] INFO Created log for partition _confluent-metrics-6 in /etc/confluent/kraft-combined-logs/_confluent-metrics-6 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:49:04,328] INFO [Partition _confluent-metrics-6 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-6 (kafka.cluster.Partition)
[2023-08-21 06:49:04,328] INFO [Partition _confluent-metrics-6 broker=1] Log loaded for partition _confluent-metrics-6 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:04,328] INFO Setting topicIdPartition sqZFK0n7T5K2DzqorpR8Pg:_confluent-metrics-6 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:04,328] INFO [MergedLog partition=_confluent-metrics-6, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:04,352] INFO [MergedLog partition=_confluent-metrics-3, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:04,353] INFO Created log for partition _confluent-metrics-3 in /etc/confluent/kraft-combined-logs/_confluent-metrics-3 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:49:04,353] INFO [Partition _confluent-metrics-3 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-3 (kafka.cluster.Partition)
[2023-08-21 06:49:04,353] INFO [Partition _confluent-metrics-3 broker=1] Log loaded for partition _confluent-metrics-3 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:04,353] INFO Setting topicIdPartition sqZFK0n7T5K2DzqorpR8Pg:_confluent-metrics-3 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:04,353] INFO [MergedLog partition=_confluent-metrics-3, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:04,358] INFO [MergedLog partition=_confluent-metrics-4, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:04,361] INFO Created log for partition _confluent-metrics-4 in /etc/confluent/kraft-combined-logs/_confluent-metrics-4 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:49:04,362] INFO [Partition _confluent-metrics-4 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-4 (kafka.cluster.Partition)
[2023-08-21 06:49:04,367] INFO [Partition _confluent-metrics-4 broker=1] Log loaded for partition _confluent-metrics-4 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:04,367] INFO Setting topicIdPartition sqZFK0n7T5K2DzqorpR8Pg:_confluent-metrics-4 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:04,367] INFO [MergedLog partition=_confluent-metrics-4, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:04,371] INFO [MergedLog partition=_confluent-metrics-1, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:04,372] INFO Created log for partition _confluent-metrics-1 in /etc/confluent/kraft-combined-logs/_confluent-metrics-1 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:49:04,373] INFO [Partition _confluent-metrics-1 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-1 (kafka.cluster.Partition)
[2023-08-21 06:49:04,373] INFO [Partition _confluent-metrics-1 broker=1] Log loaded for partition _confluent-metrics-1 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:04,373] INFO Setting topicIdPartition sqZFK0n7T5K2DzqorpR8Pg:_confluent-metrics-1 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:04,373] INFO [MergedLog partition=_confluent-metrics-1, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:04,383] INFO [MergedLog partition=_confluent-metrics-2, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:04,384] INFO Created log for partition _confluent-metrics-2 in /etc/confluent/kraft-combined-logs/_confluent-metrics-2 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:49:04,385] INFO [Partition _confluent-metrics-2 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-2 (kafka.cluster.Partition)
[2023-08-21 06:49:04,385] INFO [Partition _confluent-metrics-2 broker=1] Log loaded for partition _confluent-metrics-2 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:04,385] INFO Setting topicIdPartition sqZFK0n7T5K2DzqorpR8Pg:_confluent-metrics-2 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:04,386] INFO [MergedLog partition=_confluent-metrics-2, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:04,389] INFO [MergedLog partition=_confluent-metrics-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:04,390] INFO Created log for partition _confluent-metrics-0 in /etc/confluent/kraft-combined-logs/_confluent-metrics-0 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:49:04,391] INFO [Partition _confluent-metrics-0 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-0 (kafka.cluster.Partition)
[2023-08-21 06:49:04,391] INFO [Partition _confluent-metrics-0 broker=1] Log loaded for partition _confluent-metrics-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:04,391] INFO Setting topicIdPartition sqZFK0n7T5K2DzqorpR8Pg:_confluent-metrics-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:04,391] INFO [MergedLog partition=_confluent-metrics-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:04,392] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-metrics with new configuration : max.message.bytes -> 10485760,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,retention.ms -> 259200000,segment.ms -> 14400000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:07,766] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-ksql-ksqldb-01_command_topic-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:07,769] INFO [MergedLog partition=_confluent-ksql-ksqldb-01_command_topic-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:07,770] INFO Created log for partition _confluent-ksql-ksqldb-01_command_topic-0 in /etc/confluent/kraft-combined-logs/_confluent-ksql-ksqldb-01_command_topic-0 with properties {cleanup.policy=delete, min.insync.replicas=1, retention.ms=-1, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:07,771] INFO [Partition _confluent-ksql-ksqldb-01_command_topic-0 broker=1] No checkpointed highwatermark is found for partition _confluent-ksql-ksqldb-01_command_topic-0 (kafka.cluster.Partition)
[2023-08-21 06:49:07,771] INFO [Partition _confluent-ksql-ksqldb-01_command_topic-0 broker=1] Log loaded for partition _confluent-ksql-ksqldb-01_command_topic-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:07,771] INFO Setting topicIdPartition ljSMWGsbQka1xujRBxwSTg:_confluent-ksql-ksqldb-01_command_topic-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:07,771] INFO [MergedLog partition=_confluent-ksql-ksqldb-01_command_topic-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-ksql-ksqldb-01_command_topic-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:07,772] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-ksql-ksqldb-01_command_topic with new configuration : cleanup.policy -> delete,min.insync.replicas -> 1,retention.ms -> -1,unclean.leader.election.enable -> false (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:07,807] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(ksqldb-01ksql_processing_log-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:07,810] INFO [MergedLog partition=ksqldb-01ksql_processing_log-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:07,811] INFO Created log for partition ksqldb-01ksql_processing_log-0 in /etc/confluent/kraft-combined-logs/ksqldb-01ksql_processing_log-0 with properties {} (kafka.log.LogManager)
[2023-08-21 06:49:07,812] INFO [Partition ksqldb-01ksql_processing_log-0 broker=1] No checkpointed highwatermark is found for partition ksqldb-01ksql_processing_log-0 (kafka.cluster.Partition)
[2023-08-21 06:49:07,812] INFO [Partition ksqldb-01ksql_processing_log-0 broker=1] Log loaded for partition ksqldb-01ksql_processing_log-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:07,812] INFO Setting topicIdPartition HODin2DOSeqXHj5Jxk9ndA:ksqldb-01ksql_processing_log-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:07,812] INFO [MergedLog partition=ksqldb-01ksql_processing_log-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for ksqldb-01ksql_processing_log-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:07,869] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:07,874] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:07,876] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:07,877] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:49:07,877] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:07,877] INFO Setting topicIdPartition nuzNBOphRraNuQI-KDHpcw:_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:07,877] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:07,878] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:07,908] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:07,913] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:07,916] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:07,918] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:49:07,918] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:07,918] INFO Setting topicIdPartition B-iRZZ9MQyu8lG59Rlmgkg:_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:07,918] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:07,918] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:07,943] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:07,949] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:07,950] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:07,953] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:49:07,953] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:07,953] INFO Setting topicIdPartition rxx5utloSsKdbrCfZ1J4rQ:_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:07,953] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:07,954] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:07,979] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:07,983] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:07,985] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:07,986] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:49:07,986] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:07,986] INFO Setting topicIdPartition F7wPTOoRQsGgdrq0EXS8Kw:_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:07,986] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:07,987] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,013] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,019] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,020] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:08,021] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,021] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,021] INFO Setting topicIdPartition PWvGNjF-R3Cq1QDF6CahzA:_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,021] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,022] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,045] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,055] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,056] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:08,056] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,057] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,057] INFO Setting topicIdPartition diPR56amRuCfOcNgMv0pwA:_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,057] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,058] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,083] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,088] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,089] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:08,089] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,090] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,090] INFO Setting topicIdPartition YBP9dPqrSd2uKAk569sjtg:_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,090] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,090] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,115] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,123] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,124] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:08,125] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,125] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,125] INFO Setting topicIdPartition 9K-gOxZiSYSj8hvZFW6kIA:_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,125] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,126] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,155] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,160] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,162] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:08,165] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,166] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,166] INFO Setting topicIdPartition FHwAqTfOS3qBPWf6FybQ2A:_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,166] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,167] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,193] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,196] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,197] INFO Created log for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:08,197] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,197] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,198] INFO Setting topicIdPartition sxQc7TwySxqEkCPYA1Clkg:_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,198] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,198] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,221] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,225] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,226] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:08,227] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,227] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,228] INFO Setting topicIdPartition uTLWbHt6QKi2o1p5LjodDg:_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,228] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,229] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,264] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,268] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,269] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:08,271] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,271] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,271] INFO Setting topicIdPartition k6nrqtq4SyGbXapGkTEgvQ:_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,271] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,272] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,293] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,298] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,298] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-21 06:49:08,299] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,299] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,299] INFO Setting topicIdPartition ghjPcxCDQNW1cCwMJU1DGw:_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,300] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,300] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,332] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,336] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,336] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:08,337] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,338] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,338] INFO Setting topicIdPartition QJ8hVuOjSFKtqskd7XwXhw:_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,338] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,339] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,370] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,375] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,377] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:08,378] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,378] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,378] INFO Setting topicIdPartition q1GfbmkdSgOQhFMrxxb0SQ:_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,378] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,379] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,404] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,409] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,410] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-21 06:49:08,411] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,411] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,411] INFO Setting topicIdPartition 1fjjFI7dQMm09trq-CzLNw:_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,411] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,412] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,445] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,450] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,450] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-21 06:49:08,451] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,453] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,454] INFO Setting topicIdPartition tihNVPYZRXag6VfnW1j9Tg:_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,454] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,454] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,484] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,489] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,490] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:08,492] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,492] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,492] INFO Setting topicIdPartition ytmjtL2nTjGaJlww7xi09g:_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,493] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,493] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,521] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,524] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,525] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:08,526] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,526] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,526] INFO Setting topicIdPartition v7kcoFBFRb2ca9uiQ7pSAg:_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,526] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,527] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,532] INFO Sent auto-creation request for Set(__transaction_state) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2023-08-21 06:49:08,556] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,561] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,561] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:08,562] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,562] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,563] INFO Setting topicIdPartition PJSqcEsxSOKvdR_Qybu5bA:_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,565] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,565] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,594] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__transaction_state-42, __transaction_state-13, __transaction_state-46, __transaction_state-17, __transaction_state-34, __transaction_state-5, __transaction_state-38, __transaction_state-9, __transaction_state-26, __transaction_state-30, __transaction_state-1, __transaction_state-18, __transaction_state-22, __transaction_state-12, __transaction_state-45, __transaction_state-16, __transaction_state-49, __transaction_state-4, __transaction_state-37, __transaction_state-8, __transaction_state-41, __transaction_state-29, __transaction_state-0, __transaction_state-33, __transaction_state-21, __transaction_state-25, __transaction_state-11, __transaction_state-44, __transaction_state-15, __transaction_state-48, __transaction_state-3, __transaction_state-36, __transaction_state-7, __transaction_state-40, __transaction_state-28, __transaction_state-32, __transaction_state-20, __transaction_state-24, __transaction_state-10, __transaction_state-43, __transaction_state-14, __transaction_state-47, __transaction_state-2, __transaction_state-35, __transaction_state-6, __transaction_state-39, __transaction_state-27, __transaction_state-31, __transaction_state-19, __transaction_state-23) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,637] INFO [MergedLog partition=__transaction_state-42, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,638] INFO Created log for partition __transaction_state-42 in /etc/confluent/kraft-combined-logs/__transaction_state-42 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,639] INFO [Partition __transaction_state-42 broker=1] No checkpointed highwatermark is found for partition __transaction_state-42 (kafka.cluster.Partition)
[2023-08-21 06:49:08,639] INFO [Partition __transaction_state-42 broker=1] Log loaded for partition __transaction_state-42 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,639] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-42 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,639] INFO [MergedLog partition=__transaction_state-42, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-42 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,645] INFO [MergedLog partition=__transaction_state-13, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,646] INFO Created log for partition __transaction_state-13 in /etc/confluent/kraft-combined-logs/__transaction_state-13 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,646] INFO [Partition __transaction_state-13 broker=1] No checkpointed highwatermark is found for partition __transaction_state-13 (kafka.cluster.Partition)
[2023-08-21 06:49:08,646] INFO [Partition __transaction_state-13 broker=1] Log loaded for partition __transaction_state-13 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,647] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-13 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,647] INFO [MergedLog partition=__transaction_state-13, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-13 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,649] INFO [MergedLog partition=__transaction_state-46, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,650] INFO Created log for partition __transaction_state-46 in /etc/confluent/kraft-combined-logs/__transaction_state-46 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,650] INFO [Partition __transaction_state-46 broker=1] No checkpointed highwatermark is found for partition __transaction_state-46 (kafka.cluster.Partition)
[2023-08-21 06:49:08,651] INFO [Partition __transaction_state-46 broker=1] Log loaded for partition __transaction_state-46 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,651] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-46 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,651] INFO [MergedLog partition=__transaction_state-46, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-46 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,655] INFO [MergedLog partition=__transaction_state-17, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,655] INFO Created log for partition __transaction_state-17 in /etc/confluent/kraft-combined-logs/__transaction_state-17 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,655] INFO [Partition __transaction_state-17 broker=1] No checkpointed highwatermark is found for partition __transaction_state-17 (kafka.cluster.Partition)
[2023-08-21 06:49:08,655] INFO [Partition __transaction_state-17 broker=1] Log loaded for partition __transaction_state-17 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,655] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-17 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,655] INFO [MergedLog partition=__transaction_state-17, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-17 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,658] INFO [MergedLog partition=__transaction_state-34, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,659] INFO Created log for partition __transaction_state-34 in /etc/confluent/kraft-combined-logs/__transaction_state-34 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,660] INFO [Partition __transaction_state-34 broker=1] No checkpointed highwatermark is found for partition __transaction_state-34 (kafka.cluster.Partition)
[2023-08-21 06:49:08,660] INFO [Partition __transaction_state-34 broker=1] Log loaded for partition __transaction_state-34 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,661] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-34 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,661] INFO [MergedLog partition=__transaction_state-34, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-34 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,664] INFO [MergedLog partition=__transaction_state-5, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,665] INFO Created log for partition __transaction_state-5 in /etc/confluent/kraft-combined-logs/__transaction_state-5 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,665] INFO [Partition __transaction_state-5 broker=1] No checkpointed highwatermark is found for partition __transaction_state-5 (kafka.cluster.Partition)
[2023-08-21 06:49:08,665] INFO [Partition __transaction_state-5 broker=1] Log loaded for partition __transaction_state-5 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,665] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-5 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,665] INFO [MergedLog partition=__transaction_state-5, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,668] INFO [MergedLog partition=__transaction_state-38, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,670] INFO Created log for partition __transaction_state-38 in /etc/confluent/kraft-combined-logs/__transaction_state-38 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,671] INFO [Partition __transaction_state-38 broker=1] No checkpointed highwatermark is found for partition __transaction_state-38 (kafka.cluster.Partition)
[2023-08-21 06:49:08,671] INFO [Partition __transaction_state-38 broker=1] Log loaded for partition __transaction_state-38 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,671] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-38 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,671] INFO [MergedLog partition=__transaction_state-38, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-38 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,673] INFO [MergedLog partition=__transaction_state-9, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,674] INFO Created log for partition __transaction_state-9 in /etc/confluent/kraft-combined-logs/__transaction_state-9 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,675] INFO [Partition __transaction_state-9 broker=1] No checkpointed highwatermark is found for partition __transaction_state-9 (kafka.cluster.Partition)
[2023-08-21 06:49:08,675] INFO [Partition __transaction_state-9 broker=1] Log loaded for partition __transaction_state-9 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,675] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-9 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,676] INFO [MergedLog partition=__transaction_state-9, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,678] INFO [MergedLog partition=__transaction_state-26, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,678] INFO Created log for partition __transaction_state-26 in /etc/confluent/kraft-combined-logs/__transaction_state-26 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,678] INFO [Partition __transaction_state-26 broker=1] No checkpointed highwatermark is found for partition __transaction_state-26 (kafka.cluster.Partition)
[2023-08-21 06:49:08,679] INFO [Partition __transaction_state-26 broker=1] Log loaded for partition __transaction_state-26 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,680] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-26 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,680] INFO [MergedLog partition=__transaction_state-26, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-26 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,683] INFO [MergedLog partition=__transaction_state-30, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,683] INFO Created log for partition __transaction_state-30 in /etc/confluent/kraft-combined-logs/__transaction_state-30 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,683] INFO [Partition __transaction_state-30 broker=1] No checkpointed highwatermark is found for partition __transaction_state-30 (kafka.cluster.Partition)
[2023-08-21 06:49:08,683] INFO [Partition __transaction_state-30 broker=1] Log loaded for partition __transaction_state-30 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,684] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-30 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,685] INFO [MergedLog partition=__transaction_state-30, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-30 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,687] INFO [MergedLog partition=__transaction_state-1, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,688] INFO Created log for partition __transaction_state-1 in /etc/confluent/kraft-combined-logs/__transaction_state-1 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,688] INFO [Partition __transaction_state-1 broker=1] No checkpointed highwatermark is found for partition __transaction_state-1 (kafka.cluster.Partition)
[2023-08-21 06:49:08,688] INFO [Partition __transaction_state-1 broker=1] Log loaded for partition __transaction_state-1 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,689] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-1 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,689] INFO [MergedLog partition=__transaction_state-1, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,691] INFO [MergedLog partition=__transaction_state-18, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,692] INFO Created log for partition __transaction_state-18 in /etc/confluent/kraft-combined-logs/__transaction_state-18 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,692] INFO [Partition __transaction_state-18 broker=1] No checkpointed highwatermark is found for partition __transaction_state-18 (kafka.cluster.Partition)
[2023-08-21 06:49:08,692] INFO [Partition __transaction_state-18 broker=1] Log loaded for partition __transaction_state-18 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,692] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-18 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,692] INFO [MergedLog partition=__transaction_state-18, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-18 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,696] INFO [MergedLog partition=__transaction_state-22, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,697] INFO Created log for partition __transaction_state-22 in /etc/confluent/kraft-combined-logs/__transaction_state-22 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,697] INFO [Partition __transaction_state-22 broker=1] No checkpointed highwatermark is found for partition __transaction_state-22 (kafka.cluster.Partition)
[2023-08-21 06:49:08,697] INFO [Partition __transaction_state-22 broker=1] Log loaded for partition __transaction_state-22 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,697] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-22 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,697] INFO [MergedLog partition=__transaction_state-22, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-22 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,700] INFO [MergedLog partition=__transaction_state-12, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,701] INFO Created log for partition __transaction_state-12 in /etc/confluent/kraft-combined-logs/__transaction_state-12 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,701] INFO [Partition __transaction_state-12 broker=1] No checkpointed highwatermark is found for partition __transaction_state-12 (kafka.cluster.Partition)
[2023-08-21 06:49:08,701] INFO [Partition __transaction_state-12 broker=1] Log loaded for partition __transaction_state-12 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,701] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-12 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,701] INFO [MergedLog partition=__transaction_state-12, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-12 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,704] INFO [MergedLog partition=__transaction_state-45, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,704] INFO Created log for partition __transaction_state-45 in /etc/confluent/kraft-combined-logs/__transaction_state-45 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,704] INFO [Partition __transaction_state-45 broker=1] No checkpointed highwatermark is found for partition __transaction_state-45 (kafka.cluster.Partition)
[2023-08-21 06:49:08,704] INFO [Partition __transaction_state-45 broker=1] Log loaded for partition __transaction_state-45 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,705] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-45 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,705] INFO [MergedLog partition=__transaction_state-45, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-45 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,707] INFO [MergedLog partition=__transaction_state-16, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,707] INFO Created log for partition __transaction_state-16 in /etc/confluent/kraft-combined-logs/__transaction_state-16 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,707] INFO [Partition __transaction_state-16 broker=1] No checkpointed highwatermark is found for partition __transaction_state-16 (kafka.cluster.Partition)
[2023-08-21 06:49:08,707] INFO [Partition __transaction_state-16 broker=1] Log loaded for partition __transaction_state-16 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,707] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-16 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,707] INFO [MergedLog partition=__transaction_state-16, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-16 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,710] INFO [MergedLog partition=__transaction_state-49, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,710] INFO Created log for partition __transaction_state-49 in /etc/confluent/kraft-combined-logs/__transaction_state-49 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,710] INFO [Partition __transaction_state-49 broker=1] No checkpointed highwatermark is found for partition __transaction_state-49 (kafka.cluster.Partition)
[2023-08-21 06:49:08,710] INFO [Partition __transaction_state-49 broker=1] Log loaded for partition __transaction_state-49 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,710] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-49 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,711] INFO [MergedLog partition=__transaction_state-49, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-49 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,712] INFO [MergedLog partition=__transaction_state-4, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,713] INFO Created log for partition __transaction_state-4 in /etc/confluent/kraft-combined-logs/__transaction_state-4 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,713] INFO [Partition __transaction_state-4 broker=1] No checkpointed highwatermark is found for partition __transaction_state-4 (kafka.cluster.Partition)
[2023-08-21 06:49:08,713] INFO [Partition __transaction_state-4 broker=1] Log loaded for partition __transaction_state-4 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,713] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-4 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,713] INFO [MergedLog partition=__transaction_state-4, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,716] INFO [MergedLog partition=__transaction_state-37, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,717] INFO Created log for partition __transaction_state-37 in /etc/confluent/kraft-combined-logs/__transaction_state-37 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,717] INFO [Partition __transaction_state-37 broker=1] No checkpointed highwatermark is found for partition __transaction_state-37 (kafka.cluster.Partition)
[2023-08-21 06:49:08,717] INFO [Partition __transaction_state-37 broker=1] Log loaded for partition __transaction_state-37 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,717] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-37 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,717] INFO [MergedLog partition=__transaction_state-37, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-37 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,720] INFO [MergedLog partition=__transaction_state-8, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,721] INFO Created log for partition __transaction_state-8 in /etc/confluent/kraft-combined-logs/__transaction_state-8 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,722] INFO [Partition __transaction_state-8 broker=1] No checkpointed highwatermark is found for partition __transaction_state-8 (kafka.cluster.Partition)
[2023-08-21 06:49:08,722] INFO [Partition __transaction_state-8 broker=1] Log loaded for partition __transaction_state-8 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,722] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-8 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,722] INFO [MergedLog partition=__transaction_state-8, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,726] INFO [MergedLog partition=__transaction_state-41, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,726] INFO Created log for partition __transaction_state-41 in /etc/confluent/kraft-combined-logs/__transaction_state-41 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,727] INFO [Partition __transaction_state-41 broker=1] No checkpointed highwatermark is found for partition __transaction_state-41 (kafka.cluster.Partition)
[2023-08-21 06:49:08,727] INFO [Partition __transaction_state-41 broker=1] Log loaded for partition __transaction_state-41 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,728] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-41 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,728] INFO [MergedLog partition=__transaction_state-41, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-41 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,730] INFO [MergedLog partition=__transaction_state-29, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,731] INFO Created log for partition __transaction_state-29 in /etc/confluent/kraft-combined-logs/__transaction_state-29 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,732] INFO [Partition __transaction_state-29 broker=1] No checkpointed highwatermark is found for partition __transaction_state-29 (kafka.cluster.Partition)
[2023-08-21 06:49:08,732] INFO [Partition __transaction_state-29 broker=1] Log loaded for partition __transaction_state-29 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,732] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-29 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,732] INFO [MergedLog partition=__transaction_state-29, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-29 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,734] INFO [MergedLog partition=__transaction_state-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,734] INFO Created log for partition __transaction_state-0 in /etc/confluent/kraft-combined-logs/__transaction_state-0 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,734] INFO [Partition __transaction_state-0 broker=1] No checkpointed highwatermark is found for partition __transaction_state-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,734] INFO [Partition __transaction_state-0 broker=1] Log loaded for partition __transaction_state-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,734] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,735] INFO [MergedLog partition=__transaction_state-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,737] INFO [MergedLog partition=__transaction_state-33, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,737] INFO Created log for partition __transaction_state-33 in /etc/confluent/kraft-combined-logs/__transaction_state-33 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,737] INFO [Partition __transaction_state-33 broker=1] No checkpointed highwatermark is found for partition __transaction_state-33 (kafka.cluster.Partition)
[2023-08-21 06:49:08,737] INFO [Partition __transaction_state-33 broker=1] Log loaded for partition __transaction_state-33 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,738] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-33 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,738] INFO [MergedLog partition=__transaction_state-33, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-33 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,741] INFO [MergedLog partition=__transaction_state-21, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,742] INFO Created log for partition __transaction_state-21 in /etc/confluent/kraft-combined-logs/__transaction_state-21 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,742] INFO [Partition __transaction_state-21 broker=1] No checkpointed highwatermark is found for partition __transaction_state-21 (kafka.cluster.Partition)
[2023-08-21 06:49:08,742] INFO [Partition __transaction_state-21 broker=1] Log loaded for partition __transaction_state-21 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,742] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-21 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,742] INFO [MergedLog partition=__transaction_state-21, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-21 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,745] INFO [MergedLog partition=__transaction_state-25, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,745] INFO Created log for partition __transaction_state-25 in /etc/confluent/kraft-combined-logs/__transaction_state-25 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,745] INFO [Partition __transaction_state-25 broker=1] No checkpointed highwatermark is found for partition __transaction_state-25 (kafka.cluster.Partition)
[2023-08-21 06:49:08,745] INFO [Partition __transaction_state-25 broker=1] Log loaded for partition __transaction_state-25 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,745] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-25 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,746] INFO [MergedLog partition=__transaction_state-25, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-25 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,748] INFO [MergedLog partition=__transaction_state-11, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,749] INFO Created log for partition __transaction_state-11 in /etc/confluent/kraft-combined-logs/__transaction_state-11 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,749] INFO [Partition __transaction_state-11 broker=1] No checkpointed highwatermark is found for partition __transaction_state-11 (kafka.cluster.Partition)
[2023-08-21 06:49:08,749] INFO [Partition __transaction_state-11 broker=1] Log loaded for partition __transaction_state-11 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,749] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-11 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,749] INFO [MergedLog partition=__transaction_state-11, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,752] INFO [MergedLog partition=__transaction_state-44, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,753] INFO Created log for partition __transaction_state-44 in /etc/confluent/kraft-combined-logs/__transaction_state-44 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,753] INFO [Partition __transaction_state-44 broker=1] No checkpointed highwatermark is found for partition __transaction_state-44 (kafka.cluster.Partition)
[2023-08-21 06:49:08,753] INFO [Partition __transaction_state-44 broker=1] Log loaded for partition __transaction_state-44 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,753] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-44 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,754] INFO [MergedLog partition=__transaction_state-44, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-44 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,756] INFO [MergedLog partition=__transaction_state-15, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,757] INFO Created log for partition __transaction_state-15 in /etc/confluent/kraft-combined-logs/__transaction_state-15 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,757] INFO [Partition __transaction_state-15 broker=1] No checkpointed highwatermark is found for partition __transaction_state-15 (kafka.cluster.Partition)
[2023-08-21 06:49:08,757] INFO [Partition __transaction_state-15 broker=1] Log loaded for partition __transaction_state-15 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,757] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-15 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,757] INFO [MergedLog partition=__transaction_state-15, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-15 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,760] INFO [MergedLog partition=__transaction_state-48, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,760] INFO Created log for partition __transaction_state-48 in /etc/confluent/kraft-combined-logs/__transaction_state-48 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,760] INFO [Partition __transaction_state-48 broker=1] No checkpointed highwatermark is found for partition __transaction_state-48 (kafka.cluster.Partition)
[2023-08-21 06:49:08,760] INFO [Partition __transaction_state-48 broker=1] Log loaded for partition __transaction_state-48 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,761] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-48 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,761] INFO [MergedLog partition=__transaction_state-48, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-48 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,763] INFO [MergedLog partition=__transaction_state-3, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,764] INFO Created log for partition __transaction_state-3 in /etc/confluent/kraft-combined-logs/__transaction_state-3 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,764] INFO [Partition __transaction_state-3 broker=1] No checkpointed highwatermark is found for partition __transaction_state-3 (kafka.cluster.Partition)
[2023-08-21 06:49:08,764] INFO [Partition __transaction_state-3 broker=1] Log loaded for partition __transaction_state-3 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,765] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-3 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,765] INFO [MergedLog partition=__transaction_state-3, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,767] INFO [MergedLog partition=__transaction_state-36, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,767] INFO Created log for partition __transaction_state-36 in /etc/confluent/kraft-combined-logs/__transaction_state-36 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,767] INFO [Partition __transaction_state-36 broker=1] No checkpointed highwatermark is found for partition __transaction_state-36 (kafka.cluster.Partition)
[2023-08-21 06:49:08,767] INFO [Partition __transaction_state-36 broker=1] Log loaded for partition __transaction_state-36 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,767] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-36 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,767] INFO [MergedLog partition=__transaction_state-36, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-36 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,770] INFO [MergedLog partition=__transaction_state-7, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,771] INFO Created log for partition __transaction_state-7 in /etc/confluent/kraft-combined-logs/__transaction_state-7 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,771] INFO [Partition __transaction_state-7 broker=1] No checkpointed highwatermark is found for partition __transaction_state-7 (kafka.cluster.Partition)
[2023-08-21 06:49:08,771] INFO [Partition __transaction_state-7 broker=1] Log loaded for partition __transaction_state-7 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,771] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-7 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,771] INFO [MergedLog partition=__transaction_state-7, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,774] INFO [MergedLog partition=__transaction_state-40, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,774] INFO Created log for partition __transaction_state-40 in /etc/confluent/kraft-combined-logs/__transaction_state-40 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,774] INFO [Partition __transaction_state-40 broker=1] No checkpointed highwatermark is found for partition __transaction_state-40 (kafka.cluster.Partition)
[2023-08-21 06:49:08,774] INFO [Partition __transaction_state-40 broker=1] Log loaded for partition __transaction_state-40 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,774] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-40 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,776] INFO [MergedLog partition=__transaction_state-40, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-40 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,777] INFO [MergedLog partition=__transaction_state-28, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,778] INFO Created log for partition __transaction_state-28 in /etc/confluent/kraft-combined-logs/__transaction_state-28 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,778] INFO [Partition __transaction_state-28 broker=1] No checkpointed highwatermark is found for partition __transaction_state-28 (kafka.cluster.Partition)
[2023-08-21 06:49:08,778] INFO [Partition __transaction_state-28 broker=1] Log loaded for partition __transaction_state-28 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,778] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-28 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,778] INFO [MergedLog partition=__transaction_state-28, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-28 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,781] INFO [MergedLog partition=__transaction_state-32, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,781] INFO Created log for partition __transaction_state-32 in /etc/confluent/kraft-combined-logs/__transaction_state-32 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,781] INFO [Partition __transaction_state-32 broker=1] No checkpointed highwatermark is found for partition __transaction_state-32 (kafka.cluster.Partition)
[2023-08-21 06:49:08,781] INFO [Partition __transaction_state-32 broker=1] Log loaded for partition __transaction_state-32 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,781] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-32 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,781] INFO [MergedLog partition=__transaction_state-32, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-32 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,783] INFO [MergedLog partition=__transaction_state-20, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,784] INFO Created log for partition __transaction_state-20 in /etc/confluent/kraft-combined-logs/__transaction_state-20 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,784] INFO [Partition __transaction_state-20 broker=1] No checkpointed highwatermark is found for partition __transaction_state-20 (kafka.cluster.Partition)
[2023-08-21 06:49:08,784] INFO [Partition __transaction_state-20 broker=1] Log loaded for partition __transaction_state-20 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,784] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-20 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,784] INFO [MergedLog partition=__transaction_state-20, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-20 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,786] INFO [MergedLog partition=__transaction_state-24, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,787] INFO Created log for partition __transaction_state-24 in /etc/confluent/kraft-combined-logs/__transaction_state-24 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,787] INFO [Partition __transaction_state-24 broker=1] No checkpointed highwatermark is found for partition __transaction_state-24 (kafka.cluster.Partition)
[2023-08-21 06:49:08,787] INFO [Partition __transaction_state-24 broker=1] Log loaded for partition __transaction_state-24 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,787] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-24 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,787] INFO [MergedLog partition=__transaction_state-24, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-24 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,789] INFO [MergedLog partition=__transaction_state-10, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,789] INFO Created log for partition __transaction_state-10 in /etc/confluent/kraft-combined-logs/__transaction_state-10 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,789] INFO [Partition __transaction_state-10 broker=1] No checkpointed highwatermark is found for partition __transaction_state-10 (kafka.cluster.Partition)
[2023-08-21 06:49:08,789] INFO [Partition __transaction_state-10 broker=1] Log loaded for partition __transaction_state-10 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,789] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-10 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,790] INFO [MergedLog partition=__transaction_state-10, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,792] INFO [MergedLog partition=__transaction_state-43, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,792] INFO Created log for partition __transaction_state-43 in /etc/confluent/kraft-combined-logs/__transaction_state-43 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,792] INFO [Partition __transaction_state-43 broker=1] No checkpointed highwatermark is found for partition __transaction_state-43 (kafka.cluster.Partition)
[2023-08-21 06:49:08,792] INFO [Partition __transaction_state-43 broker=1] Log loaded for partition __transaction_state-43 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,792] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-43 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,792] INFO [MergedLog partition=__transaction_state-43, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-43 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,795] INFO [MergedLog partition=__transaction_state-14, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,795] INFO Created log for partition __transaction_state-14 in /etc/confluent/kraft-combined-logs/__transaction_state-14 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,795] INFO [Partition __transaction_state-14 broker=1] No checkpointed highwatermark is found for partition __transaction_state-14 (kafka.cluster.Partition)
[2023-08-21 06:49:08,795] INFO [Partition __transaction_state-14 broker=1] Log loaded for partition __transaction_state-14 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,795] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-14 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,795] INFO [MergedLog partition=__transaction_state-14, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-14 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,799] INFO [MergedLog partition=__transaction_state-47, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,799] INFO Created log for partition __transaction_state-47 in /etc/confluent/kraft-combined-logs/__transaction_state-47 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,799] INFO [Partition __transaction_state-47 broker=1] No checkpointed highwatermark is found for partition __transaction_state-47 (kafka.cluster.Partition)
[2023-08-21 06:49:08,799] INFO [Partition __transaction_state-47 broker=1] Log loaded for partition __transaction_state-47 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,799] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-47 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,799] INFO [MergedLog partition=__transaction_state-47, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-47 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,801] INFO [MergedLog partition=__transaction_state-2, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,801] INFO Created log for partition __transaction_state-2 in /etc/confluent/kraft-combined-logs/__transaction_state-2 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,802] INFO [Partition __transaction_state-2 broker=1] No checkpointed highwatermark is found for partition __transaction_state-2 (kafka.cluster.Partition)
[2023-08-21 06:49:08,802] INFO [Partition __transaction_state-2 broker=1] Log loaded for partition __transaction_state-2 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,802] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-2 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,802] INFO [MergedLog partition=__transaction_state-2, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,805] INFO [MergedLog partition=__transaction_state-35, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,805] INFO Created log for partition __transaction_state-35 in /etc/confluent/kraft-combined-logs/__transaction_state-35 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,805] INFO [Partition __transaction_state-35 broker=1] No checkpointed highwatermark is found for partition __transaction_state-35 (kafka.cluster.Partition)
[2023-08-21 06:49:08,805] INFO [Partition __transaction_state-35 broker=1] Log loaded for partition __transaction_state-35 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,805] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-35 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,805] INFO [MergedLog partition=__transaction_state-35, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-35 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,807] INFO [MergedLog partition=__transaction_state-6, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,808] INFO Created log for partition __transaction_state-6 in /etc/confluent/kraft-combined-logs/__transaction_state-6 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,808] INFO [Partition __transaction_state-6 broker=1] No checkpointed highwatermark is found for partition __transaction_state-6 (kafka.cluster.Partition)
[2023-08-21 06:49:08,808] INFO [Partition __transaction_state-6 broker=1] Log loaded for partition __transaction_state-6 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,808] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-6 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,808] INFO [MergedLog partition=__transaction_state-6, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,810] INFO [MergedLog partition=__transaction_state-39, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,810] INFO Created log for partition __transaction_state-39 in /etc/confluent/kraft-combined-logs/__transaction_state-39 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,810] INFO [Partition __transaction_state-39 broker=1] No checkpointed highwatermark is found for partition __transaction_state-39 (kafka.cluster.Partition)
[2023-08-21 06:49:08,810] INFO [Partition __transaction_state-39 broker=1] Log loaded for partition __transaction_state-39 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,810] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-39 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,810] INFO [MergedLog partition=__transaction_state-39, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-39 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,812] INFO [MergedLog partition=__transaction_state-27, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,813] INFO Created log for partition __transaction_state-27 in /etc/confluent/kraft-combined-logs/__transaction_state-27 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,813] INFO [Partition __transaction_state-27 broker=1] No checkpointed highwatermark is found for partition __transaction_state-27 (kafka.cluster.Partition)
[2023-08-21 06:49:08,813] INFO [Partition __transaction_state-27 broker=1] Log loaded for partition __transaction_state-27 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,813] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-27 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,813] INFO [MergedLog partition=__transaction_state-27, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-27 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,815] INFO [MergedLog partition=__transaction_state-31, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,816] INFO Created log for partition __transaction_state-31 in /etc/confluent/kraft-combined-logs/__transaction_state-31 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,816] INFO [Partition __transaction_state-31 broker=1] No checkpointed highwatermark is found for partition __transaction_state-31 (kafka.cluster.Partition)
[2023-08-21 06:49:08,816] INFO [Partition __transaction_state-31 broker=1] Log loaded for partition __transaction_state-31 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,816] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-31 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,816] INFO [MergedLog partition=__transaction_state-31, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-31 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,818] INFO [MergedLog partition=__transaction_state-19, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,818] INFO Created log for partition __transaction_state-19 in /etc/confluent/kraft-combined-logs/__transaction_state-19 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,819] INFO [Partition __transaction_state-19 broker=1] No checkpointed highwatermark is found for partition __transaction_state-19 (kafka.cluster.Partition)
[2023-08-21 06:49:08,819] INFO [Partition __transaction_state-19 broker=1] Log loaded for partition __transaction_state-19 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,819] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-19 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,819] INFO [MergedLog partition=__transaction_state-19, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-19 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,821] INFO [MergedLog partition=__transaction_state-23, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,821] INFO Created log for partition __transaction_state-23 in /etc/confluent/kraft-combined-logs/__transaction_state-23 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-21 06:49:08,821] INFO [Partition __transaction_state-23 broker=1] No checkpointed highwatermark is found for partition __transaction_state-23 (kafka.cluster.Partition)
[2023-08-21 06:49:08,821] INFO [Partition __transaction_state-23 broker=1] Log loaded for partition __transaction_state-23 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,822] INFO Setting topicIdPartition EEksywGSRY27tWfxe7PN1w:__transaction_state-23 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,822] INFO [MergedLog partition=__transaction_state-23, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-23 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,822] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 42 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,828] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 13 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,828] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 46 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,828] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 17 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,828] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 34 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,828] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 5 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,828] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 38 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,828] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 9 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,829] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 26 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,829] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 30 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,829] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 1 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,829] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 18 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,829] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 22 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,829] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-42 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,829] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 12 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,829] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 45 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,829] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 16 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,829] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 49 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,829] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 4 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,829] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 37 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,829] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 8 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,829] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 41 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,829] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 29 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,829] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 0 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,829] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 33 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,829] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 21 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,829] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 25 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,829] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 11 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,829] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 44 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,829] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 15 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,829] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 48 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,829] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 3 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,829] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 36 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,829] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-42 in 2 milliseconds, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,829] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 7 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,830] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 40 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,830] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 28 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,830] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 32 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,830] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 20 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,830] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 24 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,830] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 10 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,830] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 43 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,830] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 14 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,830] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 47 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,830] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 2 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,830] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 35 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,830] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 6 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,830] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 39 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,830] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 27 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,830] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 31 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,830] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 19 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,832] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 23 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,832] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic __transaction_state with new configuration : compression.type -> uncompressed,cleanup.policy -> compact,min.insync.replicas -> 1,segment.bytes -> 104857600,confluent.placement.constraints -> ,unclean.leader.election.enable -> false (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,832] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-42 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,832] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-13 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-13 in 5 milliseconds, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-13 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-46 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-46 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-46 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-17 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-17 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-17 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-34 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-34 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-34 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-5 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-5 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-5 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-38 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-38 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-38 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-9 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-9 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-9 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-26 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-26 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-26 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-30 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-30 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-30 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-1 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-1 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-1 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-18 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,833] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-18 in 4 milliseconds, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-18 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-22 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-22 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-22 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-12 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-12 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-12 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-45 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-cluster-rekey-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-45 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-45 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-16 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-16 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-16 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-49 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-49 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-49 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-4 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-4 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-4 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-37 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-37 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-37 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-8 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-8 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-8 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-41 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-41 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-41 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-29 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-29 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-29 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-0 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,834] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-0 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-0 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-33 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-33 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-33 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-21 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-21 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-21 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-25 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-25 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-25 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-11 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-11 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-11 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-44 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-44 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-44 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-15 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-15 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-15 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-48 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-48 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-48 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-3 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-3 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-3 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-36 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-36 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-36 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-7 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-7 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-7 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-40 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-40 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-40 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,835] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-28 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-28 in 6 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-28 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-32 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-32 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-32 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-20 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-20 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-20 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-24 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-24 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-24 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-10 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-10 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-10 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-43 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-43 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-43 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-14 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-14 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-14 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-47 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-47 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-47 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-2 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-2 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-2 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,836] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-35 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,837] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-35 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,837] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-35 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,837] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-6 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,837] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-6 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,837] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-6 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,837] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-39 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,837] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-39 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,837] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-39 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,837] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-27 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,837] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-27 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,837] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-27 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,837] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-31 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,837] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-31 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,837] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-31 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,837] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-cluster-rekey-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,837] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-19 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,837] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-19 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,837] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-19 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,837] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-23 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,837] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-23 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,837] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-23 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-21 06:49:08,837] INFO Created log for partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-cluster-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
[2023-08-21 06:49:08,838] INFO [Partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,838] INFO [Partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,838] INFO Setting topicIdPartition auy0bfg_SnimO99SDjrcnA:_confluent-controlcenter-7-4-1-1-cluster-rekey-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,839] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-cluster-rekey-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-cluster-rekey-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,839] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-cluster-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,840] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,842] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,842] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:08,843] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,844] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,844] INFO Setting topicIdPartition B-XzTf4BSCmckxUpksSh5g:_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,844] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,844] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,846] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,849] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,849] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-21 06:49:08,850] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,851] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,851] INFO Setting topicIdPartition zx9An0JbTdKEnh8PrPkdgw:_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,851] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,851] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,852] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,854] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,855] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:08,855] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,856] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,856] INFO Setting topicIdPartition I7kUMijaRlyroXtWdlUaSg:_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,856] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,857] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,858] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,860] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,860] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:08,861] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,862] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,862] INFO Setting topicIdPartition IyvLH7BhSXSobh7LpxWzZQ:_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,862] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,862] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,863] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,865] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,865] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:08,866] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,867] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,867] INFO Setting topicIdPartition BISYvAwoQCmUP8Q4fZodFA:_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,867] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,867] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,867] INFO [TransactionCoordinator id=1] Initialized transactionalId ksqldb-01 with producerId 3 and producer epoch 0 on partition __transaction_state-1 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:49:08,868] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,870] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,871] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-21 06:49:08,872] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,872] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,872] INFO Setting topicIdPartition NZ2cRyjtSJutt40RDyHUEw:_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,872] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,873] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,873] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,876] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,877] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:08,877] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,878] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,879] INFO Setting topicIdPartition Zezg52g2S9Gsjk1Bc1883w:_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,879] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,879] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,880] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,883] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,884] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:08,884] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,885] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,885] INFO Setting topicIdPartition s-uVjKLzSASCbAKUMozxBw:_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,885] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,885] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,905] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,910] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,911] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:08,911] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,912] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,912] INFO Setting topicIdPartition nEObeJyvS4aJXCV_wTnFQQ:_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,912] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,912] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,937] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,940] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,941] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
[2023-08-21 06:49:08,941] INFO [Partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,943] INFO [Partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,943] INFO Setting topicIdPartition DhyOT6HaT7C3k1zTdwybgA:_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,943] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,943] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:08,969] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:08,974] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:08,974] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-21 06:49:08,975] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,976] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:08,976] INFO Setting topicIdPartition RP-NYwFQSXSbZhog0Zn87w:_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:08,976] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:08,977] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:09,002] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:09,005] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:09,006] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:09,006] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:49:09,007] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:09,007] INFO Setting topicIdPartition xapLPFbNSry53WXoH9TnwA:_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:09,007] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:09,007] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:09,032] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:09,035] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:09,035] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:09,036] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:49:09,036] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:09,036] INFO Setting topicIdPartition _BXg2Si7R1a1yE65bB8Jow:_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:09,036] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:09,037] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:09,062] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:09,065] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:09,066] INFO Created log for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:09,066] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:49:09,066] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:09,066] INFO Setting topicIdPartition 4dxDFq1IQred6YbREN_BUA:_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:09,067] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:09,068] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:09,091] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:09,095] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:09,096] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-21 06:49:09,097] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:49:09,097] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:09,097] INFO Setting topicIdPartition _d6yZWTlSSavu3EPa6RFuQ:_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:09,098] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:09,098] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:09,122] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:09,127] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:09,127] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:49:09,129] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:49:09,129] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:09,129] INFO Setting topicIdPartition 4n8Bg0NIRDCOwWkH9QMyfQ:_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:09,129] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:09,129] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:09,155] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:09,158] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:09,159] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-21 06:49:09,159] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:49:09,160] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:09,160] INFO Setting topicIdPartition xU8qJV3dRPq7pHbX_FnqaQ:_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:09,161] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:09,161] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:09,396] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1-command in Empty state. Created a new member id _confluent-controlcenter-7-4-1-1-command-85ba3b0b-9cfe-4910-a842-4e862f41a62e-StreamThread-1-consumer-d8cb04af-161c-4892-9ade-e9d2a92476bf and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:09,399] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-7-4-1-1-command in state PreparingRebalance with old generation 0 (__consumer_offsets-23) (reason: Adding new member _confluent-controlcenter-7-4-1-1-command-85ba3b0b-9cfe-4910-a842-4e862f41a62e-StreamThread-1-consumer-d8cb04af-161c-4892-9ade-e9d2a92476bf with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:09,406] INFO [GroupCoordinator 1]: Stabilized group _confluent-controlcenter-7-4-1-1-command generation 1 (__consumer_offsets-23) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:09,439] INFO [GroupCoordinator 1]: Assignment received from leader _confluent-controlcenter-7-4-1-1-command-85ba3b0b-9cfe-4910-a842-4e862f41a62e-StreamThread-1-consumer-d8cb04af-161c-4892-9ade-e9d2a92476bf for group _confluent-controlcenter-7-4-1-1-command for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:11,594] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in Empty state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-4-consumer-3f8b9d3e-1b56-43ff-bf97-86e892f034e9 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:11,597] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-7-4-1-1 in state PreparingRebalance with old generation 0 (__consumer_offsets-5) (reason: Adding new member _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-4-consumer-3f8b9d3e-1b56-43ff-bf97-86e892f034e9 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:11,597] INFO [GroupCoordinator 1]: Stabilized group _confluent-controlcenter-7-4-1-1 generation 1 (__consumer_offsets-5) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:11,599] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in CompletingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-2-consumer-a2a5f00e-eec2-4875-93e1-0b10a891053d and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:11,609] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-7-4-1-1 in state PreparingRebalance with old generation 1 (__consumer_offsets-5) (reason: Adding new member _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-2-consumer-a2a5f00e-eec2-4875-93e1-0b10a891053d with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:11,614] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-1-consumer-f72c5886-cdf8-4aa8-9e28-0607395c2446 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:11,618] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-10-consumer-4b3cbf52-907b-44e9-80ad-a5bfed36ca3b and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:11,618] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-5-consumer-ff0ab27b-d927-42d1-8c89-7caee60008ed and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:11,619] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-3-consumer-50e84cb8-359c-4a08-8b48-ac61387a16fd and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:11,622] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-9-consumer-cf17dd5f-edce-4870-ab4e-421dce067a38 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:11,628] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-7-consumer-c3013792-6adb-4a6d-9256-a853c7694bae and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:11,642] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-12-consumer-2c06e155-1e6e-4939-8669-d8bbdaa94234 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:11,645] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-16-consumer-19e51e2a-2be3-4fd9-9ab9-017f9b860d1f and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:11,648] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-6-consumer-f3a291bd-ab14-4336-af83-b986a3b3880d and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:11,651] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-14-consumer-ca0e1804-0873-4532-980e-35f4abf2ec07 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:11,652] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-15-consumer-b7dd4a79-b92c-40a1-bb80-c29879608378 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:11,654] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-8-consumer-b87e0893-6996-40c0-8d20-d1eaa2f81d76 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:11,655] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-13-consumer-1f6c2d0b-02c2-4b33-a30b-37c779238ce1 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:11,658] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-11-consumer-5e7ab38a-b083-4c1a-85ad-6a149539a962 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:11,662] INFO [GroupCoordinator 1]: Stabilized group _confluent-controlcenter-7-4-1-1 generation 2 (__consumer_offsets-5) with 16 members (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:11,695] INFO [GroupCoordinator 1]: Assignment received from leader _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-4-consumer-3f8b9d3e-1b56-43ff-bf97-86e892f034e9 for group _confluent-controlcenter-7-4-1-1 for generation 2. The group has 16 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:14,893] ERROR [ReplicaManager broker=1] Error processing append operation on partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (kafka.server.ReplicaManager)
org.apache.kafka.common.errors.OutOfOrderSequenceException: Out of order sequence number for producer 0 at offset 1 in partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0: 0 (incoming seq. number), 54833 (current end sequence number)
[2023-08-21 06:49:14,897] ERROR [ReplicaManager broker=1] Error processing append operation on partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (kafka.server.ReplicaManager)
org.apache.kafka.common.errors.OutOfOrderSequenceException: Out of order sequence number for producer 0 at offset 1 in partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0: 0 (incoming seq. number), 54833 (current end sequence number)
[2023-08-21 06:49:14,897] ERROR [ReplicaManager broker=1] Error processing append operation on partition _confluent-monitoring-0 (kafka.server.ReplicaManager)
org.apache.kafka.common.errors.OutOfOrderSequenceException: Out of order sequence number for producer 0 at offset 1 in partition _confluent-monitoring-0: 0 (incoming seq. number), 54833 (current end sequence number)
[2023-08-21 06:49:19,158] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(telemetry-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:19,161] INFO [MergedLog partition=telemetry-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:19,162] INFO Created log for partition telemetry-0 in /etc/confluent/kraft-combined-logs/telemetry-0 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:49:19,162] INFO [Partition telemetry-0 broker=1] No checkpointed highwatermark is found for partition telemetry-0 (kafka.cluster.Partition)
[2023-08-21 06:49:19,163] INFO [Partition telemetry-0 broker=1] Log loaded for partition telemetry-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:19,163] INFO Setting topicIdPartition GaeDRRvJSiaymVz4pb6D6Q:telemetry-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:19,163] INFO [MergedLog partition=telemetry-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for telemetry-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:19,164] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic telemetry with new configuration : max.message.bytes -> 10485760,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,retention.ms -> 259200000,segment.ms -> 14400000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:19,226] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-telemetry-metrics-3, _confluent-telemetry-metrics-4, _confluent-telemetry-metrics-5, _confluent-telemetry-metrics-6, _confluent-telemetry-metrics-7, _confluent-telemetry-metrics-8, _confluent-telemetry-metrics-9, _confluent-telemetry-metrics-10, _confluent-telemetry-metrics-11, _confluent-telemetry-metrics-0, _confluent-telemetry-metrics-1, _confluent-telemetry-metrics-2) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:19,281] INFO [MergedLog partition=_confluent-telemetry-metrics-3, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:19,283] INFO Created log for partition _confluent-telemetry-metrics-3 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-3 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:49:19,283] INFO [Partition _confluent-telemetry-metrics-3 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-3 (kafka.cluster.Partition)
[2023-08-21 06:49:19,284] INFO [Partition _confluent-telemetry-metrics-3 broker=1] Log loaded for partition _confluent-telemetry-metrics-3 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:19,284] INFO Setting topicIdPartition H_Ygm6a6SBC70IoG6xl-0Q:_confluent-telemetry-metrics-3 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:19,284] INFO [MergedLog partition=_confluent-telemetry-metrics-3, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:19,286] INFO [MergedLog partition=_confluent-telemetry-metrics-4, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:19,287] INFO Created log for partition _confluent-telemetry-metrics-4 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-4 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:49:19,287] INFO [Partition _confluent-telemetry-metrics-4 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-4 (kafka.cluster.Partition)
[2023-08-21 06:49:19,287] INFO [Partition _confluent-telemetry-metrics-4 broker=1] Log loaded for partition _confluent-telemetry-metrics-4 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:19,299] INFO Setting topicIdPartition H_Ygm6a6SBC70IoG6xl-0Q:_confluent-telemetry-metrics-4 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:19,311] INFO [MergedLog partition=_confluent-telemetry-metrics-4, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:19,313] INFO [MergedLog partition=_confluent-telemetry-metrics-5, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:19,314] INFO Created log for partition _confluent-telemetry-metrics-5 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-5 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:49:19,314] INFO [Partition _confluent-telemetry-metrics-5 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-5 (kafka.cluster.Partition)
[2023-08-21 06:49:19,314] INFO [Partition _confluent-telemetry-metrics-5 broker=1] Log loaded for partition _confluent-telemetry-metrics-5 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:19,314] INFO Setting topicIdPartition H_Ygm6a6SBC70IoG6xl-0Q:_confluent-telemetry-metrics-5 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:19,314] INFO [MergedLog partition=_confluent-telemetry-metrics-5, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:19,316] INFO [MergedLog partition=_confluent-telemetry-metrics-6, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:19,316] INFO Created log for partition _confluent-telemetry-metrics-6 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-6 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:49:19,316] INFO [Partition _confluent-telemetry-metrics-6 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-6 (kafka.cluster.Partition)
[2023-08-21 06:49:19,316] INFO [Partition _confluent-telemetry-metrics-6 broker=1] Log loaded for partition _confluent-telemetry-metrics-6 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:19,317] INFO Setting topicIdPartition H_Ygm6a6SBC70IoG6xl-0Q:_confluent-telemetry-metrics-6 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:19,317] INFO [MergedLog partition=_confluent-telemetry-metrics-6, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:19,340] INFO [MergedLog partition=_confluent-telemetry-metrics-7, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:19,341] INFO Created log for partition _confluent-telemetry-metrics-7 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-7 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:49:19,341] INFO [Partition _confluent-telemetry-metrics-7 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-7 (kafka.cluster.Partition)
[2023-08-21 06:49:19,341] INFO [Partition _confluent-telemetry-metrics-7 broker=1] Log loaded for partition _confluent-telemetry-metrics-7 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:19,341] INFO Setting topicIdPartition H_Ygm6a6SBC70IoG6xl-0Q:_confluent-telemetry-metrics-7 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:19,341] INFO [MergedLog partition=_confluent-telemetry-metrics-7, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:19,350] INFO [MergedLog partition=_confluent-telemetry-metrics-8, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:19,351] INFO Created log for partition _confluent-telemetry-metrics-8 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-8 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:49:19,351] INFO [Partition _confluent-telemetry-metrics-8 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-8 (kafka.cluster.Partition)
[2023-08-21 06:49:19,351] INFO [Partition _confluent-telemetry-metrics-8 broker=1] Log loaded for partition _confluent-telemetry-metrics-8 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:19,351] INFO Setting topicIdPartition H_Ygm6a6SBC70IoG6xl-0Q:_confluent-telemetry-metrics-8 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:19,353] INFO [MergedLog partition=_confluent-telemetry-metrics-8, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:19,358] INFO [MergedLog partition=_confluent-telemetry-metrics-9, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:19,358] INFO Created log for partition _confluent-telemetry-metrics-9 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-9 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:49:19,359] INFO [Partition _confluent-telemetry-metrics-9 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-9 (kafka.cluster.Partition)
[2023-08-21 06:49:19,359] INFO [Partition _confluent-telemetry-metrics-9 broker=1] Log loaded for partition _confluent-telemetry-metrics-9 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:19,359] INFO Setting topicIdPartition H_Ygm6a6SBC70IoG6xl-0Q:_confluent-telemetry-metrics-9 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:19,359] INFO [MergedLog partition=_confluent-telemetry-metrics-9, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:19,361] INFO [MergedLog partition=_confluent-telemetry-metrics-10, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:19,362] INFO Created log for partition _confluent-telemetry-metrics-10 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-10 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:49:19,362] INFO [Partition _confluent-telemetry-metrics-10 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-10 (kafka.cluster.Partition)
[2023-08-21 06:49:19,372] INFO [Partition _confluent-telemetry-metrics-10 broker=1] Log loaded for partition _confluent-telemetry-metrics-10 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:19,372] INFO Setting topicIdPartition H_Ygm6a6SBC70IoG6xl-0Q:_confluent-telemetry-metrics-10 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:19,372] INFO [MergedLog partition=_confluent-telemetry-metrics-10, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:19,374] INFO [MergedLog partition=_confluent-telemetry-metrics-11, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:19,375] INFO Created log for partition _confluent-telemetry-metrics-11 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-11 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:49:19,375] INFO [Partition _confluent-telemetry-metrics-11 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-11 (kafka.cluster.Partition)
[2023-08-21 06:49:19,375] INFO [Partition _confluent-telemetry-metrics-11 broker=1] Log loaded for partition _confluent-telemetry-metrics-11 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:19,376] INFO Setting topicIdPartition H_Ygm6a6SBC70IoG6xl-0Q:_confluent-telemetry-metrics-11 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:19,376] INFO [MergedLog partition=_confluent-telemetry-metrics-11, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:19,381] INFO [MergedLog partition=_confluent-telemetry-metrics-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:19,383] INFO Created log for partition _confluent-telemetry-metrics-0 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-0 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:49:19,383] INFO [Partition _confluent-telemetry-metrics-0 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-0 (kafka.cluster.Partition)
[2023-08-21 06:49:19,384] INFO [Partition _confluent-telemetry-metrics-0 broker=1] Log loaded for partition _confluent-telemetry-metrics-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:19,384] INFO Setting topicIdPartition H_Ygm6a6SBC70IoG6xl-0Q:_confluent-telemetry-metrics-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:19,384] INFO [MergedLog partition=_confluent-telemetry-metrics-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:19,386] INFO [MergedLog partition=_confluent-telemetry-metrics-1, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:19,387] INFO Created log for partition _confluent-telemetry-metrics-1 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-1 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:49:19,387] INFO [Partition _confluent-telemetry-metrics-1 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-1 (kafka.cluster.Partition)
[2023-08-21 06:49:19,387] INFO [Partition _confluent-telemetry-metrics-1 broker=1] Log loaded for partition _confluent-telemetry-metrics-1 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:19,388] INFO Setting topicIdPartition H_Ygm6a6SBC70IoG6xl-0Q:_confluent-telemetry-metrics-1 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:19,388] INFO [MergedLog partition=_confluent-telemetry-metrics-1, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:19,390] INFO [MergedLog partition=_confluent-telemetry-metrics-2, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:19,391] INFO Created log for partition _confluent-telemetry-metrics-2 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-2 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:49:19,391] INFO [Partition _confluent-telemetry-metrics-2 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-2 (kafka.cluster.Partition)
[2023-08-21 06:49:19,391] INFO [Partition _confluent-telemetry-metrics-2 broker=1] Log loaded for partition _confluent-telemetry-metrics-2 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:19,391] INFO Setting topicIdPartition H_Ygm6a6SBC70IoG6xl-0Q:_confluent-telemetry-metrics-2 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:19,391] INFO [MergedLog partition=_confluent-telemetry-metrics-2, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:19,392] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-telemetry-metrics with new configuration : max.message.bytes -> 10485760,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,retention.ms -> 259200000,segment.ms -> 14400000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:21,965] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent_balancer_api_state-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:49:21,969] INFO [MergedLog partition=_confluent_balancer_api_state-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:49:21,970] INFO Created log for partition _confluent_balancer_api_state-0 in /etc/confluent/kraft-combined-logs/_confluent_balancer_api_state-0 with properties {cleanup.policy=compact, retention.ms=-1} (kafka.log.LogManager)
[2023-08-21 06:49:21,979] INFO [Partition _confluent_balancer_api_state-0 broker=1] No checkpointed highwatermark is found for partition _confluent_balancer_api_state-0 (kafka.cluster.Partition)
[2023-08-21 06:49:21,979] INFO [Partition _confluent_balancer_api_state-0 broker=1] Log loaded for partition _confluent_balancer_api_state-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:49:21,979] INFO Setting topicIdPartition dNMOC4KSSxuHUQWuafJgNg:_confluent_balancer_api_state-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:49:21,979] INFO [MergedLog partition=_confluent_balancer_api_state-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent_balancer_api_state-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:49:21,980] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent_balancer_api_state with new configuration : cleanup.policy -> compact,retention.ms -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:49:23,306] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group ConfluentTelemetryReporterSampler--6086747792590773153 in Empty state. Created a new member id kafka-cruise-control-4d661536-45c7-465e-92c9-7d948ff99b31 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:23,308] INFO [GroupCoordinator 1]: Preparing to rebalance group ConfluentTelemetryReporterSampler--6086747792590773153 in state PreparingRebalance with old generation 0 (__consumer_offsets-20) (reason: Adding new member kafka-cruise-control-4d661536-45c7-465e-92c9-7d948ff99b31 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:23,308] INFO [GroupCoordinator 1]: Stabilized group ConfluentTelemetryReporterSampler--6086747792590773153 generation 1 (__consumer_offsets-20) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:49:23,315] INFO [GroupCoordinator 1]: Assignment received from leader kafka-cruise-control-4d661536-45c7-465e-92c9-7d948ff99b31 for group ConfluentTelemetryReporterSampler--6086747792590773153 for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:00,455] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2023-08-21 06:56:01,824] INFO KafkaConfig values: 
	advertised.listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = 6y3hy_GuTIu0Uqpv31xQMw
	client.quota.callback.class = null
	compression.type = producer
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.demotion.support.enabled = false
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = true
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = ANY_UNEVEN_LOAD
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = false
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.resource.utilization.detector.enabled = false
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.partition.maximum.movements = 5
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.suspension.ms = 10800000
	confluent.balancer.topic.replication.factor = 1
	confluent.balancer.triggering.goals = []
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.kraft.mitigation.enabled = false
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.max.topics.per.snapshot = 5000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud
	confluent.cdc.api.keys.topic = 
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 1
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.metadata.load.threads = 32
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.throttle.ms = null
	confluent.metadata.active.encryptor = null
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-01:19092
	confluent.min.acks = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.prefer.tier.fetch.ms = -1
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.dynamic.reporting.min.usage = 102400
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = false
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = http://schema-registry:8081
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.telemetry.enabled = true
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.producer.enable.idempotence = false
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.partition.default.placement = 2
	confluent.topic.replica.assignor.builder.class = 
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id = 
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.valid.broker.rack.set = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-01:29092]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = INTERNAL
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
	listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092,CONTROLLER://kafka-01:29092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dirs = /etc/confluent/kraft-combined-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 2000000
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter, io.confluent.telemetry.reporter.TelemetryReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = [hidden]
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.id.quota.window.num = 11
	producer.id.quota.window.size.seconds = 1
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 2097152
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.acl.change.notification.expiration.ms = 900000
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2023-08-21 06:56:01,852] INFO KafkaConfig values: 
	advertised.listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = 6y3hy_GuTIu0Uqpv31xQMw
	client.quota.callback.class = null
	compression.type = producer
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.demotion.support.enabled = false
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = true
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = ANY_UNEVEN_LOAD
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = false
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.resource.utilization.detector.enabled = false
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.partition.maximum.movements = 5
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.suspension.ms = 10800000
	confluent.balancer.topic.replication.factor = 1
	confluent.balancer.triggering.goals = []
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.kraft.mitigation.enabled = false
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.max.topics.per.snapshot = 5000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud
	confluent.cdc.api.keys.topic = 
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 1
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.metadata.load.threads = 32
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.throttle.ms = null
	confluent.metadata.active.encryptor = null
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-01:19092
	confluent.min.acks = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.prefer.tier.fetch.ms = -1
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.dynamic.reporting.min.usage = 102400
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = false
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = http://schema-registry:8081
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.telemetry.enabled = true
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.producer.enable.idempotence = false
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.partition.default.placement = 2
	confluent.topic.replica.assignor.builder.class = 
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id = 
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.valid.broker.rack.set = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-01:29092]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = INTERNAL
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
	listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092,CONTROLLER://kafka-01:29092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dirs = /etc/confluent/kraft-combined-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 2000000
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter, io.confluent.telemetry.reporter.TelemetryReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = [hidden]
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.id.quota.window.num = 11
	producer.id.quota.window.size.seconds = 1
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 2097152
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.acl.change.notification.expiration.ms = 900000
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2023-08-21 06:56:01,862] INFO Starting controller (kafka.server.ControllerServer)
[2023-08-21 06:56:02,174] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas)
[2023-08-21 06:56:02,184] INFO Awaiting socket connections on kafka-01:29092. (kafka.network.DataPlaneAcceptor)
[2023-08-21 06:56:02,228] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)
[2023-08-21 06:56:02,237] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer)
[2023-08-21 06:56:02,373] INFO [MergedLog partition=__cluster_metadata-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:02,374] INFO [MergedLog partition=__cluster_metadata-0, dir=/etc/confluent/kraft-combined-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.MergedLog$)
[2023-08-21 06:56:02,375] INFO [MergedLog partition=__cluster_metadata-0, dir=/etc/confluent/kraft-combined-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.MergedLog$)
[2023-08-21 06:56:02,398] INFO Initialized snapshots with IDs SortedSet() from /etc/confluent/kraft-combined-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2023-08-21 06:56:02,415] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2023-08-21 06:56:02,503] INFO [kafka-raft-outbound-request-thread]: Starting (kafka.raft.RaftSendThread)
[2023-08-21 06:56:02,503] INFO [kafka-raft-io-thread]: Starting (kafka.raft.KafkaRaftManager$RaftIoThread)
[2023-08-21 06:56:02,604] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-21 06:56:02,605] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2023-08-21 06:56:02,607] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-21 06:56:02,616] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2023-08-21 06:56:02,617] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-21 06:56:02,618] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2023-08-21 06:56:02,619] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-21 06:56:02,645] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-21 06:56:02,658] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
[2023-08-21 06:56:02,663] INFO [BrokerServer id=1] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2023-08-21 06:56:02,664] INFO [BrokerServer id=1] Starting broker (kafka.server.BrokerServer)
[2023-08-21 06:56:02,687] INFO [BrokerServer id=1] FIPS mode enabled: false (kafka.server.BrokerServer)
[2023-08-21 06:56:02,701] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2023-08-21 06:56:02,701] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-21 06:56:02,702] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-21 06:56:02,703] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2023-08-21 06:56:02,703] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2023-08-21 06:56:02,704] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-21 06:56:02,704] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-21 06:56:02,747] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread)
[2023-08-21 06:56:02,748] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Recorded new controller, from now on will use node kafka-01:29092 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2023-08-21 06:56:02,785] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas)
[2023-08-21 06:56:02,786] INFO Awaiting socket connections on kafka-01:19092. (kafka.network.DataPlaneAcceptor)
[2023-08-21 06:56:02,793] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(INTERNAL) (kafka.network.SocketServer)
[2023-08-21 06:56:02,793] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas)
[2023-08-21 06:56:02,794] INFO Awaiting socket connections on kafka-01:9092. (kafka.network.DataPlaneAcceptor)
[2023-08-21 06:56:02,800] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2023-08-21 06:56:02,807] INFO [BrokerToControllerChannelManager broker=1 name=alterPartition]: Starting (kafka.server.BrokerToControllerRequestThread)
[2023-08-21 06:56:02,808] INFO [BrokerToControllerChannelManager broker=1 name=alterPartition]: Recorded new controller, from now on will use node kafka-01:29092 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2023-08-21 06:56:02,828] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-21 06:56:02,829] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-21 06:56:02,830] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-21 06:56:02,831] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-21 06:56:02,832] INFO [ExpirationReaper-1-ListOffsets]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-21 06:56:02,853] INFO [BrokerHealthManager]: Starting (kafka.availability.BrokerHealthManager)
[2023-08-21 06:56:02,859] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-21 06:56:02,859] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-21 06:56:04,721] INFO [BrokerToControllerChannelManager broker=1 name=heartbeat]: Starting (kafka.server.BrokerToControllerRequestThread)
[2023-08-21 06:56:04,721] INFO [BrokerToControllerChannelManager broker=1 name=heartbeat]: Recorded new controller, from now on will use node kafka-01:29092 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2023-08-21 06:56:04,723] INFO [BrokerLifecycleManager id=1] Incarnation uc2CNTmGRsWCUGSR3mMMHQ of broker 1 in cluster MkU3OEVBNTcwNTJENDM2Qg is now STARTING. (kafka.server.BrokerLifecycleManager)
[2023-08-21 06:56:04,782] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-21 06:56:04,790] INFO [BrokerServer id=1] Waiting for broker metadata to catch up. (kafka.server.BrokerServer)
[2023-08-21 06:56:04,820] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 6 (kafka.server.BrokerLifecycleManager)
[2023-08-21 06:56:04,827] INFO [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2023-08-21 06:56:04,832] INFO [BrokerMetadataListener id=1] Starting to publish metadata events at offset 6. (kafka.server.metadata.BrokerMetadataListener)
[2023-08-21 06:56:04,838] INFO [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=6, epoch=1) with metadata.version 3.4-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2023-08-21 06:56:04,839] INFO Loading logs from log dirs ArraySeq(/etc/confluent/kraft-combined-logs) (kafka.log.LogManager)
[2023-08-21 06:56:04,839] INFO [BrokerLifecycleManager id=1] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2023-08-21 06:56:04,841] INFO Attempting recovery for all logs in /etc/confluent/kraft-combined-logs since no clean shutdown file was found (kafka.log.LogManager)
[2023-08-21 06:56:04,846] INFO Loaded 0 logs in 8ms. (kafka.log.LogManager)
[2023-08-21 06:56:04,846] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2023-08-21 06:56:04,847] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2023-08-21 06:56:04,871] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2023-08-21 06:56:04,871] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2023-08-21 06:56:04,967] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner)
[2023-08-21 06:56:04,967] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner)
[2023-08-21 06:56:04,973] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2023-08-21 06:56:04,974] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:04,977] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:04,978] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:56:04,981] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-21 06:56:04,983] INFO [Transaction Marker Channel Manager 1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2023-08-21 06:56:04,984] INFO [BrokerMetadataPublisher id=1] Updating metadata.version to 3.4-IV0 at offset OffsetAndEpoch(offset=6, epoch=1). (kafka.server.metadata.BrokerMetadataPublisher)
[2023-08-21 06:56:05,006] INFO KafkaConfig values: 
	advertised.listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = 6y3hy_GuTIu0Uqpv31xQMw
	client.quota.callback.class = null
	compression.type = producer
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.demotion.support.enabled = false
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = true
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = ANY_UNEVEN_LOAD
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = false
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.resource.utilization.detector.enabled = false
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.partition.maximum.movements = 5
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.suspension.ms = 10800000
	confluent.balancer.topic.replication.factor = 1
	confluent.balancer.triggering.goals = []
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.kraft.mitigation.enabled = false
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.max.topics.per.snapshot = 5000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud
	confluent.cdc.api.keys.topic = 
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 1
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.metadata.load.threads = 32
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.throttle.ms = null
	confluent.metadata.active.encryptor = null
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-01:19092
	confluent.min.acks = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.prefer.tier.fetch.ms = -1
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.dynamic.reporting.min.usage = 102400
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = false
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = http://schema-registry:8081
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.telemetry.enabled = true
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.producer.enable.idempotence = false
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.partition.default.placement = 2
	confluent.topic.replica.assignor.builder.class = 
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id = 
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.valid.broker.rack.set = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-01:29092]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = INTERNAL
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
	listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092,CONTROLLER://kafka-01:29092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dirs = /etc/confluent/kraft-combined-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 2000000
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter, io.confluent.telemetry.reporter.TelemetryReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = [hidden]
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.id.quota.window.num = 11
	producer.id.quota.window.size.seconds = 1
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 2097152
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.acl.change.notification.expiration.ms = 900000
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2023-08-21 06:56:05,015] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
[2023-08-21 06:56:05,081] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2023-08-21 06:56:05,103] INFO Sent auto-creation request for Set(_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2023-08-21 06:56:05,110] INFO Sent auto-creation request for Set(_confluent-command) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2023-08-21 06:56:05,107] INFO Sent auto-creation request for Set(_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2023-08-21 06:56:05,109] INFO Sent auto-creation request for Set(_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2023-08-21 06:56:05,106] INFO Sent auto-creation request for Set(_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey, _confluent-monitoring) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2023-08-21 06:56:05,119] INFO Sent auto-creation request for Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2023-08-21 06:56:05,173] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2023-08-21 06:56:05,224] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:05,296] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:05,304] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 with properties {} (kafka.log.LogManager)
[2023-08-21 06:56:05,308] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,309] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,313] INFO Setting topicIdPartition wNtOqLELQoaSXCf5s3lBiw:_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:05,313] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:05,340] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, _confluent-monitoring-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:05,344] INFO Sent auto-creation request for Set(_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2023-08-21 06:56:05,348] INFO KafkaConfig values: 
	advertised.listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = 6y3hy_GuTIu0Uqpv31xQMw
	client.quota.callback.class = null
	compression.type = producer
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.demotion.support.enabled = false
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = true
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = ANY_UNEVEN_LOAD
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = false
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.resource.utilization.detector.enabled = false
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.partition.maximum.movements = 5
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.suspension.ms = 10800000
	confluent.balancer.topic.replication.factor = 1
	confluent.balancer.triggering.goals = []
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.kraft.mitigation.enabled = false
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.max.topics.per.snapshot = 5000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud
	confluent.cdc.api.keys.topic = 
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 1
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.metadata.load.threads = 32
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.throttle.ms = null
	confluent.metadata.active.encryptor = null
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-01:19092
	confluent.min.acks = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.prefer.tier.fetch.ms = -1
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.dynamic.reporting.min.usage = 102400
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = false
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = http://schema-registry:8081
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.telemetry.enabled = true
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.producer.enable.idempotence = false
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.partition.default.placement = 2
	confluent.topic.replica.assignor.builder.class = 
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id = 
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.valid.broker.rack.set = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-01:29092]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = INTERNAL
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
	listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092,CONTROLLER://kafka-01:29092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dirs = /etc/confluent/kraft-combined-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 2000000
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter, io.confluent.telemetry.reporter.TelemetryReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = [hidden]
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.id.quota.window.num = 11
	producer.id.quota.window.size.seconds = 1
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 2097152
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.acl.change.notification.expiration.ms = 900000
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2023-08-21 06:56:05,370] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:05,372] INFO Created log for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 with properties {} (kafka.log.LogManager)
[2023-08-21 06:56:05,373] INFO [Partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,373] INFO [Partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,373] INFO Setting topicIdPartition v3rKkuhtRi2y2lpQ4VnnAQ:_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:05,374] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:05,378] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:05,379] INFO Created log for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 with properties {} (kafka.log.LogManager)
[2023-08-21 06:56:05,380] INFO [Partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,380] INFO [Partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,381] INFO Setting topicIdPartition eKNyGXolTdSGsnv2u2plCQ:_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:05,382] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:05,385] INFO [MergedLog partition=_confluent-monitoring-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:05,386] INFO Created log for partition _confluent-monitoring-0 in /etc/confluent/kraft-combined-logs/_confluent-monitoring-0 with properties {} (kafka.log.LogManager)
[2023-08-21 06:56:05,387] INFO [Partition _confluent-monitoring-0 broker=1] No checkpointed highwatermark is found for partition _confluent-monitoring-0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,388] INFO [Partition _confluent-monitoring-0 broker=1] Log loaded for partition _confluent-monitoring-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,388] INFO Setting topicIdPartition GKRkYh_PTKGo__59jWqORA:_confluent-monitoring-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:05,389] INFO [MergedLog partition=_confluent-monitoring-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-monitoring-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:05,390] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:05,397] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:05,398] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 with properties {} (kafka.log.LogManager)
[2023-08-21 06:56:05,399] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,399] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,400] INFO Setting topicIdPartition H8j9eCrSTkSlLIJ5c5dxeg:_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:05,400] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:05,402] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0, _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:05,418] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:05,422] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 with properties {} (kafka.log.LogManager)
[2023-08-21 06:56:05,422] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,422] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,422] INFO Setting topicIdPartition kNXpLAg7QxiQcZROVnumuw:_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:05,423] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:05,428] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:05,430] INFO Created log for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 with properties {} (kafka.log.LogManager)
[2023-08-21 06:56:05,432] INFO [Partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,433] INFO [Partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,433] INFO Setting topicIdPartition d-19VptATu2hQCoR-jGS8Q:_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:05,433] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:05,434] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-command-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:05,445] INFO [MergedLog partition=_confluent-command-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:05,449] INFO Created log for partition _confluent-command-0 in /etc/confluent/kraft-combined-logs/_confluent-command-0 with properties {} (kafka.log.LogManager)
[2023-08-21 06:56:05,449] INFO [Partition _confluent-command-0 broker=1] No checkpointed highwatermark is found for partition _confluent-command-0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,450] INFO [Partition _confluent-command-0 broker=1] Log loaded for partition _confluent-command-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,450] INFO Setting topicIdPartition 0B_FHBsiQYq5iHOU_pUlIA:_confluent-command-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:05,451] INFO [MergedLog partition=_confluent-command-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-command-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:05,457] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0, _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0, _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:05,477] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:05,478] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with properties {} (kafka.log.LogManager)
[2023-08-21 06:56:05,480] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,480] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,480] INFO Setting topicIdPartition vd0P1xQ4SlGXtczXUTm0ZQ:_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:05,480] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:05,483] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:05,485] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with properties {} (kafka.log.LogManager)
[2023-08-21 06:56:05,485] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,485] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,485] INFO Setting topicIdPartition hJlRS0uORV6DEmtzjBWajA:_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:05,486] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:05,489] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:05,490] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 with properties {} (kafka.log.LogManager)
[2023-08-21 06:56:05,491] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,491] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,491] INFO Setting topicIdPartition ib_FTQ_2SuyPBOnk0NrCeQ:_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:05,491] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:05,506] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2023-08-21 06:56:05,509] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:05,778] INFO [MergedLog partition=__consumer_offsets-13, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:05,779] INFO Created log for partition __consumer_offsets-13 in /etc/confluent/kraft-combined-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:05,779] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2023-08-21 06:56:05,779] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,796] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-13 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:05,807] INFO [MergedLog partition=__consumer_offsets-13, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-13 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:05,811] INFO [MergedLog partition=__consumer_offsets-46, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:05,820] INFO Created log for partition __consumer_offsets-46 in /etc/confluent/kraft-combined-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:05,820] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2023-08-21 06:56:05,820] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,820] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-46 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:05,821] INFO [MergedLog partition=__consumer_offsets-46, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-46 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:05,825] INFO [MergedLog partition=__consumer_offsets-9, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:05,826] INFO Created log for partition __consumer_offsets-9 in /etc/confluent/kraft-combined-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:05,829] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2023-08-21 06:56:05,829] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,835] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-9 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:05,835] INFO [MergedLog partition=__consumer_offsets-9, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:05,843] INFO [MergedLog partition=__consumer_offsets-42, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:05,846] INFO Created log for partition __consumer_offsets-42 in /etc/confluent/kraft-combined-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:05,847] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2023-08-21 06:56:05,847] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,847] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-42 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:05,847] INFO [MergedLog partition=__consumer_offsets-42, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-42 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:05,855] INFO [MergedLog partition=__consumer_offsets-21, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:05,857] INFO Created log for partition __consumer_offsets-21 in /etc/confluent/kraft-combined-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:05,861] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2023-08-21 06:56:05,861] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,861] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-21 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:05,862] INFO [MergedLog partition=__consumer_offsets-21, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-21 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:05,871] INFO [MergedLog partition=__consumer_offsets-17, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:05,873] INFO Created log for partition __consumer_offsets-17 in /etc/confluent/kraft-combined-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:05,873] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2023-08-21 06:56:05,873] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,896] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-17 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:05,897] INFO [MergedLog partition=__consumer_offsets-17, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-17 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:05,923] INFO [MergedLog partition=__consumer_offsets-30, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:05,931] INFO Created log for partition __consumer_offsets-30 in /etc/confluent/kraft-combined-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:05,932] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2023-08-21 06:56:05,932] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,932] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-30 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:05,933] INFO [MergedLog partition=__consumer_offsets-30, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-30 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:05,966] INFO [MergedLog partition=__consumer_offsets-26, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:05,967] INFO Created log for partition __consumer_offsets-26 in /etc/confluent/kraft-combined-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:05,967] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2023-08-21 06:56:05,967] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,968] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-26 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:05,968] INFO [MergedLog partition=__consumer_offsets-26, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-26 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:05,972] INFO [MergedLog partition=__consumer_offsets-5, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:05,976] INFO Created log for partition __consumer_offsets-5 in /etc/confluent/kraft-combined-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:05,977] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2023-08-21 06:56:05,977] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,977] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-5 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:05,977] INFO [MergedLog partition=__consumer_offsets-5, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:05,984] INFO [MergedLog partition=__consumer_offsets-38, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:05,988] INFO Created log for partition __consumer_offsets-38 in /etc/confluent/kraft-combined-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:05,988] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2023-08-21 06:56:05,989] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:05,990] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-38 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:05,991] INFO [MergedLog partition=__consumer_offsets-38, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-38 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,000] INFO [MergedLog partition=__consumer_offsets-1, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,001] INFO Created log for partition __consumer_offsets-1 in /etc/confluent/kraft-combined-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,011] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2023-08-21 06:56:06,011] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,011] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-1 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,011] INFO [MergedLog partition=__consumer_offsets-1, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,015] INFO [MergedLog partition=__consumer_offsets-34, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,020] INFO Created log for partition __consumer_offsets-34 in /etc/confluent/kraft-combined-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,020] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2023-08-21 06:56:06,020] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,020] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-34 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,020] INFO [MergedLog partition=__consumer_offsets-34, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-34 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,045] INFO [MergedLog partition=__consumer_offsets-16, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,046] INFO Created log for partition __consumer_offsets-16 in /etc/confluent/kraft-combined-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,051] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2023-08-21 06:56:06,051] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,052] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-16 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,052] INFO [MergedLog partition=__consumer_offsets-16, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-16 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,060] INFO [MergedLog partition=__consumer_offsets-45, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,061] INFO Created log for partition __consumer_offsets-45 in /etc/confluent/kraft-combined-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,083] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2023-08-21 06:56:06,083] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,084] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-45 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,084] INFO [MergedLog partition=__consumer_offsets-45, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-45 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,108] INFO [MergedLog partition=__consumer_offsets-12, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,110] INFO Created log for partition __consumer_offsets-12 in /etc/confluent/kraft-combined-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,110] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2023-08-21 06:56:06,110] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,110] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-12 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,110] INFO [MergedLog partition=__consumer_offsets-12, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-12 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,114] INFO [MergedLog partition=__consumer_offsets-41, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,116] INFO Created log for partition __consumer_offsets-41 in /etc/confluent/kraft-combined-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,116] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2023-08-21 06:56:06,116] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,122] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-41 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,122] INFO [MergedLog partition=__consumer_offsets-41, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-41 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,126] INFO [MergedLog partition=__consumer_offsets-24, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,127] INFO Created log for partition __consumer_offsets-24 in /etc/confluent/kraft-combined-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,127] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2023-08-21 06:56:06,128] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,128] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-24 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,128] INFO [MergedLog partition=__consumer_offsets-24, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-24 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,157] INFO [MergedLog partition=__consumer_offsets-20, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,158] INFO Created log for partition __consumer_offsets-20 in /etc/confluent/kraft-combined-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,158] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2023-08-21 06:56:06,159] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,159] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-20 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,159] INFO [MergedLog partition=__consumer_offsets-20, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-20 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,165] INFO [MergedLog partition=__consumer_offsets-49, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,166] INFO Created log for partition __consumer_offsets-49 in /etc/confluent/kraft-combined-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,166] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2023-08-21 06:56:06,166] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,179] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-49 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,179] INFO [MergedLog partition=__consumer_offsets-49, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-49 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,191] INFO [MergedLog partition=__consumer_offsets-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,205] INFO Created log for partition __consumer_offsets-0 in /etc/confluent/kraft-combined-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,206] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,207] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,207] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,209] INFO [MergedLog partition=__consumer_offsets-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,216] INFO [MergedLog partition=__consumer_offsets-29, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,221] INFO Created log for partition __consumer_offsets-29 in /etc/confluent/kraft-combined-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,221] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2023-08-21 06:56:06,223] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,224] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-29 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,224] INFO [MergedLog partition=__consumer_offsets-29, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-29 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,227] INFO [MergedLog partition=__consumer_offsets-25, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,229] INFO Created log for partition __consumer_offsets-25 in /etc/confluent/kraft-combined-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,229] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2023-08-21 06:56:06,229] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,229] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-25 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,230] INFO [MergedLog partition=__consumer_offsets-25, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-25 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,238] INFO [MergedLog partition=__consumer_offsets-8, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,239] INFO Created log for partition __consumer_offsets-8 in /etc/confluent/kraft-combined-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,251] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2023-08-21 06:56:06,251] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,251] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-8 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,252] INFO [MergedLog partition=__consumer_offsets-8, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,259] INFO [MergedLog partition=__consumer_offsets-37, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,264] INFO Created log for partition __consumer_offsets-37 in /etc/confluent/kraft-combined-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,268] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2023-08-21 06:56:06,268] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,268] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-37 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,268] INFO [MergedLog partition=__consumer_offsets-37, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-37 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,279] INFO [MergedLog partition=__consumer_offsets-4, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,280] INFO Created log for partition __consumer_offsets-4 in /etc/confluent/kraft-combined-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,283] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2023-08-21 06:56:06,283] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,283] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-4 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,283] INFO [MergedLog partition=__consumer_offsets-4, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,295] INFO [MergedLog partition=__consumer_offsets-33, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,302] INFO Created log for partition __consumer_offsets-33 in /etc/confluent/kraft-combined-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,303] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2023-08-21 06:56:06,303] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,303] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-33 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,303] INFO [MergedLog partition=__consumer_offsets-33, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-33 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,315] INFO [MergedLog partition=__consumer_offsets-15, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,316] INFO Created log for partition __consumer_offsets-15 in /etc/confluent/kraft-combined-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,317] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2023-08-21 06:56:06,317] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,317] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-15 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,317] INFO [MergedLog partition=__consumer_offsets-15, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-15 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,322] INFO [MergedLog partition=__consumer_offsets-48, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,335] INFO Created log for partition __consumer_offsets-48 in /etc/confluent/kraft-combined-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,335] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2023-08-21 06:56:06,335] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,335] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-48 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,336] INFO [MergedLog partition=__consumer_offsets-48, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-48 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,348] INFO [MergedLog partition=__consumer_offsets-11, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,349] INFO Created log for partition __consumer_offsets-11 in /etc/confluent/kraft-combined-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,350] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2023-08-21 06:56:06,350] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,350] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-11 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,350] INFO [MergedLog partition=__consumer_offsets-11, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,355] INFO [MergedLog partition=__consumer_offsets-44, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,356] INFO Created log for partition __consumer_offsets-44 in /etc/confluent/kraft-combined-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,362] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2023-08-21 06:56:06,363] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,363] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-44 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,382] INFO [MergedLog partition=__consumer_offsets-44, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-44 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,387] INFO [MergedLog partition=__consumer_offsets-23, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,389] INFO Created log for partition __consumer_offsets-23 in /etc/confluent/kraft-combined-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,389] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2023-08-21 06:56:06,389] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,390] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-23 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,390] INFO [MergedLog partition=__consumer_offsets-23, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-23 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,395] INFO [MergedLog partition=__consumer_offsets-19, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,396] INFO Created log for partition __consumer_offsets-19 in /etc/confluent/kraft-combined-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,407] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2023-08-21 06:56:06,407] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,407] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-19 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,408] INFO [MergedLog partition=__consumer_offsets-19, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-19 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,427] INFO [MergedLog partition=__consumer_offsets-32, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,443] INFO Created log for partition __consumer_offsets-32 in /etc/confluent/kraft-combined-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,444] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2023-08-21 06:56:06,444] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,444] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-32 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,444] INFO [MergedLog partition=__consumer_offsets-32, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-32 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,448] INFO [MergedLog partition=__consumer_offsets-28, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,449] INFO Created log for partition __consumer_offsets-28 in /etc/confluent/kraft-combined-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,451] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2023-08-21 06:56:06,453] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,453] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-28 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,456] INFO [MergedLog partition=__consumer_offsets-28, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-28 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,473] INFO [MergedLog partition=__consumer_offsets-7, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,474] INFO Created log for partition __consumer_offsets-7 in /etc/confluent/kraft-combined-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,474] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2023-08-21 06:56:06,475] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,475] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-7 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,479] INFO [MergedLog partition=__consumer_offsets-7, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,487] INFO [MergedLog partition=__consumer_offsets-40, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,489] INFO Created log for partition __consumer_offsets-40 in /etc/confluent/kraft-combined-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,495] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2023-08-21 06:56:06,495] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,495] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-40 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,495] INFO [MergedLog partition=__consumer_offsets-40, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-40 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,499] INFO [MergedLog partition=__consumer_offsets-3, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,501] INFO Created log for partition __consumer_offsets-3 in /etc/confluent/kraft-combined-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,506] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2023-08-21 06:56:06,506] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,506] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-3 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,506] INFO [MergedLog partition=__consumer_offsets-3, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,510] INFO [MergedLog partition=__consumer_offsets-36, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,516] INFO Created log for partition __consumer_offsets-36 in /etc/confluent/kraft-combined-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,516] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2023-08-21 06:56:06,516] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,516] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-36 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,516] INFO [MergedLog partition=__consumer_offsets-36, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-36 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,522] INFO [MergedLog partition=__consumer_offsets-47, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,528] INFO Created log for partition __consumer_offsets-47 in /etc/confluent/kraft-combined-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,528] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2023-08-21 06:56:06,528] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,528] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-47 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,528] INFO [MergedLog partition=__consumer_offsets-47, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-47 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,547] INFO [MergedLog partition=__consumer_offsets-14, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,548] INFO Created log for partition __consumer_offsets-14 in /etc/confluent/kraft-combined-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,548] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2023-08-21 06:56:06,549] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,549] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-14 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,549] INFO [MergedLog partition=__consumer_offsets-14, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-14 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,559] INFO [MergedLog partition=__consumer_offsets-43, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,567] INFO Created log for partition __consumer_offsets-43 in /etc/confluent/kraft-combined-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,569] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2023-08-21 06:56:06,569] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,569] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-43 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,569] INFO [MergedLog partition=__consumer_offsets-43, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-43 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,598] INFO [MergedLog partition=__consumer_offsets-10, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,608] INFO Created log for partition __consumer_offsets-10 in /etc/confluent/kraft-combined-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,615] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2023-08-21 06:56:06,615] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,615] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-10 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,616] INFO [MergedLog partition=__consumer_offsets-10, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,633] INFO [MergedLog partition=__consumer_offsets-22, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,634] INFO Created log for partition __consumer_offsets-22 in /etc/confluent/kraft-combined-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,634] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2023-08-21 06:56:06,634] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,635] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-22 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,635] INFO [MergedLog partition=__consumer_offsets-22, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-22 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,638] INFO [MergedLog partition=__consumer_offsets-18, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,640] INFO Created log for partition __consumer_offsets-18 in /etc/confluent/kraft-combined-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,640] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2023-08-21 06:56:06,641] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,641] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-18 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,641] INFO [MergedLog partition=__consumer_offsets-18, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-18 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,649] INFO [MergedLog partition=__consumer_offsets-31, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,651] INFO Created log for partition __consumer_offsets-31 in /etc/confluent/kraft-combined-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,651] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2023-08-21 06:56:06,651] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,655] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-31 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,655] INFO [MergedLog partition=__consumer_offsets-31, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-31 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,658] INFO [MergedLog partition=__consumer_offsets-27, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,662] INFO Created log for partition __consumer_offsets-27 in /etc/confluent/kraft-combined-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,662] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2023-08-21 06:56:06,662] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,662] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-27 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,662] INFO [MergedLog partition=__consumer_offsets-27, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-27 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,672] INFO [MergedLog partition=__consumer_offsets-39, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,673] INFO Created log for partition __consumer_offsets-39 in /etc/confluent/kraft-combined-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,673] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2023-08-21 06:56:06,673] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,674] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-39 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,674] INFO [MergedLog partition=__consumer_offsets-39, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-39 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,680] INFO [MergedLog partition=__consumer_offsets-6, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,681] INFO Created log for partition __consumer_offsets-6 in /etc/confluent/kraft-combined-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,681] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2023-08-21 06:56:06,681] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,681] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-6 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,682] INFO [MergedLog partition=__consumer_offsets-6, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,689] INFO [MergedLog partition=__consumer_offsets-35, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,691] INFO Created log for partition __consumer_offsets-35 in /etc/confluent/kraft-combined-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,691] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2023-08-21 06:56:06,692] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,692] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-35 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,723] INFO [MergedLog partition=__consumer_offsets-35, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-35 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,739] INFO [MergedLog partition=__consumer_offsets-2, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:06,741] INFO Created log for partition __consumer_offsets-2 in /etc/confluent/kraft-combined-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-21 06:56:06,741] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2023-08-21 06:56:06,741] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:06,742] INFO Setting topicIdPartition cjbzyFxASjG38ta-ehEPeA:__consumer_offsets-2 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:06,751] INFO [MergedLog partition=__consumer_offsets-2, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:06,753] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,754] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,769] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,769] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,782] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,782] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,787] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,787] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,789] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,789] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,799] INFO The cleaning for partition __consumer_offsets-46 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,807] INFO The cleaning for partition __consumer_offsets-21 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,803] INFO The cleaning for partition __consumer_offsets-9 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,799] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,789] INFO The cleaning for partition __consumer_offsets-13 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,832] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,833] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,833] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,833] INFO The cleaning for partition __consumer_offsets-42 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,833] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,833] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,833] INFO The cleaning for partition __consumer_offsets-17 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,833] INFO The cleaning for partition __consumer_offsets-30 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,833] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,834] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,834] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,834] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,834] INFO The cleaning for partition __consumer_offsets-26 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,834] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,834] INFO The cleaning for partition __consumer_offsets-5 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,834] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,835] INFO The cleaning for partition __consumer_offsets-38 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,835] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,835] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,835] INFO The cleaning for partition __consumer_offsets-1 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,835] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,835] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,836] INFO The cleaning for partition __consumer_offsets-34 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,836] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,836] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,836] INFO The cleaning for partition __consumer_offsets-16 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,843] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,843] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,843] INFO The cleaning for partition __consumer_offsets-45 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,844] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,844] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,844] INFO The cleaning for partition __consumer_offsets-12 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,844] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,844] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,845] INFO The cleaning for partition __consumer_offsets-41 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,845] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,845] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,845] INFO The cleaning for partition __consumer_offsets-24 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,845] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,846] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,846] INFO The cleaning for partition __consumer_offsets-20 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,848] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,848] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,848] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,848] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,851] INFO The cleaning for partition __consumer_offsets-49 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,851] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,851] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,851] INFO The cleaning for partition __consumer_offsets-0 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,851] INFO The cleaning for partition __consumer_offsets-29 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,853] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,853] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,853] INFO The cleaning for partition __consumer_offsets-25 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,853] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,854] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,854] INFO The cleaning for partition __consumer_offsets-8 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,877] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,832] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 49 milliseconds for epoch 0, of which 49 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,832] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 78 milliseconds for epoch 0, of which 78 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,833] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 40 milliseconds for epoch 0, of which 38 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,833] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 63 milliseconds for epoch 0, of which 54 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,833] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 46 milliseconds for epoch 0, of which 46 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,833] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,895] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,834] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 1 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,834] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,834] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,835] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,835] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,897] INFO Cleaning for partition __consumer_offsets-38 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,899] INFO The cleaning for partition __consumer_offsets-37 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,836] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,844] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 8 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,844] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,845] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,846] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,846] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,899] INFO Cleaning for partition __consumer_offsets-17 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,900] INFO Cleaning for partition __consumer_offsets-13 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,900] INFO Cleaning for partition __consumer_offsets-30 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,900] INFO Cleaning for partition __consumer_offsets-9 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,901] INFO Cleaning for partition __consumer_offsets-42 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,901] INFO Cleaning for partition __consumer_offsets-46 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,902] INFO Cleaning for partition __consumer_offsets-21 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,902] INFO Cleaning for partition __consumer_offsets-24 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,903] INFO Cleaning for partition __consumer_offsets-26 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,902] INFO Cleaning for partition __consumer_offsets-12 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,902] INFO Cleaning for partition __consumer_offsets-45 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,902] INFO Cleaning for partition __consumer_offsets-41 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,854] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,853] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,926] INFO Cleaning for partition __consumer_offsets-25 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,852] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,926] INFO Cleaning for partition __consumer_offsets-29 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,851] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 16 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,926] INFO Cleaning for partition __consumer_offsets-34 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,851] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,921] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,918] INFO Cleaning for partition __consumer_offsets-5 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,927] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,927] INFO Cleaning for partition __consumer_offsets-0 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,851] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,927] INFO Cleaning for partition __consumer_offsets-49 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,918] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 64 milliseconds for epoch 0, of which 64 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,918] INFO Cleaning for partition __consumer_offsets-1 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,918] INFO Cleaning for partition __consumer_offsets-20 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,917] INFO Cleaning for partition __consumer_offsets-16 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,930] INFO Cleaning for partition __consumer_offsets-37 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,930] INFO Cleaning for partition __consumer_offsets-8 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,931] INFO The cleaning for partition __consumer_offsets-4 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,937] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 19 milliseconds for epoch 0, of which 19 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,937] INFO Cleaning for partition __consumer_offsets-4 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,942] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,942] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,952] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,952] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,952] INFO The cleaning for partition __consumer_offsets-15 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,952] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,952] INFO Cleaning for partition __consumer_offsets-15 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,953] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,953] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,953] INFO The cleaning for partition __consumer_offsets-33 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,954] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 26 milliseconds for epoch 0, of which 26 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,954] INFO Cleaning for partition __consumer_offsets-33 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,954] INFO The cleaning for partition __consumer_offsets-48 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,954] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,954] INFO Cleaning for partition __consumer_offsets-48 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,956] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,956] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,957] INFO The cleaning for partition __consumer_offsets-11 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,957] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,957] INFO Cleaning for partition __consumer_offsets-11 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,958] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in Empty state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-6-consumer-e000286f-9ea2-467d-92c9-aceff73fa1de and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,962] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,962] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in Empty state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-14-consumer-21a6f8fa-8e6a-4960-a907-bfaf13b19e2a and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,963] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,964] INFO The cleaning for partition __consumer_offsets-44 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,964] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,964] INFO Cleaning for partition __consumer_offsets-44 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,964] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,964] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,964] INFO The cleaning for partition __consumer_offsets-23 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,964] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,965] INFO Cleaning for partition __consumer_offsets-23 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,965] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in Empty state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-15-consumer-90ec9e84-5e9e-4cc9-a556-e22919f85349 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,967] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,967] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,967] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,967] INFO The cleaning for partition __consumer_offsets-32 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,968] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,968] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,968] INFO Cleaning for partition __consumer_offsets-32 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,968] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,969] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,969] INFO The cleaning for partition __consumer_offsets-28 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,969] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,969] INFO The cleaning for partition __consumer_offsets-7 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,969] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,969] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,969] INFO Cleaning for partition __consumer_offsets-28 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,969] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,970] INFO Cleaning for partition __consumer_offsets-7 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,970] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,970] INFO The cleaning for partition __consumer_offsets-40 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,970] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,971] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,971] INFO Cleaning for partition __consumer_offsets-40 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,971] INFO The cleaning for partition __consumer_offsets-3 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,971] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,972] INFO Cleaning for partition __consumer_offsets-3 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,972] INFO The cleaning for partition __consumer_offsets-19 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,972] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,972] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,972] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,972] INFO The cleaning for partition __consumer_offsets-36 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,972] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,972] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,973] INFO The cleaning for partition __consumer_offsets-47 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,973] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,973] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,973] INFO The cleaning for partition __consumer_offsets-14 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,973] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,973] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,973] INFO The cleaning for partition __consumer_offsets-43 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,974] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,974] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,974] INFO The cleaning for partition __consumer_offsets-10 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,974] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,974] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,974] INFO The cleaning for partition __consumer_offsets-22 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,977] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,979] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 15 milliseconds for epoch 0, of which 14 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,979] INFO Cleaning for partition __consumer_offsets-19 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,979] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,979] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,979] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,980] INFO Cleaning for partition __consumer_offsets-10 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,980] INFO The cleaning for partition __consumer_offsets-18 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,980] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,980] INFO Cleaning for partition __consumer_offsets-18 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,980] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,980] INFO Cleaning for partition __consumer_offsets-43 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,981] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,981] INFO Cleaning for partition __consumer_offsets-14 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,981] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,981] INFO Cleaning for partition __consumer_offsets-47 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,982] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 10 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,982] INFO Cleaning for partition __consumer_offsets-36 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,982] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,982] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,982] INFO The cleaning for partition __consumer_offsets-31 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,982] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,983] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,983] INFO The cleaning for partition __consumer_offsets-27 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,983] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,983] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,983] INFO The cleaning for partition __consumer_offsets-39 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,983] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,983] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,983] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,984] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:06,984] INFO The cleaning for partition __consumer_offsets-6 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,984] INFO The cleaning for partition __consumer_offsets-35 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,984] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,986] INFO The cleaning for partition __consumer_offsets-2 is aborted and paused (kafka.log.LogManager)
[2023-08-21 06:56:06,986] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,986] INFO Cleaning for partition __consumer_offsets-2 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,986] INFO Cleaning for partition __consumer_offsets-22 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,987] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,987] INFO Cleaning for partition __consumer_offsets-35 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,987] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,988] INFO Cleaning for partition __consumer_offsets-6 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,988] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,988] INFO Cleaning for partition __consumer_offsets-39 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,988] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,988] INFO Cleaning for partition __consumer_offsets-27 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:06,989] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-21 06:56:06,989] INFO Cleaning for partition __consumer_offsets-31 is resumed (kafka.log.LogManager)
[2023-08-21 06:56:07,006] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600,confluent.placement.constraints ->  (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:07,034] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-7-4-1-1 in state PreparingRebalance with old generation 0 (__consumer_offsets-5) (reason: Adding new member _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-6-consumer-e000286f-9ea2-467d-92c9-aceff73fa1de with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:07,157] INFO [GroupCoordinator 1]: Stabilized group _confluent-controlcenter-7-4-1-1 generation 1 (__consumer_offsets-5) with 2 members (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:07,245] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in CompletingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-7-consumer-6d483566-e92d-4dd5-8cc8-d2dc4a36a59b and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:07,252] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in CompletingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-9-consumer-bc1f1d23-ed0a-4b70-b875-f3c300d04fc9 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:07,257] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in CompletingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-13-consumer-4e3a947b-68a7-4e8b-9150-583df58dcbdf and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:07,258] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in CompletingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-8-consumer-0e7c4e92-da9f-440d-b541-38f09c5114ef and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:07,284] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-7-4-1-1 in state PreparingRebalance with old generation 1 (__consumer_offsets-5) (reason: Adding new member _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-14-consumer-21a6f8fa-8e6a-4960-a907-bfaf13b19e2a with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:07,428] INFO [GroupCoordinator 1]: Stabilized group _confluent-controlcenter-7-4-1-1 generation 2 (__consumer_offsets-5) with 7 members (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:07,450] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1-command in Empty state. Created a new member id _confluent-controlcenter-7-4-1-1-command-85ba3b0b-9cfe-4910-a842-4e862f41a62e-StreamThread-1-consumer-6bfa35a3-7590-431a-84ac-ca8df9f1c301 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:07,459] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-7-4-1-1-command in state PreparingRebalance with old generation 0 (__consumer_offsets-23) (reason: Adding new member _confluent-controlcenter-7-4-1-1-command-85ba3b0b-9cfe-4910-a842-4e862f41a62e-StreamThread-1-consumer-6bfa35a3-7590-431a-84ac-ca8df9f1c301 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:07,461] INFO [GroupCoordinator 1]: Stabilized group _confluent-controlcenter-7-4-1-1-command generation 1 (__consumer_offsets-23) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:07,595] INFO [GroupCoordinator 1]: Assignment received from leader _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-6-consumer-e000286f-9ea2-467d-92c9-aceff73fa1de for group _confluent-controlcenter-7-4-1-1 for generation 2. The group has 7 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:07,646] INFO Connection id 172.20.0.2:19092-172.20.0.6:33684-10 disconnected while trying to send response for request 1031 (kafka.network.Processor)
[2023-08-21 06:56:07,693] INFO [GroupCoordinator 1]: Assignment received from leader _confluent-controlcenter-7-4-1-1-command-85ba3b0b-9cfe-4910-a842-4e862f41a62e-StreamThread-1-consumer-6bfa35a3-7590-431a-84ac-ca8df9f1c301 for group _confluent-controlcenter-7-4-1-1-command for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:07,805] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-7-4-1-1 in state PreparingRebalance with old generation 2 (__consumer_offsets-5) (reason: Removing member _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-8-consumer-0e7c4e92-da9f-440d-b541-38f09c5114ef on LeaveGroup; client reason: the consumer unsubscribed from all topics) (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:07,819] INFO [GroupCoordinator 1]: Member MemberMetadata(memberId=_confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-8-consumer-0e7c4e92-da9f-440d-b541-38f09c5114ef, groupInstanceId=None, clientId=_confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-8-consumer, clientHost=/172.20.0.6, sessionTimeoutMs=60000, rebalanceTimeoutMs=21600000, supportedProtocols=List(stream)) has left group _confluent-controlcenter-7-4-1-1 through explicit `LeaveGroup`; client reason: the consumer unsubscribed from all topics (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:07,823] INFO [GroupCoordinator 1]: Member MemberMetadata(memberId=_confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-7-consumer-6d483566-e92d-4dd5-8cc8-d2dc4a36a59b, groupInstanceId=None, clientId=_confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-7-consumer, clientHost=/172.20.0.6, sessionTimeoutMs=60000, rebalanceTimeoutMs=21600000, supportedProtocols=List(stream)) has left group _confluent-controlcenter-7-4-1-1 through explicit `LeaveGroup`; client reason: the consumer unsubscribed from all topics (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:07,826] INFO [GroupCoordinator 1]: Member MemberMetadata(memberId=_confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-9-consumer-bc1f1d23-ed0a-4b70-b875-f3c300d04fc9, groupInstanceId=None, clientId=_confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-9-consumer, clientHost=/172.20.0.6, sessionTimeoutMs=60000, rebalanceTimeoutMs=21600000, supportedProtocols=List(stream)) has left group _confluent-controlcenter-7-4-1-1 through explicit `LeaveGroup`; client reason: the consumer unsubscribed from all topics (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:07,853] INFO [GroupCoordinator 1]: Member MemberMetadata(memberId=_confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-6-consumer-e000286f-9ea2-467d-92c9-aceff73fa1de, groupInstanceId=None, clientId=_confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-6-consumer, clientHost=/172.20.0.6, sessionTimeoutMs=60000, rebalanceTimeoutMs=21600000, supportedProtocols=List(stream)) has left group _confluent-controlcenter-7-4-1-1 through explicit `LeaveGroup`; client reason: the consumer unsubscribed from all topics (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:07,871] INFO [GroupCoordinator 1]: Member MemberMetadata(memberId=_confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-13-consumer-4e3a947b-68a7-4e8b-9150-583df58dcbdf, groupInstanceId=None, clientId=_confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-13-consumer, clientHost=/172.20.0.6, sessionTimeoutMs=60000, rebalanceTimeoutMs=21600000, supportedProtocols=List(stream)) has left group _confluent-controlcenter-7-4-1-1 through explicit `LeaveGroup`; client reason: the consumer unsubscribed from all topics (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:07,884] INFO [GroupCoordinator 1]: Member MemberMetadata(memberId=_confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-14-consumer-21a6f8fa-8e6a-4960-a907-bfaf13b19e2a, groupInstanceId=None, clientId=_confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-14-consumer, clientHost=/172.20.0.6, sessionTimeoutMs=60000, rebalanceTimeoutMs=21600000, supportedProtocols=List(stream)) has left group _confluent-controlcenter-7-4-1-1 through explicit `LeaveGroup`; client reason: the consumer unsubscribed from all topics (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:07,892] INFO [GroupCoordinator 1]: Group _confluent-controlcenter-7-4-1-1 with generation 3 is now empty (__consumer_offsets-5) (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:07,895] INFO [GroupCoordinator 1]: Member MemberMetadata(memberId=_confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-15-consumer-90ec9e84-5e9e-4cc9-a556-e22919f85349, groupInstanceId=None, clientId=_confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-15-consumer, clientHost=/172.20.0.6, sessionTimeoutMs=60000, rebalanceTimeoutMs=21600000, supportedProtocols=List(stream)) has left group _confluent-controlcenter-7-4-1-1 through explicit `LeaveGroup`; client reason: the consumer unsubscribed from all topics (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:09,607] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:09,612] INFO [MergedLog partition=_schemas-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:09,612] INFO Created log for partition _schemas-0 in /etc/confluent/kraft-combined-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:56:09,613] INFO [Partition _schemas-0 broker=1] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2023-08-21 06:56:09,614] INFO [Partition _schemas-0 broker=1] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:09,614] INFO Setting topicIdPartition vZQenMxGRBSE8IXAkK-MxQ:_schemas-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:09,614] INFO [MergedLog partition=_schemas-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _schemas-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:09,614] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:10,192] INFO [BrokerServer id=1] Skipping durability audit instantiation (kafka.server.BrokerServer)
[2023-08-21 06:56:10,194] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-d431a2c6-d4a5-4dc2-8132-07758275b09c and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:10,201] INFO [GroupCoordinator 1]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 0 (__consumer_offsets-29) (reason: Adding new member sr-1-d431a2c6-d4a5-4dc2-8132-07758275b09c with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:10,202] INFO [GroupCoordinator 1]: Stabilized group schema-registry generation 1 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:10,222] INFO [GroupCoordinator 1]: Assignment received from leader sr-1-d431a2c6-d4a5-4dc2-8132-07758275b09c for group schema-registry for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:11,179] INFO Starting delay for broker load metric (kafka.metrics.BrokerLoad)
[2023-08-21 06:56:11,181] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2023-08-21 06:56:11,182] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer)
[2023-08-21 06:56:19,734] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-metrics-11, _confluent-metrics-9, _confluent-metrics-10, _confluent-metrics-7, _confluent-metrics-8, _confluent-metrics-5, _confluent-metrics-6, _confluent-metrics-3, _confluent-metrics-4, _confluent-metrics-1, _confluent-metrics-2, _confluent-metrics-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:19,789] INFO [MergedLog partition=_confluent-metrics-11, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:19,790] INFO Created log for partition _confluent-metrics-11 in /etc/confluent/kraft-combined-logs/_confluent-metrics-11 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:56:19,792] INFO [Partition _confluent-metrics-11 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-11 (kafka.cluster.Partition)
[2023-08-21 06:56:19,793] INFO [Partition _confluent-metrics-11 broker=1] Log loaded for partition _confluent-metrics-11 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:19,793] INFO Setting topicIdPartition MFcxifMpQc-oEuiQBPEiZw:_confluent-metrics-11 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:19,793] INFO [MergedLog partition=_confluent-metrics-11, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:19,798] INFO [MergedLog partition=_confluent-metrics-9, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:19,799] INFO Created log for partition _confluent-metrics-9 in /etc/confluent/kraft-combined-logs/_confluent-metrics-9 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:56:19,799] INFO [Partition _confluent-metrics-9 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-9 (kafka.cluster.Partition)
[2023-08-21 06:56:19,799] INFO [Partition _confluent-metrics-9 broker=1] Log loaded for partition _confluent-metrics-9 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:19,800] INFO Setting topicIdPartition MFcxifMpQc-oEuiQBPEiZw:_confluent-metrics-9 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:19,801] INFO [MergedLog partition=_confluent-metrics-9, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:19,804] INFO [MergedLog partition=_confluent-metrics-10, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:19,805] INFO Created log for partition _confluent-metrics-10 in /etc/confluent/kraft-combined-logs/_confluent-metrics-10 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:56:19,806] INFO [Partition _confluent-metrics-10 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-10 (kafka.cluster.Partition)
[2023-08-21 06:56:19,806] INFO [Partition _confluent-metrics-10 broker=1] Log loaded for partition _confluent-metrics-10 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:19,806] INFO Setting topicIdPartition MFcxifMpQc-oEuiQBPEiZw:_confluent-metrics-10 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:19,807] INFO [MergedLog partition=_confluent-metrics-10, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:19,813] INFO [MergedLog partition=_confluent-metrics-7, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:19,824] INFO Created log for partition _confluent-metrics-7 in /etc/confluent/kraft-combined-logs/_confluent-metrics-7 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:56:19,824] INFO [Partition _confluent-metrics-7 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-7 (kafka.cluster.Partition)
[2023-08-21 06:56:19,824] INFO [Partition _confluent-metrics-7 broker=1] Log loaded for partition _confluent-metrics-7 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:19,824] INFO Setting topicIdPartition MFcxifMpQc-oEuiQBPEiZw:_confluent-metrics-7 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:19,825] INFO [MergedLog partition=_confluent-metrics-7, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:19,828] INFO [MergedLog partition=_confluent-metrics-8, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:19,829] INFO Created log for partition _confluent-metrics-8 in /etc/confluent/kraft-combined-logs/_confluent-metrics-8 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:56:19,829] INFO [Partition _confluent-metrics-8 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-8 (kafka.cluster.Partition)
[2023-08-21 06:56:19,829] INFO [Partition _confluent-metrics-8 broker=1] Log loaded for partition _confluent-metrics-8 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:19,829] INFO Setting topicIdPartition MFcxifMpQc-oEuiQBPEiZw:_confluent-metrics-8 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:19,829] INFO [MergedLog partition=_confluent-metrics-8, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:19,833] INFO [MergedLog partition=_confluent-metrics-5, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:19,834] INFO Created log for partition _confluent-metrics-5 in /etc/confluent/kraft-combined-logs/_confluent-metrics-5 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:56:19,837] INFO [Partition _confluent-metrics-5 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-5 (kafka.cluster.Partition)
[2023-08-21 06:56:19,837] INFO [Partition _confluent-metrics-5 broker=1] Log loaded for partition _confluent-metrics-5 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:19,837] INFO Setting topicIdPartition MFcxifMpQc-oEuiQBPEiZw:_confluent-metrics-5 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:19,838] INFO [MergedLog partition=_confluent-metrics-5, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:19,841] INFO [MergedLog partition=_confluent-metrics-6, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:19,842] INFO Created log for partition _confluent-metrics-6 in /etc/confluent/kraft-combined-logs/_confluent-metrics-6 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:56:19,843] INFO [Partition _confluent-metrics-6 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-6 (kafka.cluster.Partition)
[2023-08-21 06:56:19,843] INFO [Partition _confluent-metrics-6 broker=1] Log loaded for partition _confluent-metrics-6 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:19,843] INFO Setting topicIdPartition MFcxifMpQc-oEuiQBPEiZw:_confluent-metrics-6 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:19,844] INFO [MergedLog partition=_confluent-metrics-6, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:19,848] INFO [MergedLog partition=_confluent-metrics-3, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:19,850] INFO Created log for partition _confluent-metrics-3 in /etc/confluent/kraft-combined-logs/_confluent-metrics-3 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:56:19,850] INFO [Partition _confluent-metrics-3 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-3 (kafka.cluster.Partition)
[2023-08-21 06:56:19,850] INFO [Partition _confluent-metrics-3 broker=1] Log loaded for partition _confluent-metrics-3 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:19,850] INFO Setting topicIdPartition MFcxifMpQc-oEuiQBPEiZw:_confluent-metrics-3 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:19,850] INFO [MergedLog partition=_confluent-metrics-3, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:19,854] INFO [MergedLog partition=_confluent-metrics-4, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:19,855] INFO Created log for partition _confluent-metrics-4 in /etc/confluent/kraft-combined-logs/_confluent-metrics-4 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:56:19,856] INFO [Partition _confluent-metrics-4 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-4 (kafka.cluster.Partition)
[2023-08-21 06:56:19,856] INFO [Partition _confluent-metrics-4 broker=1] Log loaded for partition _confluent-metrics-4 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:19,856] INFO Setting topicIdPartition MFcxifMpQc-oEuiQBPEiZw:_confluent-metrics-4 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:19,856] INFO [MergedLog partition=_confluent-metrics-4, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:19,859] INFO [MergedLog partition=_confluent-metrics-1, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:19,860] INFO Created log for partition _confluent-metrics-1 in /etc/confluent/kraft-combined-logs/_confluent-metrics-1 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:56:19,860] INFO [Partition _confluent-metrics-1 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-1 (kafka.cluster.Partition)
[2023-08-21 06:56:19,860] INFO [Partition _confluent-metrics-1 broker=1] Log loaded for partition _confluent-metrics-1 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:19,860] INFO Setting topicIdPartition MFcxifMpQc-oEuiQBPEiZw:_confluent-metrics-1 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:19,860] INFO [MergedLog partition=_confluent-metrics-1, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:19,863] INFO [MergedLog partition=_confluent-metrics-2, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:19,863] INFO Created log for partition _confluent-metrics-2 in /etc/confluent/kraft-combined-logs/_confluent-metrics-2 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:56:19,864] INFO [Partition _confluent-metrics-2 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-2 (kafka.cluster.Partition)
[2023-08-21 06:56:19,864] INFO [Partition _confluent-metrics-2 broker=1] Log loaded for partition _confluent-metrics-2 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:19,864] INFO Setting topicIdPartition MFcxifMpQc-oEuiQBPEiZw:_confluent-metrics-2 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:19,865] INFO [MergedLog partition=_confluent-metrics-2, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:19,869] INFO [MergedLog partition=_confluent-metrics-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:19,869] INFO Created log for partition _confluent-metrics-0 in /etc/confluent/kraft-combined-logs/_confluent-metrics-0 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:56:19,870] INFO [Partition _confluent-metrics-0 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-0 (kafka.cluster.Partition)
[2023-08-21 06:56:19,870] INFO [Partition _confluent-metrics-0 broker=1] Log loaded for partition _confluent-metrics-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:19,870] INFO Setting topicIdPartition MFcxifMpQc-oEuiQBPEiZw:_confluent-metrics-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:19,870] INFO [MergedLog partition=_confluent-metrics-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:19,871] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-metrics with new configuration : max.message.bytes -> 10485760,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,retention.ms -> 259200000,segment.ms -> 14400000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:20,545] ERROR [KafkaApi-1] Unexpected error handling request RequestHeader(apiKey=INIT_PRODUCER_ID, apiVersion=4, clientId=confluent-control-center-heartbeat-sender-1, correlationId=2, headerVersion=2) -- InitProducerIdRequestData(transactionalId=null, transactionTimeoutMs=2147483647, producerId=-1, producerEpoch=-1) with context RequestContext(header=RequestHeader(apiKey=INIT_PRODUCER_ID, apiVersion=4, clientId=confluent-control-center-heartbeat-sender-1, correlationId=2, headerVersion=2), connectionId='172.20.0.2:19092-172.20.0.6:39550-75', clientAddress=/172.20.0.6, principal=User:ANONYMOUS, listenerName=ListenerName(INTERNAL), securityProtocol=PLAINTEXT, clientInformation=ClientInformation(softwareName=apache-kafka-java, softwareVersion=7.4.1-ce), fromPrivilegedListener=true, principalSerde=Optional[org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder@50f2eb06]) (kafka.server.KafkaApis)
org.apache.kafka.common.errors.CoordinatorLoadInProgressException: Timed out waiting for next producer ID block
[2023-08-21 06:56:24,362] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:24,365] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:24,366] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:24,367] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,367] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,367] INFO Setting topicIdPartition nxfB9753Siq7811JX4wZew:_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:24,368] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:24,368] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:24,396] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:24,400] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:24,401] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:24,402] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,402] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,403] INFO Setting topicIdPartition 4-tv03zJTfuu3oVQrk1gYw:_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:24,403] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:24,403] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:24,428] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:24,434] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:24,434] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:24,437] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,437] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,437] INFO Setting topicIdPartition fd6Swz16RyqxbN6bCmNVSg:_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:24,438] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:24,439] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:24,465] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:24,470] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:24,470] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:24,471] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,471] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,471] INFO Setting topicIdPartition O4S_bVuKThGObCAjVWbHVw:_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:24,472] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:24,472] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:24,498] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:24,502] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:24,502] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:24,504] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,504] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,504] INFO Setting topicIdPartition Iw-XjEweRSmiJdWLysIDgQ:_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:24,504] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:24,505] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:24,529] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:24,533] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:24,534] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:24,534] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,534] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,535] INFO Setting topicIdPartition ABzsU6xnTqeo1uVc3iYPRg:_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:24,535] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:24,536] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:24,560] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:24,564] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:24,565] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:24,565] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,566] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,566] INFO Setting topicIdPartition UGyqflgBSo-6V2GZDbQzQg:_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:24,567] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:24,567] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:24,591] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:24,595] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:24,596] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:24,597] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,597] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,597] INFO Setting topicIdPartition Kz5mYOscSF2Sf68XK10YDA:_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:24,597] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:24,598] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:24,621] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:24,626] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:24,626] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:24,627] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,628] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,628] INFO Setting topicIdPartition QJmTu4oTTMObDr0dvOo0ag:_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:24,628] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:24,629] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:24,653] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:24,658] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:24,659] INFO Created log for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:24,660] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,660] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,661] INFO Setting topicIdPartition n62QsZ1wRHOq_xwGtKJt6g:_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:24,661] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:24,661] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:24,685] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:24,690] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:24,691] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:24,692] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,692] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,693] INFO Setting topicIdPartition 2iPDd0jORQGnHizt2Qejpw:_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:24,693] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:24,693] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:24,718] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:24,722] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:24,723] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:24,724] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,724] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,725] INFO Setting topicIdPartition TYoWlJIwRUq-6ViZt21ENg:_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:24,725] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:24,725] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:24,748] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:24,766] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:24,767] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-21 06:56:24,767] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,768] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,768] INFO Setting topicIdPartition euyeOVFuQDerNm-Ibth2oQ:_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:24,768] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:24,769] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:24,792] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:24,797] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:24,797] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:24,798] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,798] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,798] INFO Setting topicIdPartition B85aDzQ7QTiTsS6Iaxq5yg:_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:24,799] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:24,799] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:24,823] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:24,827] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:24,828] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:24,828] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,829] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,829] INFO Setting topicIdPartition uagWa-RjRKa4yBUW0lpteg:_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:24,829] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:24,830] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:24,854] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:24,858] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:24,858] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-21 06:56:24,859] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,860] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,860] INFO Setting topicIdPartition 68PsxylXRgqoL7TLLeXfaQ:_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:24,860] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:24,861] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:24,885] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:24,891] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:24,892] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-21 06:56:24,893] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,893] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,893] INFO Setting topicIdPartition HloKkiA7RwOlk6u39SQ57w:_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:24,893] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:24,894] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:24,918] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:24,922] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:24,924] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:24,925] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,925] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,925] INFO Setting topicIdPartition Uy6N_mx-Tb6P8hWVf8iiHg:_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:24,925] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:24,926] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:24,950] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:24,955] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:24,956] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:24,957] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,957] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,957] INFO Setting topicIdPartition dfCXcQNBTcO7xCkkGMgaRw:_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:24,957] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:24,958] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:24,982] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:24,987] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:24,988] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:24,988] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,989] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:24,989] INFO Setting topicIdPartition ea4MX0ePRtaSKFh3Ms0lwg:_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:24,989] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:24,989] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:25,013] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-cluster-rekey-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:25,018] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-cluster-rekey-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:25,018] INFO Created log for partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-cluster-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
[2023-08-21 06:56:25,019] INFO [Partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,019] INFO [Partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,020] INFO Setting topicIdPartition gBypNUbYRQmntuhOzmVtag:_confluent-controlcenter-7-4-1-1-cluster-rekey-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:25,020] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-cluster-rekey-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-cluster-rekey-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:25,020] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-cluster-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:25,043] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:25,046] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:25,047] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:25,048] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,048] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,049] INFO Setting topicIdPartition xtqY61gcRn-BE3RUKlhu9A:_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:25,049] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:25,049] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:25,073] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:25,077] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:25,078] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-21 06:56:25,078] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,079] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,079] INFO Setting topicIdPartition jYbIvrCsQeuFY4ZpWXv8DQ:_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:25,079] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:25,080] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:25,103] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:25,108] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:25,109] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:25,109] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,110] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,110] INFO Setting topicIdPartition m_Hlgc8ZRbS3ZtDwlA9PPQ:_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:25,110] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:25,111] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:25,134] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:25,138] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:25,139] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:25,140] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,141] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,141] INFO Setting topicIdPartition _CxfwpTZSsmUl4UyCW29Ww:_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:25,141] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:25,141] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:25,165] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:25,169] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:25,170] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:25,171] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,171] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,171] INFO Setting topicIdPartition yGjcAHCvSV-Riw7hQplJYA:_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:25,172] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:25,172] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:25,197] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:25,201] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:25,203] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-21 06:56:25,204] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,204] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,204] INFO Setting topicIdPartition VF4WjFUjQD-1Jq9tBfSlxQ:_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:25,204] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:25,205] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:25,229] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:25,232] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:25,233] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:25,233] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,234] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,234] INFO Setting topicIdPartition cHT13TwOTSCGinBNtiFZXA:_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:25,234] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:25,235] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:25,260] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:25,263] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:25,264] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:25,264] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,265] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,265] INFO Setting topicIdPartition 31LRLx09R02U4jfLKtKC9w:_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:25,265] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:25,265] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:25,291] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:25,296] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:25,296] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:25,297] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,297] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,297] INFO Setting topicIdPartition DgFeYDUUSKKp3A3elinR-Q:_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:25,297] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:25,297] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:25,322] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:25,326] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:25,326] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
[2023-08-21 06:56:25,328] INFO [Partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,328] INFO [Partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,328] INFO Setting topicIdPartition OAWvGcM4Rfqmi0Lg6fbxXw:_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:25,328] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:25,329] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:25,353] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:25,357] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:25,357] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-21 06:56:25,358] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,359] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,359] INFO Setting topicIdPartition bfr-DnH6Q6qridDlaCuBOA:_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:25,359] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:25,360] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:25,385] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:25,389] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:25,390] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:25,391] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,391] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,392] INFO Setting topicIdPartition 0ktwPY7mRLCo2x48WXonuQ:_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:25,392] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:25,392] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:25,416] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:25,420] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:25,421] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:25,422] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,422] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,423] INFO Setting topicIdPartition UX7Ga65TRg2M8NeCGp-r8g:_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:25,423] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:25,423] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:25,448] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:25,452] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:25,453] INFO Created log for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:25,453] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,454] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,454] INFO Setting topicIdPartition 6bBQ0tVDS--DUomXg6hOtA:_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:25,454] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:25,455] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:25,478] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:25,483] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:25,484] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-21 06:56:25,485] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,485] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,485] INFO Setting topicIdPartition yN7Hgt7eRK2h_-q7e7s-eg:_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:25,488] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:25,489] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:25,511] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:25,516] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:25,517] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-21 06:56:25,518] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,518] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,519] INFO Setting topicIdPartition FfycscOfTaKlOBbIAWxGsg:_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:25,519] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:25,519] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:25,542] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:25,545] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:25,546] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-21 06:56:25,547] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,547] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:25,547] INFO Setting topicIdPartition Gq4fkJ8TT-mGHwkKIP4jcg:_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:25,547] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:25,548] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:25,751] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1-command in Stable state. Created a new member id _confluent-controlcenter-7-4-1-1-command-85ba3b0b-9cfe-4910-a842-4e862f41a62e-StreamThread-1-consumer-d6a9e553-9e48-4326-b21d-4002dbf0b27e and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:25,754] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-7-4-1-1-command in state PreparingRebalance with old generation 1 (__consumer_offsets-23) (reason: Adding new member _confluent-controlcenter-7-4-1-1-command-85ba3b0b-9cfe-4910-a842-4e862f41a62e-StreamThread-1-consumer-d6a9e553-9e48-4326-b21d-4002dbf0b27e with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:34,668] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(telemetry-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:34,672] INFO [MergedLog partition=telemetry-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:34,673] INFO Created log for partition telemetry-0 in /etc/confluent/kraft-combined-logs/telemetry-0 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:56:34,674] INFO [Partition telemetry-0 broker=1] No checkpointed highwatermark is found for partition telemetry-0 (kafka.cluster.Partition)
[2023-08-21 06:56:34,674] INFO [Partition telemetry-0 broker=1] Log loaded for partition telemetry-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:34,675] INFO Setting topicIdPartition VU8RH87VTSOjEyoes2niNg:telemetry-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:34,675] INFO [MergedLog partition=telemetry-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for telemetry-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:34,675] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic telemetry with new configuration : max.message.bytes -> 10485760,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,retention.ms -> 259200000,segment.ms -> 14400000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:34,746] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-telemetry-metrics-3, _confluent-telemetry-metrics-4, _confluent-telemetry-metrics-5, _confluent-telemetry-metrics-6, _confluent-telemetry-metrics-7, _confluent-telemetry-metrics-8, _confluent-telemetry-metrics-9, _confluent-telemetry-metrics-10, _confluent-telemetry-metrics-11, _confluent-telemetry-metrics-0, _confluent-telemetry-metrics-1, _confluent-telemetry-metrics-2) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:34,790] INFO [MergedLog partition=_confluent-telemetry-metrics-3, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:34,791] INFO Created log for partition _confluent-telemetry-metrics-3 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-3 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:56:34,792] INFO [Partition _confluent-telemetry-metrics-3 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-3 (kafka.cluster.Partition)
[2023-08-21 06:56:34,792] INFO [Partition _confluent-telemetry-metrics-3 broker=1] Log loaded for partition _confluent-telemetry-metrics-3 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:34,792] INFO Setting topicIdPartition yhYcqGL2Sgi2aj8LWvIotw:_confluent-telemetry-metrics-3 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:34,793] INFO [MergedLog partition=_confluent-telemetry-metrics-3, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:34,797] INFO [MergedLog partition=_confluent-telemetry-metrics-4, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:34,798] INFO Created log for partition _confluent-telemetry-metrics-4 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-4 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:56:34,798] INFO [Partition _confluent-telemetry-metrics-4 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-4 (kafka.cluster.Partition)
[2023-08-21 06:56:34,798] INFO [Partition _confluent-telemetry-metrics-4 broker=1] Log loaded for partition _confluent-telemetry-metrics-4 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:34,798] INFO Setting topicIdPartition yhYcqGL2Sgi2aj8LWvIotw:_confluent-telemetry-metrics-4 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:34,798] INFO [MergedLog partition=_confluent-telemetry-metrics-4, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:34,803] INFO [MergedLog partition=_confluent-telemetry-metrics-5, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:34,804] INFO Created log for partition _confluent-telemetry-metrics-5 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-5 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:56:34,804] INFO [Partition _confluent-telemetry-metrics-5 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-5 (kafka.cluster.Partition)
[2023-08-21 06:56:34,804] INFO [Partition _confluent-telemetry-metrics-5 broker=1] Log loaded for partition _confluent-telemetry-metrics-5 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:34,804] INFO Setting topicIdPartition yhYcqGL2Sgi2aj8LWvIotw:_confluent-telemetry-metrics-5 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:34,804] INFO [MergedLog partition=_confluent-telemetry-metrics-5, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:34,809] INFO [MergedLog partition=_confluent-telemetry-metrics-6, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:34,810] INFO Created log for partition _confluent-telemetry-metrics-6 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-6 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:56:34,811] INFO [Partition _confluent-telemetry-metrics-6 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-6 (kafka.cluster.Partition)
[2023-08-21 06:56:34,811] INFO [Partition _confluent-telemetry-metrics-6 broker=1] Log loaded for partition _confluent-telemetry-metrics-6 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:34,812] INFO Setting topicIdPartition yhYcqGL2Sgi2aj8LWvIotw:_confluent-telemetry-metrics-6 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:34,812] INFO [MergedLog partition=_confluent-telemetry-metrics-6, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:34,814] INFO [MergedLog partition=_confluent-telemetry-metrics-7, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:34,814] INFO Created log for partition _confluent-telemetry-metrics-7 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-7 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:56:34,814] INFO [Partition _confluent-telemetry-metrics-7 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-7 (kafka.cluster.Partition)
[2023-08-21 06:56:34,815] INFO [Partition _confluent-telemetry-metrics-7 broker=1] Log loaded for partition _confluent-telemetry-metrics-7 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:34,815] INFO Setting topicIdPartition yhYcqGL2Sgi2aj8LWvIotw:_confluent-telemetry-metrics-7 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:34,815] INFO [MergedLog partition=_confluent-telemetry-metrics-7, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:34,819] INFO [MergedLog partition=_confluent-telemetry-metrics-8, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:34,819] INFO Created log for partition _confluent-telemetry-metrics-8 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-8 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:56:34,819] INFO [Partition _confluent-telemetry-metrics-8 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-8 (kafka.cluster.Partition)
[2023-08-21 06:56:34,819] INFO [Partition _confluent-telemetry-metrics-8 broker=1] Log loaded for partition _confluent-telemetry-metrics-8 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:34,820] INFO Setting topicIdPartition yhYcqGL2Sgi2aj8LWvIotw:_confluent-telemetry-metrics-8 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:34,822] INFO [MergedLog partition=_confluent-telemetry-metrics-8, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:34,826] INFO [MergedLog partition=_confluent-telemetry-metrics-9, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:34,827] INFO Created log for partition _confluent-telemetry-metrics-9 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-9 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:56:34,827] INFO [Partition _confluent-telemetry-metrics-9 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-9 (kafka.cluster.Partition)
[2023-08-21 06:56:34,827] INFO [Partition _confluent-telemetry-metrics-9 broker=1] Log loaded for partition _confluent-telemetry-metrics-9 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:34,827] INFO Setting topicIdPartition yhYcqGL2Sgi2aj8LWvIotw:_confluent-telemetry-metrics-9 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:34,828] INFO [MergedLog partition=_confluent-telemetry-metrics-9, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:34,831] INFO [MergedLog partition=_confluent-telemetry-metrics-10, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:34,832] INFO Created log for partition _confluent-telemetry-metrics-10 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-10 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:56:34,833] INFO [Partition _confluent-telemetry-metrics-10 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-10 (kafka.cluster.Partition)
[2023-08-21 06:56:34,833] INFO [Partition _confluent-telemetry-metrics-10 broker=1] Log loaded for partition _confluent-telemetry-metrics-10 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:34,833] INFO Setting topicIdPartition yhYcqGL2Sgi2aj8LWvIotw:_confluent-telemetry-metrics-10 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:34,833] INFO [MergedLog partition=_confluent-telemetry-metrics-10, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:34,840] INFO [MergedLog partition=_confluent-telemetry-metrics-11, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:34,841] INFO Created log for partition _confluent-telemetry-metrics-11 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-11 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:56:34,842] INFO [Partition _confluent-telemetry-metrics-11 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-11 (kafka.cluster.Partition)
[2023-08-21 06:56:34,842] INFO [Partition _confluent-telemetry-metrics-11 broker=1] Log loaded for partition _confluent-telemetry-metrics-11 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:34,842] INFO Setting topicIdPartition yhYcqGL2Sgi2aj8LWvIotw:_confluent-telemetry-metrics-11 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:34,842] INFO [MergedLog partition=_confluent-telemetry-metrics-11, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:34,845] INFO [MergedLog partition=_confluent-telemetry-metrics-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:34,846] INFO Created log for partition _confluent-telemetry-metrics-0 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-0 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:56:34,846] INFO [Partition _confluent-telemetry-metrics-0 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-0 (kafka.cluster.Partition)
[2023-08-21 06:56:34,846] INFO [Partition _confluent-telemetry-metrics-0 broker=1] Log loaded for partition _confluent-telemetry-metrics-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:34,846] INFO Setting topicIdPartition yhYcqGL2Sgi2aj8LWvIotw:_confluent-telemetry-metrics-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:34,846] INFO [MergedLog partition=_confluent-telemetry-metrics-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:34,854] INFO [MergedLog partition=_confluent-telemetry-metrics-1, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:34,855] INFO Created log for partition _confluent-telemetry-metrics-1 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-1 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:56:34,855] INFO [Partition _confluent-telemetry-metrics-1 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-1 (kafka.cluster.Partition)
[2023-08-21 06:56:34,855] INFO [Partition _confluent-telemetry-metrics-1 broker=1] Log loaded for partition _confluent-telemetry-metrics-1 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:34,855] INFO Setting topicIdPartition yhYcqGL2Sgi2aj8LWvIotw:_confluent-telemetry-metrics-1 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:34,855] INFO [MergedLog partition=_confluent-telemetry-metrics-1, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:34,866] INFO [MergedLog partition=_confluent-telemetry-metrics-2, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:34,867] INFO Created log for partition _confluent-telemetry-metrics-2 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-2 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-21 06:56:34,867] INFO [Partition _confluent-telemetry-metrics-2 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-2 (kafka.cluster.Partition)
[2023-08-21 06:56:34,867] INFO [Partition _confluent-telemetry-metrics-2 broker=1] Log loaded for partition _confluent-telemetry-metrics-2 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:34,867] INFO Setting topicIdPartition yhYcqGL2Sgi2aj8LWvIotw:_confluent-telemetry-metrics-2 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:34,867] INFO [MergedLog partition=_confluent-telemetry-metrics-2, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:34,867] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-telemetry-metrics with new configuration : max.message.bytes -> 10485760,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,retention.ms -> 259200000,segment.ms -> 14400000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:39,083] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent_balancer_api_state-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:56:39,087] INFO [MergedLog partition=_confluent_balancer_api_state-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:56:39,088] INFO Created log for partition _confluent_balancer_api_state-0 in /etc/confluent/kraft-combined-logs/_confluent_balancer_api_state-0 with properties {cleanup.policy=compact, retention.ms=-1} (kafka.log.LogManager)
[2023-08-21 06:56:39,090] INFO [Partition _confluent_balancer_api_state-0 broker=1] No checkpointed highwatermark is found for partition _confluent_balancer_api_state-0 (kafka.cluster.Partition)
[2023-08-21 06:56:39,090] INFO [Partition _confluent_balancer_api_state-0 broker=1] Log loaded for partition _confluent_balancer_api_state-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:56:39,090] INFO Setting topicIdPartition JUbZdqMcTwKM0SN_tsTp7w:_confluent_balancer_api_state-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:56:39,090] INFO [MergedLog partition=_confluent_balancer_api_state-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent_balancer_api_state-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:56:39,090] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent_balancer_api_state with new configuration : cleanup.policy -> compact,retention.ms -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:56:40,487] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group ConfluentTelemetryReporterSampler-5706900102103725631 in Empty state. Created a new member id kafka-cruise-control-ac29ca2c-c6fd-41cc-9ec6-b316844fbb7d and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:40,489] INFO [GroupCoordinator 1]: Preparing to rebalance group ConfluentTelemetryReporterSampler-5706900102103725631 in state PreparingRebalance with old generation 0 (__consumer_offsets-23) (reason: Adding new member kafka-cruise-control-ac29ca2c-c6fd-41cc-9ec6-b316844fbb7d with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:40,490] INFO [GroupCoordinator 1]: Stabilized group ConfluentTelemetryReporterSampler-5706900102103725631 generation 1 (__consumer_offsets-23) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:56:40,497] INFO [GroupCoordinator 1]: Assignment received from leader kafka-cruise-control-ac29ca2c-c6fd-41cc-9ec6-b316844fbb7d for group ConfluentTelemetryReporterSampler-5706900102103725631 for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:07,128] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(connect-offsets-10, connect-offsets-8, connect-offsets-14, connect-offsets-12, connect-offsets-2, connect-offsets-0, connect-offsets-6, connect-offsets-4, connect-offsets-24, connect-offsets-18, connect-offsets-16, connect-offsets-22, connect-offsets-20, connect-offsets-9, connect-offsets-7, connect-offsets-13, connect-offsets-11, connect-offsets-1, connect-offsets-5, connect-offsets-3, connect-offsets-23, connect-offsets-17, connect-offsets-15, connect-offsets-21, connect-offsets-19) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:57:07,146] INFO [MergedLog partition=connect-offsets-10, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,147] INFO Created log for partition connect-offsets-10 in /etc/confluent/kraft-combined-logs/connect-offsets-10 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,148] INFO [Partition connect-offsets-10 broker=1] No checkpointed highwatermark is found for partition connect-offsets-10 (kafka.cluster.Partition)
[2023-08-21 06:57:07,148] INFO [Partition connect-offsets-10 broker=1] Log loaded for partition connect-offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,148] INFO Setting topicIdPartition Wsi0raXERfi_RSfgo1c9uQ:connect-offsets-10 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,148] INFO [MergedLog partition=connect-offsets-10, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,151] INFO [MergedLog partition=connect-offsets-8, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,151] INFO Created log for partition connect-offsets-8 in /etc/confluent/kraft-combined-logs/connect-offsets-8 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,151] INFO [Partition connect-offsets-8 broker=1] No checkpointed highwatermark is found for partition connect-offsets-8 (kafka.cluster.Partition)
[2023-08-21 06:57:07,151] INFO [Partition connect-offsets-8 broker=1] Log loaded for partition connect-offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,152] INFO Setting topicIdPartition Wsi0raXERfi_RSfgo1c9uQ:connect-offsets-8 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,153] INFO [MergedLog partition=connect-offsets-8, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,155] INFO [MergedLog partition=connect-offsets-14, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,155] INFO Created log for partition connect-offsets-14 in /etc/confluent/kraft-combined-logs/connect-offsets-14 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,155] INFO [Partition connect-offsets-14 broker=1] No checkpointed highwatermark is found for partition connect-offsets-14 (kafka.cluster.Partition)
[2023-08-21 06:57:07,155] INFO [Partition connect-offsets-14 broker=1] Log loaded for partition connect-offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,155] INFO Setting topicIdPartition Wsi0raXERfi_RSfgo1c9uQ:connect-offsets-14 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,155] INFO [MergedLog partition=connect-offsets-14, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-14 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,157] INFO [MergedLog partition=connect-offsets-12, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,158] INFO Created log for partition connect-offsets-12 in /etc/confluent/kraft-combined-logs/connect-offsets-12 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,158] INFO [Partition connect-offsets-12 broker=1] No checkpointed highwatermark is found for partition connect-offsets-12 (kafka.cluster.Partition)
[2023-08-21 06:57:07,158] INFO [Partition connect-offsets-12 broker=1] Log loaded for partition connect-offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,158] INFO Setting topicIdPartition Wsi0raXERfi_RSfgo1c9uQ:connect-offsets-12 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,158] INFO [MergedLog partition=connect-offsets-12, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-12 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,160] INFO [MergedLog partition=connect-offsets-2, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,161] INFO Created log for partition connect-offsets-2 in /etc/confluent/kraft-combined-logs/connect-offsets-2 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,161] INFO [Partition connect-offsets-2 broker=1] No checkpointed highwatermark is found for partition connect-offsets-2 (kafka.cluster.Partition)
[2023-08-21 06:57:07,161] INFO [Partition connect-offsets-2 broker=1] Log loaded for partition connect-offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,161] INFO Setting topicIdPartition Wsi0raXERfi_RSfgo1c9uQ:connect-offsets-2 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,161] INFO [MergedLog partition=connect-offsets-2, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,164] INFO [MergedLog partition=connect-offsets-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,165] INFO Created log for partition connect-offsets-0 in /etc/confluent/kraft-combined-logs/connect-offsets-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,165] INFO [Partition connect-offsets-0 broker=1] No checkpointed highwatermark is found for partition connect-offsets-0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,165] INFO [Partition connect-offsets-0 broker=1] Log loaded for partition connect-offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,165] INFO Setting topicIdPartition Wsi0raXERfi_RSfgo1c9uQ:connect-offsets-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,165] INFO [MergedLog partition=connect-offsets-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,167] INFO [MergedLog partition=connect-offsets-6, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,168] INFO Created log for partition connect-offsets-6 in /etc/confluent/kraft-combined-logs/connect-offsets-6 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,168] INFO [Partition connect-offsets-6 broker=1] No checkpointed highwatermark is found for partition connect-offsets-6 (kafka.cluster.Partition)
[2023-08-21 06:57:07,168] INFO [Partition connect-offsets-6 broker=1] Log loaded for partition connect-offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,168] INFO Setting topicIdPartition Wsi0raXERfi_RSfgo1c9uQ:connect-offsets-6 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,168] INFO [MergedLog partition=connect-offsets-6, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,171] INFO [MergedLog partition=connect-offsets-4, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,171] INFO Created log for partition connect-offsets-4 in /etc/confluent/kraft-combined-logs/connect-offsets-4 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,171] INFO [Partition connect-offsets-4 broker=1] No checkpointed highwatermark is found for partition connect-offsets-4 (kafka.cluster.Partition)
[2023-08-21 06:57:07,171] INFO [Partition connect-offsets-4 broker=1] Log loaded for partition connect-offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,172] INFO Setting topicIdPartition Wsi0raXERfi_RSfgo1c9uQ:connect-offsets-4 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,172] INFO [MergedLog partition=connect-offsets-4, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,174] INFO [MergedLog partition=connect-offsets-24, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,175] INFO Created log for partition connect-offsets-24 in /etc/confluent/kraft-combined-logs/connect-offsets-24 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,175] INFO [Partition connect-offsets-24 broker=1] No checkpointed highwatermark is found for partition connect-offsets-24 (kafka.cluster.Partition)
[2023-08-21 06:57:07,175] INFO [Partition connect-offsets-24 broker=1] Log loaded for partition connect-offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,175] INFO Setting topicIdPartition Wsi0raXERfi_RSfgo1c9uQ:connect-offsets-24 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,175] INFO [MergedLog partition=connect-offsets-24, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-24 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,177] INFO [MergedLog partition=connect-offsets-18, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,177] INFO Created log for partition connect-offsets-18 in /etc/confluent/kraft-combined-logs/connect-offsets-18 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,177] INFO [Partition connect-offsets-18 broker=1] No checkpointed highwatermark is found for partition connect-offsets-18 (kafka.cluster.Partition)
[2023-08-21 06:57:07,177] INFO [Partition connect-offsets-18 broker=1] Log loaded for partition connect-offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,177] INFO Setting topicIdPartition Wsi0raXERfi_RSfgo1c9uQ:connect-offsets-18 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,178] INFO [MergedLog partition=connect-offsets-18, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-18 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,180] INFO [MergedLog partition=connect-offsets-16, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,181] INFO Created log for partition connect-offsets-16 in /etc/confluent/kraft-combined-logs/connect-offsets-16 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,181] INFO [Partition connect-offsets-16 broker=1] No checkpointed highwatermark is found for partition connect-offsets-16 (kafka.cluster.Partition)
[2023-08-21 06:57:07,181] INFO [Partition connect-offsets-16 broker=1] Log loaded for partition connect-offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,181] INFO Setting topicIdPartition Wsi0raXERfi_RSfgo1c9uQ:connect-offsets-16 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,182] INFO [MergedLog partition=connect-offsets-16, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-16 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,184] INFO [MergedLog partition=connect-offsets-22, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,184] INFO Created log for partition connect-offsets-22 in /etc/confluent/kraft-combined-logs/connect-offsets-22 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,184] INFO [Partition connect-offsets-22 broker=1] No checkpointed highwatermark is found for partition connect-offsets-22 (kafka.cluster.Partition)
[2023-08-21 06:57:07,184] INFO [Partition connect-offsets-22 broker=1] Log loaded for partition connect-offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,184] INFO Setting topicIdPartition Wsi0raXERfi_RSfgo1c9uQ:connect-offsets-22 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,184] INFO [MergedLog partition=connect-offsets-22, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-22 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,187] INFO [MergedLog partition=connect-offsets-20, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,187] INFO Created log for partition connect-offsets-20 in /etc/confluent/kraft-combined-logs/connect-offsets-20 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,187] INFO [Partition connect-offsets-20 broker=1] No checkpointed highwatermark is found for partition connect-offsets-20 (kafka.cluster.Partition)
[2023-08-21 06:57:07,188] INFO [Partition connect-offsets-20 broker=1] Log loaded for partition connect-offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,188] INFO Setting topicIdPartition Wsi0raXERfi_RSfgo1c9uQ:connect-offsets-20 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,188] INFO [MergedLog partition=connect-offsets-20, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-20 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,191] INFO [MergedLog partition=connect-offsets-9, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,191] INFO Created log for partition connect-offsets-9 in /etc/confluent/kraft-combined-logs/connect-offsets-9 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,191] INFO [Partition connect-offsets-9 broker=1] No checkpointed highwatermark is found for partition connect-offsets-9 (kafka.cluster.Partition)
[2023-08-21 06:57:07,192] INFO [Partition connect-offsets-9 broker=1] Log loaded for partition connect-offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,192] INFO Setting topicIdPartition Wsi0raXERfi_RSfgo1c9uQ:connect-offsets-9 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,192] INFO [MergedLog partition=connect-offsets-9, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,194] INFO [MergedLog partition=connect-offsets-7, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,194] INFO Created log for partition connect-offsets-7 in /etc/confluent/kraft-combined-logs/connect-offsets-7 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,194] INFO [Partition connect-offsets-7 broker=1] No checkpointed highwatermark is found for partition connect-offsets-7 (kafka.cluster.Partition)
[2023-08-21 06:57:07,194] INFO [Partition connect-offsets-7 broker=1] Log loaded for partition connect-offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,194] INFO Setting topicIdPartition Wsi0raXERfi_RSfgo1c9uQ:connect-offsets-7 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,195] INFO [MergedLog partition=connect-offsets-7, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,198] INFO [MergedLog partition=connect-offsets-13, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,198] INFO Created log for partition connect-offsets-13 in /etc/confluent/kraft-combined-logs/connect-offsets-13 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,198] INFO [Partition connect-offsets-13 broker=1] No checkpointed highwatermark is found for partition connect-offsets-13 (kafka.cluster.Partition)
[2023-08-21 06:57:07,198] INFO [Partition connect-offsets-13 broker=1] Log loaded for partition connect-offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,198] INFO Setting topicIdPartition Wsi0raXERfi_RSfgo1c9uQ:connect-offsets-13 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,199] INFO [MergedLog partition=connect-offsets-13, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-13 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,200] INFO [MergedLog partition=connect-offsets-11, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,201] INFO Created log for partition connect-offsets-11 in /etc/confluent/kraft-combined-logs/connect-offsets-11 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,201] INFO [Partition connect-offsets-11 broker=1] No checkpointed highwatermark is found for partition connect-offsets-11 (kafka.cluster.Partition)
[2023-08-21 06:57:07,201] INFO [Partition connect-offsets-11 broker=1] Log loaded for partition connect-offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,201] INFO Setting topicIdPartition Wsi0raXERfi_RSfgo1c9uQ:connect-offsets-11 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,201] INFO [MergedLog partition=connect-offsets-11, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,203] INFO [MergedLog partition=connect-offsets-1, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,203] INFO Created log for partition connect-offsets-1 in /etc/confluent/kraft-combined-logs/connect-offsets-1 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,203] INFO [Partition connect-offsets-1 broker=1] No checkpointed highwatermark is found for partition connect-offsets-1 (kafka.cluster.Partition)
[2023-08-21 06:57:07,204] INFO [Partition connect-offsets-1 broker=1] Log loaded for partition connect-offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,204] INFO Setting topicIdPartition Wsi0raXERfi_RSfgo1c9uQ:connect-offsets-1 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,204] INFO [MergedLog partition=connect-offsets-1, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,206] INFO [MergedLog partition=connect-offsets-5, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,206] INFO Created log for partition connect-offsets-5 in /etc/confluent/kraft-combined-logs/connect-offsets-5 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,207] INFO [Partition connect-offsets-5 broker=1] No checkpointed highwatermark is found for partition connect-offsets-5 (kafka.cluster.Partition)
[2023-08-21 06:57:07,207] INFO [Partition connect-offsets-5 broker=1] Log loaded for partition connect-offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,207] INFO Setting topicIdPartition Wsi0raXERfi_RSfgo1c9uQ:connect-offsets-5 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,207] INFO [MergedLog partition=connect-offsets-5, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,209] INFO [MergedLog partition=connect-offsets-3, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,210] INFO Created log for partition connect-offsets-3 in /etc/confluent/kraft-combined-logs/connect-offsets-3 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,210] INFO [Partition connect-offsets-3 broker=1] No checkpointed highwatermark is found for partition connect-offsets-3 (kafka.cluster.Partition)
[2023-08-21 06:57:07,210] INFO [Partition connect-offsets-3 broker=1] Log loaded for partition connect-offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,210] INFO Setting topicIdPartition Wsi0raXERfi_RSfgo1c9uQ:connect-offsets-3 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,210] INFO [MergedLog partition=connect-offsets-3, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,213] INFO [MergedLog partition=connect-offsets-23, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,214] INFO Created log for partition connect-offsets-23 in /etc/confluent/kraft-combined-logs/connect-offsets-23 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,214] INFO [Partition connect-offsets-23 broker=1] No checkpointed highwatermark is found for partition connect-offsets-23 (kafka.cluster.Partition)
[2023-08-21 06:57:07,214] INFO [Partition connect-offsets-23 broker=1] Log loaded for partition connect-offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,215] INFO Setting topicIdPartition Wsi0raXERfi_RSfgo1c9uQ:connect-offsets-23 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,215] INFO [MergedLog partition=connect-offsets-23, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-23 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,216] INFO [MergedLog partition=connect-offsets-17, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,217] INFO Created log for partition connect-offsets-17 in /etc/confluent/kraft-combined-logs/connect-offsets-17 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,217] INFO [Partition connect-offsets-17 broker=1] No checkpointed highwatermark is found for partition connect-offsets-17 (kafka.cluster.Partition)
[2023-08-21 06:57:07,217] INFO [Partition connect-offsets-17 broker=1] Log loaded for partition connect-offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,217] INFO Setting topicIdPartition Wsi0raXERfi_RSfgo1c9uQ:connect-offsets-17 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,217] INFO [MergedLog partition=connect-offsets-17, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-17 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,220] INFO [MergedLog partition=connect-offsets-15, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,220] INFO Created log for partition connect-offsets-15 in /etc/confluent/kraft-combined-logs/connect-offsets-15 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,221] INFO [Partition connect-offsets-15 broker=1] No checkpointed highwatermark is found for partition connect-offsets-15 (kafka.cluster.Partition)
[2023-08-21 06:57:07,221] INFO [Partition connect-offsets-15 broker=1] Log loaded for partition connect-offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,221] INFO Setting topicIdPartition Wsi0raXERfi_RSfgo1c9uQ:connect-offsets-15 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,221] INFO [MergedLog partition=connect-offsets-15, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-15 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,223] INFO [MergedLog partition=connect-offsets-21, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,223] INFO Created log for partition connect-offsets-21 in /etc/confluent/kraft-combined-logs/connect-offsets-21 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,223] INFO [Partition connect-offsets-21 broker=1] No checkpointed highwatermark is found for partition connect-offsets-21 (kafka.cluster.Partition)
[2023-08-21 06:57:07,223] INFO [Partition connect-offsets-21 broker=1] Log loaded for partition connect-offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,224] INFO Setting topicIdPartition Wsi0raXERfi_RSfgo1c9uQ:connect-offsets-21 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,224] INFO [MergedLog partition=connect-offsets-21, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-21 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,226] INFO [MergedLog partition=connect-offsets-19, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,226] INFO Created log for partition connect-offsets-19 in /etc/confluent/kraft-combined-logs/connect-offsets-19 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,227] INFO [Partition connect-offsets-19 broker=1] No checkpointed highwatermark is found for partition connect-offsets-19 (kafka.cluster.Partition)
[2023-08-21 06:57:07,227] INFO [Partition connect-offsets-19 broker=1] Log loaded for partition connect-offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,227] INFO Setting topicIdPartition Wsi0raXERfi_RSfgo1c9uQ:connect-offsets-19 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,227] INFO [MergedLog partition=connect-offsets-19, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-19 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,227] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic connect-offsets with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:57:07,307] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(connect-statuses-0, connect-statuses-3, connect-statuses-4, connect-statuses-1, connect-statuses-2) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:57:07,313] INFO [MergedLog partition=connect-statuses-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,313] INFO Created log for partition connect-statuses-0 in /etc/confluent/kraft-combined-logs/connect-statuses-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,314] INFO [Partition connect-statuses-0 broker=1] No checkpointed highwatermark is found for partition connect-statuses-0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,315] INFO [Partition connect-statuses-0 broker=1] Log loaded for partition connect-statuses-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,315] INFO Setting topicIdPartition B-119fZhQ0-lUzXLr4zRMg:connect-statuses-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,315] INFO [MergedLog partition=connect-statuses-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-statuses-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,317] INFO [MergedLog partition=connect-statuses-3, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,317] INFO Created log for partition connect-statuses-3 in /etc/confluent/kraft-combined-logs/connect-statuses-3 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,317] INFO [Partition connect-statuses-3 broker=1] No checkpointed highwatermark is found for partition connect-statuses-3 (kafka.cluster.Partition)
[2023-08-21 06:57:07,318] INFO [Partition connect-statuses-3 broker=1] Log loaded for partition connect-statuses-3 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,318] INFO Setting topicIdPartition B-119fZhQ0-lUzXLr4zRMg:connect-statuses-3 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,318] INFO [MergedLog partition=connect-statuses-3, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-statuses-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,320] INFO [MergedLog partition=connect-statuses-4, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,320] INFO Created log for partition connect-statuses-4 in /etc/confluent/kraft-combined-logs/connect-statuses-4 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,321] INFO [Partition connect-statuses-4 broker=1] No checkpointed highwatermark is found for partition connect-statuses-4 (kafka.cluster.Partition)
[2023-08-21 06:57:07,321] INFO [Partition connect-statuses-4 broker=1] Log loaded for partition connect-statuses-4 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,321] INFO Setting topicIdPartition B-119fZhQ0-lUzXLr4zRMg:connect-statuses-4 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,321] INFO [MergedLog partition=connect-statuses-4, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-statuses-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,323] INFO [MergedLog partition=connect-statuses-1, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,324] INFO Created log for partition connect-statuses-1 in /etc/confluent/kraft-combined-logs/connect-statuses-1 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,325] INFO [Partition connect-statuses-1 broker=1] No checkpointed highwatermark is found for partition connect-statuses-1 (kafka.cluster.Partition)
[2023-08-21 06:57:07,325] INFO [Partition connect-statuses-1 broker=1] Log loaded for partition connect-statuses-1 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,325] INFO Setting topicIdPartition B-119fZhQ0-lUzXLr4zRMg:connect-statuses-1 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,325] INFO [MergedLog partition=connect-statuses-1, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-statuses-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,327] INFO [MergedLog partition=connect-statuses-2, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,328] INFO Created log for partition connect-statuses-2 in /etc/confluent/kraft-combined-logs/connect-statuses-2 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,328] INFO [Partition connect-statuses-2 broker=1] No checkpointed highwatermark is found for partition connect-statuses-2 (kafka.cluster.Partition)
[2023-08-21 06:57:07,328] INFO [Partition connect-statuses-2 broker=1] Log loaded for partition connect-statuses-2 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,328] INFO Setting topicIdPartition B-119fZhQ0-lUzXLr4zRMg:connect-statuses-2 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,328] INFO [MergedLog partition=connect-statuses-2, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-statuses-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,328] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic connect-statuses with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:57:07,370] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(connect-configs-0) (kafka.server.ReplicaFetcherManager)
[2023-08-21 06:57:07,373] INFO [MergedLog partition=connect-configs-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-21 06:57:07,374] INFO Created log for partition connect-configs-0 in /etc/confluent/kraft-combined-logs/connect-configs-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-21 06:57:07,375] INFO [Partition connect-configs-0 broker=1] No checkpointed highwatermark is found for partition connect-configs-0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,376] INFO [Partition connect-configs-0 broker=1] Log loaded for partition connect-configs-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-21 06:57:07,376] INFO Setting topicIdPartition tV63vw24S86Jodi4D5ojCQ:connect-configs-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-21 06:57:07,376] INFO [MergedLog partition=connect-configs-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-configs-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-21 06:57:07,377] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic connect-configs with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-21 06:57:07,431] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group kafka-connect in Empty state. Created a new member id connect-1-df9e72f3-521f-45a4-a1bb-5df6a027c520 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:07,433] INFO [GroupCoordinator 1]: Preparing to rebalance group kafka-connect in state PreparingRebalance with old generation 0 (__consumer_offsets-11) (reason: Adding new member connect-1-df9e72f3-521f-45a4-a1bb-5df6a027c520 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:07,434] INFO [GroupCoordinator 1]: Stabilized group kafka-connect generation 1 (__consumer_offsets-11) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:07,452] INFO [GroupCoordinator 1]: Assignment received from leader connect-1-df9e72f3-521f-45a4-a1bb-5df6a027c520 for group kafka-connect for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:07,755] INFO [GroupCoordinator 1]: Member _confluent-controlcenter-7-4-1-1-command-85ba3b0b-9cfe-4910-a842-4e862f41a62e-StreamThread-1-consumer-6bfa35a3-7590-431a-84ac-ca8df9f1c301 in group _confluent-controlcenter-7-4-1-1-command has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:07,756] INFO [GroupCoordinator 1]: Stabilized group _confluent-controlcenter-7-4-1-1-command generation 2 (__consumer_offsets-23) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:07,786] INFO [GroupCoordinator 1]: Assignment received from leader _confluent-controlcenter-7-4-1-1-command-85ba3b0b-9cfe-4910-a842-4e862f41a62e-StreamThread-1-consumer-d6a9e553-9e48-4326-b21d-4002dbf0b27e for group _confluent-controlcenter-7-4-1-1-command for generation 2. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:10,005] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in Empty state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-7-consumer-ca67f057-e907-4da4-a8c6-362dd762624a and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:10,010] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in Empty state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-3-consumer-cf78a22e-5eb8-436c-a310-36cfbf702f77 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:10,013] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-7-4-1-1 in state PreparingRebalance with old generation 3 (__consumer_offsets-5) (reason: Adding new member _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-3-consumer-cf78a22e-5eb8-436c-a310-36cfbf702f77 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:10,013] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-9-consumer-86df1e59-abc4-481e-b69d-1c311b24bfa2 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:10,015] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-1-consumer-a4c30121-e3fb-43c7-98a5-7b61bcae4d58 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:10,016] INFO [GroupCoordinator 1]: Stabilized group _confluent-controlcenter-7-4-1-1 generation 4 (__consumer_offsets-5) with 2 members (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:10,017] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in CompletingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-14-consumer-45542862-2d40-40da-9303-72124b53ad5c and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:10,017] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in CompletingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-6-consumer-f863ef33-2d52-4416-a833-13a29683f760 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:10,017] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in CompletingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-13-consumer-ab1e9ff7-2a54-459b-8641-9f4184be1ca7 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:10,018] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-7-4-1-1 in state PreparingRebalance with old generation 4 (__consumer_offsets-5) (reason: Adding new member _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-9-consumer-86df1e59-abc4-481e-b69d-1c311b24bfa2 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:10,018] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-2-consumer-dc72cf25-2385-4a1d-bc93-3f63d26bc9d1 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:10,028] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-5-consumer-531ebae1-014e-49f6-828b-34e82caa2402 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:10,031] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-11-consumer-fddb4c71-020a-4f76-b1e2-ff65b456c104 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:10,032] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-8-consumer-8a3b6e46-16e0-4d39-afaf-a5379c4330e2 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:10,034] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-4-consumer-f4da6383-7ec7-4bf8-b35a-e52817761e95 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:10,047] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-10-consumer-94227efa-1cab-474c-838f-418230811b09 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:10,050] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-16-consumer-8352fdae-17c4-4777-b6ab-420d718e4719 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:10,051] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-12-consumer-58bd2624-f616-414c-8235-7fe7b976d7b7 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:10,052] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-15-consumer-6f37bf54-3a21-46ce-b594-72ca97303a5a and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:10,063] INFO [GroupCoordinator 1]: Stabilized group _confluent-controlcenter-7-4-1-1 generation 5 (__consumer_offsets-5) with 16 members (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:10,090] INFO [GroupCoordinator 1]: Assignment received from leader _confluent-controlcenter-7-4-1-1-5becb68f-9aa6-4e00-ab50-f30087b158c8-StreamThread-3-consumer-cf78a22e-5eb8-436c-a310-36cfbf702f77 for group _confluent-controlcenter-7-4-1-1 for generation 5. The group has 16 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-08-21 06:57:13,253] ERROR [ReplicaManager broker=1] Error processing append operation on partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (kafka.server.ReplicaManager)
org.apache.kafka.common.errors.InvalidProducerEpochException: Epoch of producer 0 at offset 1 in _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 is 0, which is smaller than the last seen epoch 1
[2023-08-21 06:57:13,253] ERROR [ReplicaManager broker=1] Error processing append operation on partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (kafka.server.ReplicaManager)
org.apache.kafka.common.errors.InvalidProducerEpochException: Epoch of producer 0 at offset 1 in _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 is 0, which is smaller than the last seen epoch 1
[2023-08-21 06:57:13,253] ERROR [ReplicaManager broker=1] Error processing append operation on partition _confluent-monitoring-0 (kafka.server.ReplicaManager)
org.apache.kafka.common.errors.InvalidProducerEpochException: Epoch of producer 0 at offset 1 in _confluent-monitoring-0 is 0, which is smaller than the last seen epoch 1
[2023-08-21 06:57:28,762] ERROR [ReplicaManager broker=1] Error processing append operation on partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (kafka.server.ReplicaManager)
org.apache.kafka.common.errors.OutOfOrderSequenceException: Out of order sequence number for producer 0 at offset 1 in partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0: 0 (incoming seq. number), 26 (current end sequence number)
[2023-08-21 06:57:28,762] ERROR [ReplicaManager broker=1] Error processing append operation on partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (kafka.server.ReplicaManager)
org.apache.kafka.common.errors.OutOfOrderSequenceException: Out of order sequence number for producer 0 at offset 1 in partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0: 0 (incoming seq. number), 26 (current end sequence number)
[2023-08-21 06:57:28,762] ERROR [ReplicaManager broker=1] Error processing append operation on partition _confluent-monitoring-0 (kafka.server.ReplicaManager)
org.apache.kafka.common.errors.OutOfOrderSequenceException: Out of order sequence number for producer 0 at offset 1 in partition _confluent-monitoring-0: 0 (incoming seq. number), 26 (current end sequence number)
