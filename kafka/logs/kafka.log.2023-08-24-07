[2023-08-24 07:00:04,608] INFO [ProducerStateManager partition=_confluent-telemetry-metrics-11] Wrote producer snapshot at offset 479058 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager)
[2023-08-24 07:00:04,608] INFO [MergedLog partition=_confluent-telemetry-metrics-11, dir=/etc/confluent/kraft-combined-logs] Rolled new log segment at offset 479058 in 1 ms. (kafka.log.MergedLog)
[2023-08-24 07:00:04,618] INFO [ProducerStateManager partition=_confluent-telemetry-metrics-2] Wrote producer snapshot at offset 472183 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager)
[2023-08-24 07:00:04,618] INFO [MergedLog partition=_confluent-telemetry-metrics-2, dir=/etc/confluent/kraft-combined-logs] Rolled new log segment at offset 472183 in 1 ms. (kafka.log.MergedLog)
[2023-08-24 07:00:04,636] INFO [ProducerStateManager partition=_confluent-telemetry-metrics-7] Wrote producer snapshot at offset 469561 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager)
[2023-08-24 07:00:04,636] INFO [MergedLog partition=_confluent-telemetry-metrics-7, dir=/etc/confluent/kraft-combined-logs] Rolled new log segment at offset 469561 in 0 ms. (kafka.log.MergedLog)
[2023-08-24 07:00:34,609] INFO [ProducerStateManager partition=_confluent-telemetry-metrics-10] Wrote producer snapshot at offset 472087 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager)
[2023-08-24 07:00:34,609] INFO [MergedLog partition=_confluent-telemetry-metrics-10, dir=/etc/confluent/kraft-combined-logs] Rolled new log segment at offset 472087 in 0 ms. (kafka.log.MergedLog)
[2023-08-24 07:00:34,631] INFO [ProducerStateManager partition=_confluent-telemetry-metrics-1] Wrote producer snapshot at offset 469534 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager)
[2023-08-24 07:00:34,631] INFO [MergedLog partition=_confluent-telemetry-metrics-1, dir=/etc/confluent/kraft-combined-logs] Rolled new log segment at offset 469534 in 0 ms. (kafka.log.MergedLog)
[2023-08-24 07:00:34,649] INFO [ProducerStateManager partition=_confluent-telemetry-metrics-4] Wrote producer snapshot at offset 474002 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager)
[2023-08-24 07:00:34,649] INFO [MergedLog partition=_confluent-telemetry-metrics-4, dir=/etc/confluent/kraft-combined-logs] Rolled new log segment at offset 474002 in 0 ms. (kafka.log.MergedLog)
[2023-08-24 07:01:04,610] INFO [ProducerStateManager partition=_confluent-telemetry-metrics-5] Wrote producer snapshot at offset 467296 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager)
[2023-08-24 07:01:04,610] INFO [MergedLog partition=_confluent-telemetry-metrics-5, dir=/etc/confluent/kraft-combined-logs] Rolled new log segment at offset 467296 in 1 ms. (kafka.log.MergedLog)
[2023-08-24 07:01:04,616] INFO [ProducerStateManager partition=_confluent-telemetry-metrics-9] Wrote producer snapshot at offset 472003 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager)
[2023-08-24 07:01:04,616] INFO [MergedLog partition=_confluent-telemetry-metrics-9, dir=/etc/confluent/kraft-combined-logs] Rolled new log segment at offset 472003 in 0 ms. (kafka.log.MergedLog)
[2023-08-24 07:01:04,644] INFO [ProducerStateManager partition=_confluent-telemetry-metrics-6] Wrote producer snapshot at offset 480837 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager)
[2023-08-24 07:01:04,644] INFO [MergedLog partition=_confluent-telemetry-metrics-6, dir=/etc/confluent/kraft-combined-logs] Rolled new log segment at offset 480837 in 0 ms. (kafka.log.MergedLog)
[2023-08-24 07:01:19,721] INFO [ProducerStateManager partition=_confluent-metrics-9] Wrote producer snapshot at offset 87977 with 0 producer ids in 1 ms. (kafka.log.ProducerStateManager)
[2023-08-24 07:01:19,721] INFO [MergedLog partition=_confluent-metrics-9, dir=/etc/confluent/kraft-combined-logs] Rolled new log segment at offset 87977 in 1 ms. (kafka.log.MergedLog)
[2023-08-24 07:01:34,631] INFO [ProducerStateManager partition=_confluent-telemetry-metrics-8] Wrote producer snapshot at offset 471318 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager)
[2023-08-24 07:01:34,631] INFO [MergedLog partition=_confluent-telemetry-metrics-8, dir=/etc/confluent/kraft-combined-logs] Rolled new log segment at offset 471318 in 0 ms. (kafka.log.MergedLog)
[2023-08-24 07:01:49,721] INFO [ProducerStateManager partition=_confluent-metrics-4] Wrote producer snapshot at offset 87824 with 0 producer ids in 1 ms. (kafka.log.ProducerStateManager)
[2023-08-24 07:01:49,721] INFO [MergedLog partition=_confluent-metrics-4, dir=/etc/confluent/kraft-combined-logs] Rolled new log segment at offset 87824 in 1 ms. (kafka.log.MergedLog)
[2023-08-24 07:01:49,723] INFO [ProducerStateManager partition=_confluent-metrics-11] Wrote producer snapshot at offset 87888 with 0 producer ids in 1 ms. (kafka.log.ProducerStateManager)
[2023-08-24 07:01:49,723] INFO [MergedLog partition=_confluent-metrics-11, dir=/etc/confluent/kraft-combined-logs] Rolled new log segment at offset 87888 in 2 ms. (kafka.log.MergedLog)
[2023-08-24 07:02:04,720] INFO [ProducerStateManager partition=_confluent-metrics-6] Wrote producer snapshot at offset 88036 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager)
[2023-08-24 07:02:04,720] INFO [MergedLog partition=_confluent-metrics-6, dir=/etc/confluent/kraft-combined-logs] Rolled new log segment at offset 88036 in 1 ms. (kafka.log.MergedLog)
[2023-08-24 07:02:04,723] INFO [ProducerStateManager partition=_confluent-metrics-7] Wrote producer snapshot at offset 88142 with 0 producer ids in 1 ms. (kafka.log.ProducerStateManager)
[2023-08-24 07:02:04,723] INFO [MergedLog partition=_confluent-metrics-7, dir=/etc/confluent/kraft-combined-logs] Rolled new log segment at offset 88142 in 1 ms. (kafka.log.MergedLog)
[2023-08-24 07:02:19,720] INFO [ProducerStateManager partition=_confluent-metrics-8] Wrote producer snapshot at offset 87464 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager)
[2023-08-24 07:02:19,720] INFO [MergedLog partition=_confluent-metrics-8, dir=/etc/confluent/kraft-combined-logs] Rolled new log segment at offset 87464 in 0 ms. (kafka.log.MergedLog)
[2023-08-24 07:02:34,639] INFO [ProducerStateManager partition=_confluent-telemetry-metrics-3] Wrote producer snapshot at offset 475968 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager)
[2023-08-24 07:02:34,639] INFO [MergedLog partition=_confluent-telemetry-metrics-3, dir=/etc/confluent/kraft-combined-logs] Rolled new log segment at offset 475968 in 1 ms. (kafka.log.MergedLog)
[2023-08-24 07:02:34,720] INFO [ProducerStateManager partition=_confluent-metrics-3] Wrote producer snapshot at offset 88388 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager)
[2023-08-24 07:02:34,720] INFO [MergedLog partition=_confluent-metrics-3, dir=/etc/confluent/kraft-combined-logs] Rolled new log segment at offset 88388 in 1 ms. (kafka.log.MergedLog)
[2023-08-24 07:02:34,722] INFO [ProducerStateManager partition=_confluent-metrics-5] Wrote producer snapshot at offset 88086 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager)
[2023-08-24 07:02:34,722] INFO [MergedLog partition=_confluent-metrics-5, dir=/etc/confluent/kraft-combined-logs] Rolled new log segment at offset 88086 in 1 ms. (kafka.log.MergedLog)
[2023-08-24 07:02:49,721] INFO [ProducerStateManager partition=_confluent-metrics-10] Wrote producer snapshot at offset 87367 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager)
[2023-08-24 07:02:49,721] INFO [MergedLog partition=_confluent-metrics-10, dir=/etc/confluent/kraft-combined-logs] Rolled new log segment at offset 87367 in 1 ms. (kafka.log.MergedLog)
[2023-08-24 07:02:49,724] INFO [ProducerStateManager partition=_confluent-metrics-2] Wrote producer snapshot at offset 88382 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager)
[2023-08-24 07:02:49,725] INFO [MergedLog partition=_confluent-metrics-2, dir=/etc/confluent/kraft-combined-logs] Rolled new log segment at offset 88382 in 1 ms. (kafka.log.MergedLog)
[2023-08-24 07:03:04,723] INFO [ProducerStateManager partition=_confluent-metrics-0] Wrote producer snapshot at offset 87959 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager)
[2023-08-24 07:03:04,723] INFO [MergedLog partition=_confluent-metrics-0, dir=/etc/confluent/kraft-combined-logs] Rolled new log segment at offset 87959 in 1 ms. (kafka.log.MergedLog)
[2023-08-24 07:03:34,722] INFO [ProducerStateManager partition=_confluent-metrics-1] Wrote producer snapshot at offset 87986 with 0 producer ids in 0 ms. (kafka.log.ProducerStateManager)
[2023-08-24 07:03:34,722] INFO [MergedLog partition=_confluent-metrics-1, dir=/etc/confluent/kraft-combined-logs] Rolled new log segment at offset 87986 in 1 ms. (kafka.log.MergedLog)
[2023-08-24 07:23:10,065] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2023-08-24 07:23:11,434] INFO KafkaConfig values: 
	advertised.listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = LfQY6ZKYRb6NOuiRwgTYfw
	client.quota.callback.class = null
	compression.type = producer
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.demotion.support.enabled = false
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = true
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = ANY_UNEVEN_LOAD
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = false
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.resource.utilization.detector.enabled = false
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.partition.maximum.movements = 5
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.suspension.ms = 10800000
	confluent.balancer.topic.replication.factor = 1
	confluent.balancer.triggering.goals = []
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.kraft.mitigation.enabled = false
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.max.topics.per.snapshot = 5000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud
	confluent.cdc.api.keys.topic = 
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 1
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.metadata.load.threads = 32
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.throttle.ms = null
	confluent.metadata.active.encryptor = null
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-01:19092
	confluent.min.acks = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.prefer.tier.fetch.ms = -1
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.dynamic.reporting.min.usage = 102400
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = false
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = http://schema-registry:8081
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.telemetry.enabled = true
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.producer.enable.idempotence = false
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.partition.default.placement = 2
	confluent.topic.replica.assignor.builder.class = 
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id = 
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.valid.broker.rack.set = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-01:29092]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = INTERNAL
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
	listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092,CONTROLLER://kafka-01:29092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dirs = /etc/confluent/kraft-combined-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 2000000
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter, io.confluent.telemetry.reporter.TelemetryReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = [hidden]
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.id.quota.window.num = 11
	producer.id.quota.window.size.seconds = 1
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 2097152
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.acl.change.notification.expiration.ms = 900000
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2023-08-24 07:23:11,470] INFO KafkaConfig values: 
	advertised.listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = LfQY6ZKYRb6NOuiRwgTYfw
	client.quota.callback.class = null
	compression.type = producer
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.demotion.support.enabled = false
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = true
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = ANY_UNEVEN_LOAD
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = false
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.resource.utilization.detector.enabled = false
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.partition.maximum.movements = 5
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.suspension.ms = 10800000
	confluent.balancer.topic.replication.factor = 1
	confluent.balancer.triggering.goals = []
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.kraft.mitigation.enabled = false
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.max.topics.per.snapshot = 5000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud
	confluent.cdc.api.keys.topic = 
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 1
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.metadata.load.threads = 32
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.throttle.ms = null
	confluent.metadata.active.encryptor = null
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-01:19092
	confluent.min.acks = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.prefer.tier.fetch.ms = -1
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.dynamic.reporting.min.usage = 102400
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = false
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = http://schema-registry:8081
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.telemetry.enabled = true
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.producer.enable.idempotence = false
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.partition.default.placement = 2
	confluent.topic.replica.assignor.builder.class = 
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id = 
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.valid.broker.rack.set = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-01:29092]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = INTERNAL
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
	listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092,CONTROLLER://kafka-01:29092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dirs = /etc/confluent/kraft-combined-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 2000000
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter, io.confluent.telemetry.reporter.TelemetryReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = [hidden]
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.id.quota.window.num = 11
	producer.id.quota.window.size.seconds = 1
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 2097152
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.acl.change.notification.expiration.ms = 900000
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2023-08-24 07:23:11,483] INFO Starting controller (kafka.server.ControllerServer)
[2023-08-24 07:23:11,816] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas)
[2023-08-24 07:23:11,827] INFO Awaiting socket connections on kafka-01:29092. (kafka.network.DataPlaneAcceptor)
[2023-08-24 07:23:11,880] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)
[2023-08-24 07:23:11,892] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer)
[2023-08-24 07:23:12,043] INFO [MergedLog partition=__cluster_metadata-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:12,044] INFO [MergedLog partition=__cluster_metadata-0, dir=/etc/confluent/kraft-combined-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.MergedLog$)
[2023-08-24 07:23:12,045] INFO [MergedLog partition=__cluster_metadata-0, dir=/etc/confluent/kraft-combined-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.MergedLog$)
[2023-08-24 07:23:12,062] INFO Initialized snapshots with IDs SortedSet() from /etc/confluent/kraft-combined-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2023-08-24 07:23:12,080] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2023-08-24 07:23:12,170] INFO [kafka-raft-outbound-request-thread]: Starting (kafka.raft.RaftSendThread)
[2023-08-24 07:23:12,170] INFO [kafka-raft-io-thread]: Starting (kafka.raft.KafkaRaftManager$RaftIoThread)
[2023-08-24 07:23:12,274] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-24 07:23:12,275] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2023-08-24 07:23:12,277] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-24 07:23:12,286] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2023-08-24 07:23:12,287] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-24 07:23:12,288] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2023-08-24 07:23:12,289] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-24 07:23:12,310] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-24 07:23:12,319] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
[2023-08-24 07:23:12,324] INFO [BrokerServer id=1] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2023-08-24 07:23:12,325] INFO [BrokerServer id=1] Starting broker (kafka.server.BrokerServer)
[2023-08-24 07:23:12,348] INFO [BrokerServer id=1] FIPS mode enabled: false (kafka.server.BrokerServer)
[2023-08-24 07:23:12,363] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2023-08-24 07:23:12,363] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-24 07:23:12,364] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-24 07:23:12,365] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2023-08-24 07:23:12,366] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2023-08-24 07:23:12,366] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-24 07:23:12,366] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-24 07:23:12,416] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread)
[2023-08-24 07:23:12,417] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Recorded new controller, from now on will use node kafka-01:29092 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2023-08-24 07:23:12,477] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas)
[2023-08-24 07:23:12,478] INFO Awaiting socket connections on kafka-01:19092. (kafka.network.DataPlaneAcceptor)
[2023-08-24 07:23:12,488] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(INTERNAL) (kafka.network.SocketServer)
[2023-08-24 07:23:12,489] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas)
[2023-08-24 07:23:12,489] INFO Awaiting socket connections on kafka-01:9092. (kafka.network.DataPlaneAcceptor)
[2023-08-24 07:23:12,497] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2023-08-24 07:23:12,508] INFO [BrokerToControllerChannelManager broker=1 name=alterPartition]: Starting (kafka.server.BrokerToControllerRequestThread)
[2023-08-24 07:23:12,510] INFO [BrokerToControllerChannelManager broker=1 name=alterPartition]: Recorded new controller, from now on will use node kafka-01:29092 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2023-08-24 07:23:12,542] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-24 07:23:12,541] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-24 07:23:12,542] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-24 07:23:12,542] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-24 07:23:12,543] INFO [ExpirationReaper-1-ListOffsets]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-24 07:23:12,575] INFO [BrokerHealthManager]: Starting (kafka.availability.BrokerHealthManager)
[2023-08-24 07:23:12,583] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-24 07:23:12,583] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-24 07:23:14,677] INFO [BrokerLifecycleManager id=1] Incarnation Zq4Zg-oTTbilytALNvBwQQ of broker 1 in cluster MkU3OEVBNTcwNTJENDM2Qg is now STARTING. (kafka.server.BrokerLifecycleManager)
[2023-08-24 07:23:14,678] INFO [BrokerToControllerChannelManager broker=1 name=heartbeat]: Starting (kafka.server.BrokerToControllerRequestThread)
[2023-08-24 07:23:14,683] INFO [BrokerToControllerChannelManager broker=1 name=heartbeat]: Recorded new controller, from now on will use node kafka-01:29092 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2023-08-24 07:23:14,761] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-24 07:23:14,769] INFO [BrokerServer id=1] Waiting for broker metadata to catch up. (kafka.server.BrokerServer)
[2023-08-24 07:23:14,784] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 6 (kafka.server.BrokerLifecycleManager)
[2023-08-24 07:23:14,790] INFO [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2023-08-24 07:23:14,795] INFO [BrokerMetadataListener id=1] Starting to publish metadata events at offset 7. (kafka.server.metadata.BrokerMetadataListener)
[2023-08-24 07:23:14,799] INFO [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=7, epoch=1) with metadata.version 3.4-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2023-08-24 07:23:14,800] INFO Loading logs from log dirs ArraySeq(/etc/confluent/kraft-combined-logs) (kafka.log.LogManager)
[2023-08-24 07:23:14,802] INFO [BrokerLifecycleManager id=1] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2023-08-24 07:23:14,803] INFO Attempting recovery for all logs in /etc/confluent/kraft-combined-logs since no clean shutdown file was found (kafka.log.LogManager)
[2023-08-24 07:23:14,809] INFO Loaded 0 logs in 9ms. (kafka.log.LogManager)
[2023-08-24 07:23:14,810] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2023-08-24 07:23:14,811] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2023-08-24 07:23:14,830] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2023-08-24 07:23:14,830] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2023-08-24 07:23:14,921] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner)
[2023-08-24 07:23:14,921] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner)
[2023-08-24 07:23:14,927] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2023-08-24 07:23:14,928] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:14,930] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:14,931] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:14,934] INFO [Transaction Marker Channel Manager 1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2023-08-24 07:23:14,934] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:14,937] INFO [BrokerMetadataPublisher id=1] Updating metadata.version to 3.4-IV0 at offset OffsetAndEpoch(offset=7, epoch=1). (kafka.server.metadata.BrokerMetadataPublisher)
[2023-08-24 07:23:14,953] INFO KafkaConfig values: 
	advertised.listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = LfQY6ZKYRb6NOuiRwgTYfw
	client.quota.callback.class = null
	compression.type = producer
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.demotion.support.enabled = false
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = true
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = ANY_UNEVEN_LOAD
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = false
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.resource.utilization.detector.enabled = false
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.partition.maximum.movements = 5
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.suspension.ms = 10800000
	confluent.balancer.topic.replication.factor = 1
	confluent.balancer.triggering.goals = []
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.kraft.mitigation.enabled = false
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.max.topics.per.snapshot = 5000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud
	confluent.cdc.api.keys.topic = 
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 1
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.metadata.load.threads = 32
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.throttle.ms = null
	confluent.metadata.active.encryptor = null
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-01:19092
	confluent.min.acks = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.prefer.tier.fetch.ms = -1
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.dynamic.reporting.min.usage = 102400
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = false
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = http://schema-registry:8081
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.telemetry.enabled = true
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.producer.enable.idempotence = false
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.partition.default.placement = 2
	confluent.topic.replica.assignor.builder.class = 
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id = 
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.valid.broker.rack.set = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-01:29092]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = INTERNAL
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
	listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092,CONTROLLER://kafka-01:29092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dirs = /etc/confluent/kraft-combined-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 2000000
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter, io.confluent.telemetry.reporter.TelemetryReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = [hidden]
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.id.quota.window.num = 11
	producer.id.quota.window.size.seconds = 1
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 2097152
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.acl.change.notification.expiration.ms = 900000
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2023-08-24 07:23:14,962] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
[2023-08-24 07:23:15,026] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2023-08-24 07:23:15,174] INFO KafkaConfig values: 
	advertised.listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = LfQY6ZKYRb6NOuiRwgTYfw
	client.quota.callback.class = null
	compression.type = producer
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.demotion.support.enabled = false
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = true
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = ANY_UNEVEN_LOAD
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = false
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.resource.utilization.detector.enabled = false
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.partition.maximum.movements = 5
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.suspension.ms = 10800000
	confluent.balancer.topic.replication.factor = 1
	confluent.balancer.triggering.goals = []
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.kraft.mitigation.enabled = false
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.max.topics.per.snapshot = 5000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud
	confluent.cdc.api.keys.topic = 
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 1
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.metadata.load.threads = 32
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.throttle.ms = null
	confluent.metadata.active.encryptor = null
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-01:19092
	confluent.min.acks = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.prefer.tier.fetch.ms = -1
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.dynamic.reporting.min.usage = 102400
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = false
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = http://schema-registry:8081
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.telemetry.enabled = true
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.producer.enable.idempotence = false
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.partition.default.placement = 2
	confluent.topic.replica.assignor.builder.class = 
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id = 
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.valid.broker.rack.set = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-01:29092]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = INTERNAL
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
	listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092,CONTROLLER://kafka-01:29092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dirs = /etc/confluent/kraft-combined-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 2000000
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter, io.confluent.telemetry.reporter.TelemetryReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = [hidden]
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.id.quota.window.num = 11
	producer.id.quota.window.size.seconds = 1
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 2097152
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.acl.change.notification.expiration.ms = 900000
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2023-08-24 07:23:18,207] INFO [BrokerServer id=1] Skipping durability audit instantiation (kafka.server.BrokerServer)
[2023-08-24 07:23:18,472] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-command-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:18,532] INFO [MergedLog partition=_confluent-command-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:18,541] INFO Created log for partition _confluent-command-0 in /etc/confluent/kraft-combined-logs/_confluent-command-0 with properties {cleanup.policy=compact, min.insync.replicas=1} (kafka.log.LogManager)
[2023-08-24 07:23:18,543] INFO [Partition _confluent-command-0 broker=1] No checkpointed highwatermark is found for partition _confluent-command-0 (kafka.cluster.Partition)
[2023-08-24 07:23:18,551] INFO [Partition _confluent-command-0 broker=1] Log loaded for partition _confluent-command-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:18,554] INFO Setting topicIdPartition beMJufSySMymQpLU9p-f1w:_confluent-command-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:18,555] INFO [MergedLog partition=_confluent-command-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-command-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:18,593] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-command with new configuration : cleanup.policy -> compact,min.insync.replicas -> 1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:18,804] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:18,811] INFO [MergedLog partition=_schemas-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:18,812] INFO Created log for partition _schemas-0 in /etc/confluent/kraft-combined-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:23:18,815] INFO [Partition _schemas-0 broker=1] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2023-08-24 07:23:18,815] INFO [Partition _schemas-0 broker=1] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:18,815] INFO Setting topicIdPartition Oh17e_M1Rj6TGcoayG3gJA:_schemas-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:18,816] INFO [MergedLog partition=_schemas-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _schemas-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:18,817] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:19,427] INFO Starting delay for broker load metric (kafka.metrics.BrokerLoad)
[2023-08-24 07:23:19,429] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2023-08-24 07:23:19,430] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer)
[2023-08-24 07:23:19,605] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2023-08-24 07:23:19,646] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:19,813] INFO [MergedLog partition=__consumer_offsets-13, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:19,814] INFO Created log for partition __consumer_offsets-13 in /etc/confluent/kraft-combined-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:19,815] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2023-08-24 07:23:19,815] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:19,816] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-13 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:19,816] INFO [MergedLog partition=__consumer_offsets-13, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-13 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:19,821] INFO [MergedLog partition=__consumer_offsets-46, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:19,822] INFO Created log for partition __consumer_offsets-46 in /etc/confluent/kraft-combined-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:19,824] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2023-08-24 07:23:19,824] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:19,824] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-46 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:19,825] INFO [MergedLog partition=__consumer_offsets-46, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-46 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:19,843] INFO [MergedLog partition=__consumer_offsets-9, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:19,844] INFO Created log for partition __consumer_offsets-9 in /etc/confluent/kraft-combined-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:19,845] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2023-08-24 07:23:19,845] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:19,845] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-9 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:19,846] INFO [MergedLog partition=__consumer_offsets-9, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:19,849] INFO [MergedLog partition=__consumer_offsets-42, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:19,850] INFO Created log for partition __consumer_offsets-42 in /etc/confluent/kraft-combined-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:19,851] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2023-08-24 07:23:19,851] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:19,851] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-42 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:19,852] INFO [MergedLog partition=__consumer_offsets-42, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-42 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:19,856] INFO [MergedLog partition=__consumer_offsets-21, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:19,857] INFO Created log for partition __consumer_offsets-21 in /etc/confluent/kraft-combined-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:19,899] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2023-08-24 07:23:19,899] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:19,899] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-21 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:19,901] INFO [MergedLog partition=__consumer_offsets-21, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-21 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:19,905] INFO [MergedLog partition=__consumer_offsets-17, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:19,909] INFO Created log for partition __consumer_offsets-17 in /etc/confluent/kraft-combined-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:19,910] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2023-08-24 07:23:19,910] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:19,911] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-17 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:19,911] INFO [MergedLog partition=__consumer_offsets-17, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-17 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:19,922] INFO [MergedLog partition=__consumer_offsets-30, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:19,924] INFO Created log for partition __consumer_offsets-30 in /etc/confluent/kraft-combined-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:19,933] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2023-08-24 07:23:19,933] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:19,933] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-30 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:19,933] INFO [MergedLog partition=__consumer_offsets-30, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-30 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:19,953] INFO [MergedLog partition=__consumer_offsets-26, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:19,954] INFO Created log for partition __consumer_offsets-26 in /etc/confluent/kraft-combined-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:19,955] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2023-08-24 07:23:19,956] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:19,957] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-26 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:19,957] INFO [MergedLog partition=__consumer_offsets-26, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-26 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:19,967] INFO [MergedLog partition=__consumer_offsets-5, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:19,968] INFO Created log for partition __consumer_offsets-5 in /etc/confluent/kraft-combined-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:19,969] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2023-08-24 07:23:19,969] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:19,969] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-5 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:19,969] INFO [MergedLog partition=__consumer_offsets-5, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:19,987] INFO [MergedLog partition=__consumer_offsets-38, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:19,991] INFO Created log for partition __consumer_offsets-38 in /etc/confluent/kraft-combined-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:19,992] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2023-08-24 07:23:19,992] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:19,992] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-38 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:19,993] INFO [MergedLog partition=__consumer_offsets-38, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-38 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:19,998] INFO [MergedLog partition=__consumer_offsets-1, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:19,999] INFO Created log for partition __consumer_offsets-1 in /etc/confluent/kraft-combined-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,001] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2023-08-24 07:23:20,001] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,001] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-1 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,001] INFO [MergedLog partition=__consumer_offsets-1, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,005] INFO [MergedLog partition=__consumer_offsets-34, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,006] INFO Created log for partition __consumer_offsets-34 in /etc/confluent/kraft-combined-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,007] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2023-08-24 07:23:20,016] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,016] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-34 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,017] INFO [MergedLog partition=__consumer_offsets-34, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-34 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,021] INFO [MergedLog partition=__consumer_offsets-16, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,022] INFO Created log for partition __consumer_offsets-16 in /etc/confluent/kraft-combined-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,023] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2023-08-24 07:23:20,023] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,023] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-16 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,024] INFO [MergedLog partition=__consumer_offsets-16, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-16 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,041] INFO [MergedLog partition=__consumer_offsets-45, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,042] INFO Created log for partition __consumer_offsets-45 in /etc/confluent/kraft-combined-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,042] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2023-08-24 07:23:20,042] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,042] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-45 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,042] INFO [MergedLog partition=__consumer_offsets-45, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-45 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,047] INFO [MergedLog partition=__consumer_offsets-12, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,050] INFO Created log for partition __consumer_offsets-12 in /etc/confluent/kraft-combined-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,055] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2023-08-24 07:23:20,055] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,055] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-12 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,055] INFO [MergedLog partition=__consumer_offsets-12, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-12 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,071] INFO [MergedLog partition=__consumer_offsets-41, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,072] INFO Created log for partition __consumer_offsets-41 in /etc/confluent/kraft-combined-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,073] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2023-08-24 07:23:20,073] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,073] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-41 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,073] INFO [MergedLog partition=__consumer_offsets-41, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-41 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,077] INFO [MergedLog partition=__consumer_offsets-24, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,078] INFO Created log for partition __consumer_offsets-24 in /etc/confluent/kraft-combined-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,079] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2023-08-24 07:23:20,079] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,079] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-24 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,079] INFO [MergedLog partition=__consumer_offsets-24, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-24 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,084] INFO [MergedLog partition=__consumer_offsets-20, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,085] INFO Created log for partition __consumer_offsets-20 in /etc/confluent/kraft-combined-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,085] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2023-08-24 07:23:20,085] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,090] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-20 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,091] INFO [MergedLog partition=__consumer_offsets-20, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-20 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,095] INFO [MergedLog partition=__consumer_offsets-49, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,095] INFO Created log for partition __consumer_offsets-49 in /etc/confluent/kraft-combined-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,096] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2023-08-24 07:23:20,096] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,096] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-49 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,096] INFO [MergedLog partition=__consumer_offsets-49, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-49 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,099] INFO [MergedLog partition=__consumer_offsets-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,101] INFO Created log for partition __consumer_offsets-0 in /etc/confluent/kraft-combined-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,103] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,103] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,103] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,103] INFO [MergedLog partition=__consumer_offsets-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,110] INFO [MergedLog partition=__consumer_offsets-29, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,117] INFO Created log for partition __consumer_offsets-29 in /etc/confluent/kraft-combined-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,117] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2023-08-24 07:23:20,117] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,117] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-29 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,117] INFO [MergedLog partition=__consumer_offsets-29, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-29 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,122] INFO [MergedLog partition=__consumer_offsets-25, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,123] INFO Created log for partition __consumer_offsets-25 in /etc/confluent/kraft-combined-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,127] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2023-08-24 07:23:20,127] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,127] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-25 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,128] INFO [MergedLog partition=__consumer_offsets-25, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-25 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,132] INFO [MergedLog partition=__consumer_offsets-8, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,133] INFO Created log for partition __consumer_offsets-8 in /etc/confluent/kraft-combined-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,139] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2023-08-24 07:23:20,140] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,140] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-8 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,140] INFO [MergedLog partition=__consumer_offsets-8, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,144] INFO [MergedLog partition=__consumer_offsets-37, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,148] INFO Created log for partition __consumer_offsets-37 in /etc/confluent/kraft-combined-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,149] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2023-08-24 07:23:20,149] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,149] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-37 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,150] INFO [MergedLog partition=__consumer_offsets-37, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-37 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,154] INFO [MergedLog partition=__consumer_offsets-4, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,156] INFO Created log for partition __consumer_offsets-4 in /etc/confluent/kraft-combined-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,157] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2023-08-24 07:23:20,157] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,157] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-4 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,157] INFO [MergedLog partition=__consumer_offsets-4, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,160] INFO [MergedLog partition=__consumer_offsets-33, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,161] INFO Created log for partition __consumer_offsets-33 in /etc/confluent/kraft-combined-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,162] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2023-08-24 07:23:20,162] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,162] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-33 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,162] INFO [MergedLog partition=__consumer_offsets-33, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-33 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,165] INFO [MergedLog partition=__consumer_offsets-15, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,166] INFO Created log for partition __consumer_offsets-15 in /etc/confluent/kraft-combined-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,167] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2023-08-24 07:23:20,167] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,167] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-15 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,167] INFO [MergedLog partition=__consumer_offsets-15, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-15 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,171] INFO [MergedLog partition=__consumer_offsets-48, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,172] INFO Created log for partition __consumer_offsets-48 in /etc/confluent/kraft-combined-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,173] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2023-08-24 07:23:20,173] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,173] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-48 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,174] INFO [MergedLog partition=__consumer_offsets-48, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-48 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,177] INFO [MergedLog partition=__consumer_offsets-11, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,178] INFO Created log for partition __consumer_offsets-11 in /etc/confluent/kraft-combined-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,179] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2023-08-24 07:23:20,179] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,180] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-11 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,180] INFO [MergedLog partition=__consumer_offsets-11, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,183] INFO [MergedLog partition=__consumer_offsets-44, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,184] INFO Created log for partition __consumer_offsets-44 in /etc/confluent/kraft-combined-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,185] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2023-08-24 07:23:20,185] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,186] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-44 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,186] INFO [MergedLog partition=__consumer_offsets-44, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-44 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,189] INFO [MergedLog partition=__consumer_offsets-23, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,190] INFO Created log for partition __consumer_offsets-23 in /etc/confluent/kraft-combined-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,191] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2023-08-24 07:23:20,191] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,191] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-23 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,192] INFO [MergedLog partition=__consumer_offsets-23, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-23 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,196] INFO [MergedLog partition=__consumer_offsets-19, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,197] INFO Created log for partition __consumer_offsets-19 in /etc/confluent/kraft-combined-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,197] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2023-08-24 07:23:20,197] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,198] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-19 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,198] INFO [MergedLog partition=__consumer_offsets-19, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-19 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,201] INFO [MergedLog partition=__consumer_offsets-32, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,202] INFO Created log for partition __consumer_offsets-32 in /etc/confluent/kraft-combined-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,203] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2023-08-24 07:23:20,203] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,204] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-32 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,204] INFO [MergedLog partition=__consumer_offsets-32, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-32 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,208] INFO [MergedLog partition=__consumer_offsets-28, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,209] INFO Created log for partition __consumer_offsets-28 in /etc/confluent/kraft-combined-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,210] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2023-08-24 07:23:20,210] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,210] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-28 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,210] INFO [MergedLog partition=__consumer_offsets-28, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-28 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,213] INFO [MergedLog partition=__consumer_offsets-7, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,214] INFO Created log for partition __consumer_offsets-7 in /etc/confluent/kraft-combined-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,218] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2023-08-24 07:23:20,218] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,218] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-7 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,219] INFO [MergedLog partition=__consumer_offsets-7, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,224] INFO [MergedLog partition=__consumer_offsets-40, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,225] INFO Created log for partition __consumer_offsets-40 in /etc/confluent/kraft-combined-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,225] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2023-08-24 07:23:20,225] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,226] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-40 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,226] INFO [MergedLog partition=__consumer_offsets-40, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-40 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,230] INFO [MergedLog partition=__consumer_offsets-3, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,231] INFO Created log for partition __consumer_offsets-3 in /etc/confluent/kraft-combined-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,232] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2023-08-24 07:23:20,233] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,233] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-3 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,233] INFO [MergedLog partition=__consumer_offsets-3, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,238] INFO [MergedLog partition=__consumer_offsets-36, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,240] INFO Created log for partition __consumer_offsets-36 in /etc/confluent/kraft-combined-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,240] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2023-08-24 07:23:20,241] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,241] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-36 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,242] INFO [MergedLog partition=__consumer_offsets-36, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-36 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,246] INFO [MergedLog partition=__consumer_offsets-47, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,247] INFO Created log for partition __consumer_offsets-47 in /etc/confluent/kraft-combined-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,247] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2023-08-24 07:23:20,248] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,248] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-47 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,249] INFO [MergedLog partition=__consumer_offsets-47, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-47 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,253] INFO [MergedLog partition=__consumer_offsets-14, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,254] INFO Created log for partition __consumer_offsets-14 in /etc/confluent/kraft-combined-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,254] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2023-08-24 07:23:20,255] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,255] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-14 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,256] INFO [MergedLog partition=__consumer_offsets-14, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-14 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,259] INFO [MergedLog partition=__consumer_offsets-43, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,259] INFO Created log for partition __consumer_offsets-43 in /etc/confluent/kraft-combined-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,260] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2023-08-24 07:23:20,260] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,260] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-43 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,260] INFO [MergedLog partition=__consumer_offsets-43, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-43 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,264] INFO [MergedLog partition=__consumer_offsets-10, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,264] INFO Created log for partition __consumer_offsets-10 in /etc/confluent/kraft-combined-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,265] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2023-08-24 07:23:20,265] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,266] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-10 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,266] INFO [MergedLog partition=__consumer_offsets-10, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,269] INFO [MergedLog partition=__consumer_offsets-22, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,269] INFO Created log for partition __consumer_offsets-22 in /etc/confluent/kraft-combined-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,270] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2023-08-24 07:23:20,270] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,270] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-22 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,270] INFO [MergedLog partition=__consumer_offsets-22, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-22 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,273] INFO [MergedLog partition=__consumer_offsets-18, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,274] INFO Created log for partition __consumer_offsets-18 in /etc/confluent/kraft-combined-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,274] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2023-08-24 07:23:20,274] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,275] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-18 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,275] INFO [MergedLog partition=__consumer_offsets-18, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-18 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,278] INFO [MergedLog partition=__consumer_offsets-31, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,279] INFO Created log for partition __consumer_offsets-31 in /etc/confluent/kraft-combined-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,280] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2023-08-24 07:23:20,280] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,280] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-31 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,281] INFO [MergedLog partition=__consumer_offsets-31, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-31 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,284] INFO [MergedLog partition=__consumer_offsets-27, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,284] INFO Created log for partition __consumer_offsets-27 in /etc/confluent/kraft-combined-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,284] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2023-08-24 07:23:20,284] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,285] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-27 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,286] INFO [MergedLog partition=__consumer_offsets-27, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-27 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,288] INFO [MergedLog partition=__consumer_offsets-39, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,289] INFO Created log for partition __consumer_offsets-39 in /etc/confluent/kraft-combined-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,289] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2023-08-24 07:23:20,289] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,289] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-39 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,290] INFO [MergedLog partition=__consumer_offsets-39, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-39 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,293] INFO [MergedLog partition=__consumer_offsets-6, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,294] INFO Created log for partition __consumer_offsets-6 in /etc/confluent/kraft-combined-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,297] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2023-08-24 07:23:20,297] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,297] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-6 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,297] INFO [MergedLog partition=__consumer_offsets-6, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,300] INFO [MergedLog partition=__consumer_offsets-35, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,301] INFO Created log for partition __consumer_offsets-35 in /etc/confluent/kraft-combined-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,301] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2023-08-24 07:23:20,301] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,301] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-35 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,301] INFO [MergedLog partition=__consumer_offsets-35, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-35 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,306] INFO [MergedLog partition=__consumer_offsets-2, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:20,307] INFO Created log for partition __consumer_offsets-2 in /etc/confluent/kraft-combined-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-24 07:23:20,307] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2023-08-24 07:23:20,307] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:20,307] INFO Setting topicIdPartition GiOpbg6dQRWHiCdSwEc7qw:__consumer_offsets-2 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:20,307] INFO [MergedLog partition=__consumer_offsets-2, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:20,309] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,310] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,311] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,311] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,318] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,318] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,318] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,318] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,318] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,318] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,319] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,319] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,320] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,320] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,322] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,322] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,322] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,322] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,323] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,323] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,325] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,325] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,325] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,325] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,328] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,329] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,330] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,330] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,330] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,331] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,331] INFO The cleaning for partition __consumer_offsets-5 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,331] INFO The cleaning for partition __consumer_offsets-13 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,331] INFO The cleaning for partition __consumer_offsets-26 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,332] INFO The cleaning for partition __consumer_offsets-12 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,332] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,333] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,331] INFO The cleaning for partition __consumer_offsets-45 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,331] INFO The cleaning for partition __consumer_offsets-34 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,334] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,331] INFO The cleaning for partition __consumer_offsets-16 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,331] INFO The cleaning for partition __consumer_offsets-38 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,331] INFO The cleaning for partition __consumer_offsets-46 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,331] INFO The cleaning for partition __consumer_offsets-17 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,331] INFO The cleaning for partition __consumer_offsets-30 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,331] INFO The cleaning for partition __consumer_offsets-21 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,331] INFO The cleaning for partition __consumer_offsets-9 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,331] INFO The cleaning for partition __consumer_offsets-42 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,336] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,331] INFO The cleaning for partition __consumer_offsets-1 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,335] INFO The cleaning for partition __consumer_offsets-41 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,337] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,337] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,338] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,339] INFO The cleaning for partition __consumer_offsets-20 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,339] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,339] INFO The cleaning for partition __consumer_offsets-24 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,340] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,346] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 25 milliseconds for epoch 0, of which 16 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,347] INFO The cleaning for partition __consumer_offsets-49 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,347] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,347] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,347] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,347] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,348] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,348] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,348] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,348] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,348] INFO The cleaning for partition __consumer_offsets-29 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,348] INFO The cleaning for partition __consumer_offsets-25 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,349] INFO The cleaning for partition __consumer_offsets-0 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,349] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,349] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 2 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,349] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,350] INFO The cleaning for partition __consumer_offsets-8 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,350] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 12 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,350] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 33 milliseconds for epoch 0, of which 26 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,350] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 30 milliseconds for epoch 0, of which 22 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,350] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 20 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,350] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 25 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,351] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 32 milliseconds for epoch 0, of which 18 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,351] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 20 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,350] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 7 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,351] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 18 milliseconds for epoch 0, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,351] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 11 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,351] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 28 milliseconds for epoch 0, of which 13 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,351] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 32 milliseconds for epoch 0, of which 16 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,352] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 30 milliseconds for epoch 0, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,353] INFO Cleaning for partition __consumer_offsets-26 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,354] INFO Cleaning for partition __consumer_offsets-13 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,351] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 33 milliseconds for epoch 0, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,354] INFO Cleaning for partition __consumer_offsets-42 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,354] INFO Cleaning for partition __consumer_offsets-16 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,354] INFO Cleaning for partition __consumer_offsets-30 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,354] INFO Cleaning for partition __consumer_offsets-49 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,358] INFO Cleaning for partition __consumer_offsets-25 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,354] INFO Cleaning for partition __consumer_offsets-5 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,354] INFO Cleaning for partition __consumer_offsets-21 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,354] INFO Cleaning for partition __consumer_offsets-41 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,353] INFO Cleaning for partition __consumer_offsets-38 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,353] INFO Cleaning for partition __consumer_offsets-12 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,353] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 15 milliseconds for epoch 0, of which 14 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,368] INFO Cleaning for partition __consumer_offsets-1 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,353] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 35 milliseconds for epoch 0, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,368] INFO Cleaning for partition __consumer_offsets-9 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,369] INFO Cleaning for partition __consumer_offsets-46 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,353] INFO Cleaning for partition __consumer_offsets-34 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,363] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 16 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,369] INFO Cleaning for partition __consumer_offsets-0 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,361] INFO Cleaning for partition __consumer_offsets-45 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,361] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 13 milliseconds for epoch 0, of which 13 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,370] INFO Cleaning for partition __consumer_offsets-8 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,361] INFO Cleaning for partition __consumer_offsets-17 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,361] INFO Cleaning for partition __consumer_offsets-20 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,361] INFO Cleaning for partition __consumer_offsets-29 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,361] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,375] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,376] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,376] INFO The cleaning for partition __consumer_offsets-37 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,377] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,377] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,377] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,377] INFO The cleaning for partition __consumer_offsets-4 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,378] INFO Cleaning for partition __consumer_offsets-37 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,378] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,378] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 4 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,379] INFO Cleaning for partition __consumer_offsets-24 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,380] INFO The cleaning for partition __consumer_offsets-33 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,378] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,379] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,380] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,380] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,380] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,380] INFO Cleaning for partition __consumer_offsets-33 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,380] INFO Cleaning for partition __consumer_offsets-4 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,380] INFO The cleaning for partition __consumer_offsets-15 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,381] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 1 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,381] INFO Cleaning for partition __consumer_offsets-15 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,382] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,382] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,388] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,388] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,389] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,389] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,389] INFO The cleaning for partition __consumer_offsets-48 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,390] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 10 milliseconds for epoch 0, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,390] INFO The cleaning for partition __consumer_offsets-11 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,390] INFO Cleaning for partition __consumer_offsets-48 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,391] INFO The cleaning for partition __consumer_offsets-44 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,391] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,391] INFO Cleaning for partition __consumer_offsets-11 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,391] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,393] INFO The cleaning for partition __consumer_offsets-23 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,393] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,394] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,394] INFO Cleaning for partition __consumer_offsets-44 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,394] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,394] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,394] INFO Cleaning for partition __consumer_offsets-23 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,394] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,394] INFO The cleaning for partition __consumer_offsets-19 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,394] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,394] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,394] INFO The cleaning for partition __consumer_offsets-32 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,395] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,395] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,395] INFO Cleaning for partition __consumer_offsets-19 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,395] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,395] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,395] INFO Cleaning for partition __consumer_offsets-32 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,395] INFO The cleaning for partition __consumer_offsets-28 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,395] INFO The cleaning for partition __consumer_offsets-7 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,395] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,396] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,396] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,396] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,396] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,396] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,396] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,396] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,396] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,396] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,396] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,396] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,396] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,396] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,396] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,396] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,396] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,396] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,396] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,396] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,396] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,396] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,396] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,396] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,396] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,396] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,396] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,396] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,396] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,396] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,396] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,397] INFO The cleaning for partition __consumer_offsets-36 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,397] INFO The cleaning for partition __consumer_offsets-40 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,397] INFO The cleaning for partition __consumer_offsets-43 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,397] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,397] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600,confluent.placement.constraints ->  (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:20,397] INFO The cleaning for partition __consumer_offsets-47 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,397] INFO The cleaning for partition __consumer_offsets-3 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,398] INFO Cleaning for partition __consumer_offsets-43 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,398] INFO The cleaning for partition __consumer_offsets-10 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,397] INFO Cleaning for partition __consumer_offsets-28 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,398] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,399] INFO Cleaning for partition __consumer_offsets-36 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,397] INFO The cleaning for partition __consumer_offsets-14 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,399] INFO The cleaning for partition __consumer_offsets-22 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,399] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 3 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,399] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,399] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,399] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 3 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,399] INFO Cleaning for partition __consumer_offsets-10 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,399] INFO The cleaning for partition __consumer_offsets-6 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,399] INFO The cleaning for partition __consumer_offsets-39 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,399] INFO The cleaning for partition __consumer_offsets-27 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,399] INFO The cleaning for partition __consumer_offsets-31 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,400] INFO The cleaning for partition __consumer_offsets-35 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,400] INFO The cleaning for partition __consumer_offsets-2 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,400] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 4 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,400] INFO Cleaning for partition __consumer_offsets-14 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,400] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,400] INFO The cleaning for partition __consumer_offsets-18 is aborted and paused (kafka.log.LogManager)
[2023-08-24 07:23:20,400] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,401] INFO Cleaning for partition __consumer_offsets-35 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,401] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 5 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,401] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,401] INFO Cleaning for partition __consumer_offsets-7 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,402] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 6 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,402] INFO Cleaning for partition __consumer_offsets-31 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,400] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 4 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,401] INFO Cleaning for partition __consumer_offsets-27 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,404] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,405] INFO Cleaning for partition __consumer_offsets-39 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,405] INFO Cleaning for partition __consumer_offsets-3 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,405] INFO Cleaning for partition __consumer_offsets-22 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,405] INFO Cleaning for partition __consumer_offsets-47 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,406] INFO Cleaning for partition __consumer_offsets-40 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,406] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 10 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,406] INFO Cleaning for partition __consumer_offsets-6 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,407] INFO Cleaning for partition __consumer_offsets-2 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,407] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 11 milliseconds for epoch 0, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-24 07:23:20,407] INFO Cleaning for partition __consumer_offsets-18 is resumed (kafka.log.LogManager)
[2023-08-24 07:23:20,492] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-a240a9bc-551f-4ad8-a4f9-afcfccdcb14e and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,506] INFO [GroupCoordinator 1]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 0 (__consumer_offsets-29) (reason: Adding new member sr-1-a240a9bc-551f-4ad8-a4f9-afcfccdcb14e with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,514] INFO [GroupCoordinator 1]: Stabilized group schema-registry generation 1 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:20,539] INFO [GroupCoordinator 1]: Assignment received from leader sr-1-a240a9bc-551f-4ad8-a4f9-afcfccdcb14e for group schema-registry for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:27,281] ERROR [KafkaApi-1] Unexpected error handling request RequestHeader(apiKey=INIT_PRODUCER_ID, apiVersion=4, clientId=confluent-control-center-heartbeat-sender-1, correlationId=2, headerVersion=2) -- InitProducerIdRequestData(transactionalId=null, transactionTimeoutMs=2147483647, producerId=-1, producerEpoch=-1) with context RequestContext(header=RequestHeader(apiKey=INIT_PRODUCER_ID, apiVersion=4, clientId=confluent-control-center-heartbeat-sender-1, correlationId=2, headerVersion=2), connectionId='172.20.0.2:19092-172.20.0.6:40044-26', clientAddress=/172.20.0.6, principal=User:ANONYMOUS, listenerName=ListenerName(INTERNAL), securityProtocol=PLAINTEXT, clientInformation=ClientInformation(softwareName=apache-kafka-java, softwareVersion=7.4.1-ce), fromPrivilegedListener=true, principalSerde=Optional[org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder@5e4ed1a7]) (kafka.server.KafkaApis)
org.apache.kafka.common.errors.CoordinatorLoadInProgressException: Timed out waiting for next producer ID block
[2023-08-24 07:23:29,684] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-metrics-11, _confluent-metrics-9, _confluent-metrics-10, _confluent-metrics-7, _confluent-metrics-8, _confluent-metrics-5, _confluent-metrics-6, _confluent-metrics-3, _confluent-metrics-4, _confluent-metrics-1, _confluent-metrics-2, _confluent-metrics-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:29,709] INFO [MergedLog partition=_confluent-metrics-11, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:29,710] INFO Created log for partition _confluent-metrics-11 in /etc/confluent/kraft-combined-logs/_confluent-metrics-11 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-24 07:23:29,716] INFO [Partition _confluent-metrics-11 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-11 (kafka.cluster.Partition)
[2023-08-24 07:23:29,716] INFO [Partition _confluent-metrics-11 broker=1] Log loaded for partition _confluent-metrics-11 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:29,716] INFO Setting topicIdPartition DyeBTBMOQCCdN9YUy3tpGw:_confluent-metrics-11 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:29,717] INFO [MergedLog partition=_confluent-metrics-11, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:29,722] INFO [MergedLog partition=_confluent-metrics-9, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:29,724] INFO Created log for partition _confluent-metrics-9 in /etc/confluent/kraft-combined-logs/_confluent-metrics-9 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-24 07:23:29,724] INFO [Partition _confluent-metrics-9 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-9 (kafka.cluster.Partition)
[2023-08-24 07:23:29,724] INFO [Partition _confluent-metrics-9 broker=1] Log loaded for partition _confluent-metrics-9 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:29,724] INFO Setting topicIdPartition DyeBTBMOQCCdN9YUy3tpGw:_confluent-metrics-9 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:29,725] INFO [MergedLog partition=_confluent-metrics-9, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:29,728] INFO [MergedLog partition=_confluent-metrics-10, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:29,729] INFO Created log for partition _confluent-metrics-10 in /etc/confluent/kraft-combined-logs/_confluent-metrics-10 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-24 07:23:29,731] INFO [Partition _confluent-metrics-10 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-10 (kafka.cluster.Partition)
[2023-08-24 07:23:29,731] INFO [Partition _confluent-metrics-10 broker=1] Log loaded for partition _confluent-metrics-10 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:29,732] INFO Setting topicIdPartition DyeBTBMOQCCdN9YUy3tpGw:_confluent-metrics-10 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:29,732] INFO [MergedLog partition=_confluent-metrics-10, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:29,736] INFO [MergedLog partition=_confluent-metrics-7, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:29,737] INFO Created log for partition _confluent-metrics-7 in /etc/confluent/kraft-combined-logs/_confluent-metrics-7 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-24 07:23:29,738] INFO [Partition _confluent-metrics-7 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-7 (kafka.cluster.Partition)
[2023-08-24 07:23:29,739] INFO [Partition _confluent-metrics-7 broker=1] Log loaded for partition _confluent-metrics-7 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:29,739] INFO Setting topicIdPartition DyeBTBMOQCCdN9YUy3tpGw:_confluent-metrics-7 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:29,740] INFO [MergedLog partition=_confluent-metrics-7, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:29,743] INFO [MergedLog partition=_confluent-metrics-8, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:29,748] INFO Created log for partition _confluent-metrics-8 in /etc/confluent/kraft-combined-logs/_confluent-metrics-8 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-24 07:23:29,748] INFO [Partition _confluent-metrics-8 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-8 (kafka.cluster.Partition)
[2023-08-24 07:23:29,748] INFO [Partition _confluent-metrics-8 broker=1] Log loaded for partition _confluent-metrics-8 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:29,748] INFO Setting topicIdPartition DyeBTBMOQCCdN9YUy3tpGw:_confluent-metrics-8 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:29,748] INFO [MergedLog partition=_confluent-metrics-8, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:29,752] INFO [MergedLog partition=_confluent-metrics-5, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:29,753] INFO Created log for partition _confluent-metrics-5 in /etc/confluent/kraft-combined-logs/_confluent-metrics-5 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-24 07:23:29,753] INFO [Partition _confluent-metrics-5 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-5 (kafka.cluster.Partition)
[2023-08-24 07:23:29,753] INFO [Partition _confluent-metrics-5 broker=1] Log loaded for partition _confluent-metrics-5 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:29,753] INFO Setting topicIdPartition DyeBTBMOQCCdN9YUy3tpGw:_confluent-metrics-5 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:29,753] INFO [MergedLog partition=_confluent-metrics-5, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:29,756] INFO [MergedLog partition=_confluent-metrics-6, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:29,757] INFO Created log for partition _confluent-metrics-6 in /etc/confluent/kraft-combined-logs/_confluent-metrics-6 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-24 07:23:29,757] INFO [Partition _confluent-metrics-6 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-6 (kafka.cluster.Partition)
[2023-08-24 07:23:29,757] INFO [Partition _confluent-metrics-6 broker=1] Log loaded for partition _confluent-metrics-6 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:29,757] INFO Setting topicIdPartition DyeBTBMOQCCdN9YUy3tpGw:_confluent-metrics-6 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:29,758] INFO [MergedLog partition=_confluent-metrics-6, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:29,764] INFO [MergedLog partition=_confluent-metrics-3, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:29,765] INFO Created log for partition _confluent-metrics-3 in /etc/confluent/kraft-combined-logs/_confluent-metrics-3 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-24 07:23:29,765] INFO [Partition _confluent-metrics-3 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-3 (kafka.cluster.Partition)
[2023-08-24 07:23:29,766] INFO [Partition _confluent-metrics-3 broker=1] Log loaded for partition _confluent-metrics-3 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:29,766] INFO Setting topicIdPartition DyeBTBMOQCCdN9YUy3tpGw:_confluent-metrics-3 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:29,766] INFO [MergedLog partition=_confluent-metrics-3, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:29,772] INFO [MergedLog partition=_confluent-metrics-4, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:29,773] INFO Created log for partition _confluent-metrics-4 in /etc/confluent/kraft-combined-logs/_confluent-metrics-4 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-24 07:23:29,774] INFO [Partition _confluent-metrics-4 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-4 (kafka.cluster.Partition)
[2023-08-24 07:23:29,774] INFO [Partition _confluent-metrics-4 broker=1] Log loaded for partition _confluent-metrics-4 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:29,775] INFO Setting topicIdPartition DyeBTBMOQCCdN9YUy3tpGw:_confluent-metrics-4 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:29,794] INFO [MergedLog partition=_confluent-metrics-4, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:29,802] INFO [MergedLog partition=_confluent-metrics-1, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:29,804] INFO Created log for partition _confluent-metrics-1 in /etc/confluent/kraft-combined-logs/_confluent-metrics-1 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-24 07:23:29,817] INFO [Partition _confluent-metrics-1 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-1 (kafka.cluster.Partition)
[2023-08-24 07:23:29,817] INFO [Partition _confluent-metrics-1 broker=1] Log loaded for partition _confluent-metrics-1 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:29,817] INFO Setting topicIdPartition DyeBTBMOQCCdN9YUy3tpGw:_confluent-metrics-1 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:29,817] INFO [MergedLog partition=_confluent-metrics-1, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:29,821] INFO [MergedLog partition=_confluent-metrics-2, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:29,822] INFO Created log for partition _confluent-metrics-2 in /etc/confluent/kraft-combined-logs/_confluent-metrics-2 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-24 07:23:29,822] INFO [Partition _confluent-metrics-2 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-2 (kafka.cluster.Partition)
[2023-08-24 07:23:29,823] INFO [Partition _confluent-metrics-2 broker=1] Log loaded for partition _confluent-metrics-2 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:29,823] INFO Setting topicIdPartition DyeBTBMOQCCdN9YUy3tpGw:_confluent-metrics-2 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:29,823] INFO [MergedLog partition=_confluent-metrics-2, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:29,827] INFO [MergedLog partition=_confluent-metrics-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:29,828] INFO Created log for partition _confluent-metrics-0 in /etc/confluent/kraft-combined-logs/_confluent-metrics-0 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-24 07:23:29,828] INFO [Partition _confluent-metrics-0 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-0 (kafka.cluster.Partition)
[2023-08-24 07:23:29,828] INFO [Partition _confluent-metrics-0 broker=1] Log loaded for partition _confluent-metrics-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:29,829] INFO Setting topicIdPartition DyeBTBMOQCCdN9YUy3tpGw:_confluent-metrics-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:29,829] INFO [MergedLog partition=_confluent-metrics-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:29,830] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-metrics with new configuration : max.message.bytes -> 10485760,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,retention.ms -> 259200000,segment.ms -> 14400000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:31,552] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-ksql-ksqldb-01_command_topic-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:31,556] INFO [MergedLog partition=_confluent-ksql-ksqldb-01_command_topic-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:31,557] INFO Created log for partition _confluent-ksql-ksqldb-01_command_topic-0 in /etc/confluent/kraft-combined-logs/_confluent-ksql-ksqldb-01_command_topic-0 with properties {cleanup.policy=delete, min.insync.replicas=1, retention.ms=-1, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:31,559] INFO [Partition _confluent-ksql-ksqldb-01_command_topic-0 broker=1] No checkpointed highwatermark is found for partition _confluent-ksql-ksqldb-01_command_topic-0 (kafka.cluster.Partition)
[2023-08-24 07:23:31,559] INFO [Partition _confluent-ksql-ksqldb-01_command_topic-0 broker=1] Log loaded for partition _confluent-ksql-ksqldb-01_command_topic-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:31,559] INFO Setting topicIdPartition fEY9AgDLRYq3wqqXacLtdw:_confluent-ksql-ksqldb-01_command_topic-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:31,559] INFO [MergedLog partition=_confluent-ksql-ksqldb-01_command_topic-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-ksql-ksqldb-01_command_topic-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:31,560] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-ksql-ksqldb-01_command_topic with new configuration : cleanup.policy -> delete,min.insync.replicas -> 1,retention.ms -> -1,unclean.leader.election.enable -> false (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:31,593] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(ksqldb-01ksql_processing_log-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:31,597] INFO [MergedLog partition=ksqldb-01ksql_processing_log-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:31,597] INFO Created log for partition ksqldb-01ksql_processing_log-0 in /etc/confluent/kraft-combined-logs/ksqldb-01ksql_processing_log-0 with properties {} (kafka.log.LogManager)
[2023-08-24 07:23:31,598] INFO [Partition ksqldb-01ksql_processing_log-0 broker=1] No checkpointed highwatermark is found for partition ksqldb-01ksql_processing_log-0 (kafka.cluster.Partition)
[2023-08-24 07:23:31,599] INFO [Partition ksqldb-01ksql_processing_log-0 broker=1] Log loaded for partition ksqldb-01ksql_processing_log-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:31,599] INFO Setting topicIdPartition STLeYzTlRNab6pgIIDkl7Q:ksqldb-01ksql_processing_log-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:31,599] INFO [MergedLog partition=ksqldb-01ksql_processing_log-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for ksqldb-01ksql_processing_log-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,044] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,051] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,063] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:32,068] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,068] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,068] INFO Setting topicIdPartition p7pFYvAwT7SKYIr4RRxCCw:_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,069] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,069] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:32,087] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,094] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,095] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:32,097] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,097] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,097] INFO Setting topicIdPartition 67EiJB0_SPGEvFHQFNVg0g:_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,098] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,098] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:32,127] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,132] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,134] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:32,135] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,136] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,136] INFO Setting topicIdPartition xMhkbxkXSpWRg-arDBcRuA:_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,136] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,137] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:32,163] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,167] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,168] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:32,170] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,170] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,170] INFO Setting topicIdPartition YLMK8MSfTuWj2cgAFL6tNw:_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,170] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,171] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:32,196] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,201] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,203] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:32,205] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,205] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,205] INFO Setting topicIdPartition xHlQU1eqTtePf5reRNOgfQ:_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,205] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,206] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:32,231] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,236] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,237] INFO Created log for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:32,240] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,240] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,241] INFO Setting topicIdPartition 2uk7pTRtSkSMsFWQ4kxyog:_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,241] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,241] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:32,273] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,279] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,279] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:32,280] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,281] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,281] INFO Setting topicIdPartition dc5mwuWJQde0FzkIzBb6aw:_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,281] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,282] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:32,304] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,310] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,311] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:32,312] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,312] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,312] INFO Setting topicIdPartition znoLnkBZSlyLqZk3CqQieg:_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,313] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,314] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:32,321] INFO Sent auto-creation request for Set(__transaction_state) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2023-08-24 07:23:32,331] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,339] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,341] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
[2023-08-24 07:23:32,342] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,342] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,342] INFO Setting topicIdPartition PMCCoZGsRIuYor0VTfnGbw:_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,342] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,343] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:32,369] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__transaction_state-42, __transaction_state-13, __transaction_state-46, __transaction_state-17, __transaction_state-34, __transaction_state-5, __transaction_state-38, __transaction_state-9, __transaction_state-26, __transaction_state-30, __transaction_state-1, __transaction_state-18, __transaction_state-22, __transaction_state-12, __transaction_state-45, __transaction_state-16, __transaction_state-49, __transaction_state-4, __transaction_state-37, __transaction_state-8, __transaction_state-41, __transaction_state-29, __transaction_state-0, __transaction_state-33, __transaction_state-21, __transaction_state-25, __transaction_state-11, __transaction_state-44, __transaction_state-15, __transaction_state-48, __transaction_state-3, __transaction_state-36, __transaction_state-7, __transaction_state-40, __transaction_state-28, __transaction_state-32, __transaction_state-20, __transaction_state-24, __transaction_state-10, __transaction_state-43, __transaction_state-14, __transaction_state-47, __transaction_state-2, __transaction_state-35, __transaction_state-6, __transaction_state-39, __transaction_state-27, __transaction_state-31, __transaction_state-19, __transaction_state-23) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,417] INFO [MergedLog partition=__transaction_state-42, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,418] INFO Created log for partition __transaction_state-42 in /etc/confluent/kraft-combined-logs/__transaction_state-42 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,419] INFO [Partition __transaction_state-42 broker=1] No checkpointed highwatermark is found for partition __transaction_state-42 (kafka.cluster.Partition)
[2023-08-24 07:23:32,419] INFO [Partition __transaction_state-42 broker=1] Log loaded for partition __transaction_state-42 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,419] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-42 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,420] INFO [MergedLog partition=__transaction_state-42, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-42 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,422] INFO [MergedLog partition=__transaction_state-13, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,423] INFO Created log for partition __transaction_state-13 in /etc/confluent/kraft-combined-logs/__transaction_state-13 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,423] INFO [Partition __transaction_state-13 broker=1] No checkpointed highwatermark is found for partition __transaction_state-13 (kafka.cluster.Partition)
[2023-08-24 07:23:32,423] INFO [Partition __transaction_state-13 broker=1] Log loaded for partition __transaction_state-13 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,423] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-13 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,425] INFO [MergedLog partition=__transaction_state-13, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-13 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,427] INFO [MergedLog partition=__transaction_state-46, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,428] INFO Created log for partition __transaction_state-46 in /etc/confluent/kraft-combined-logs/__transaction_state-46 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,428] INFO [Partition __transaction_state-46 broker=1] No checkpointed highwatermark is found for partition __transaction_state-46 (kafka.cluster.Partition)
[2023-08-24 07:23:32,428] INFO [Partition __transaction_state-46 broker=1] Log loaded for partition __transaction_state-46 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,429] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-46 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,429] INFO [MergedLog partition=__transaction_state-46, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-46 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,434] INFO [MergedLog partition=__transaction_state-17, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,435] INFO Created log for partition __transaction_state-17 in /etc/confluent/kraft-combined-logs/__transaction_state-17 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,435] INFO [Partition __transaction_state-17 broker=1] No checkpointed highwatermark is found for partition __transaction_state-17 (kafka.cluster.Partition)
[2023-08-24 07:23:32,436] INFO [Partition __transaction_state-17 broker=1] Log loaded for partition __transaction_state-17 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,436] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-17 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,436] INFO [MergedLog partition=__transaction_state-17, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-17 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,440] INFO [MergedLog partition=__transaction_state-34, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,440] INFO Created log for partition __transaction_state-34 in /etc/confluent/kraft-combined-logs/__transaction_state-34 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,441] INFO [Partition __transaction_state-34 broker=1] No checkpointed highwatermark is found for partition __transaction_state-34 (kafka.cluster.Partition)
[2023-08-24 07:23:32,442] INFO [Partition __transaction_state-34 broker=1] Log loaded for partition __transaction_state-34 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,442] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-34 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,445] INFO [MergedLog partition=__transaction_state-34, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-34 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,448] INFO [MergedLog partition=__transaction_state-5, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,448] INFO Created log for partition __transaction_state-5 in /etc/confluent/kraft-combined-logs/__transaction_state-5 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,448] INFO [Partition __transaction_state-5 broker=1] No checkpointed highwatermark is found for partition __transaction_state-5 (kafka.cluster.Partition)
[2023-08-24 07:23:32,450] INFO [Partition __transaction_state-5 broker=1] Log loaded for partition __transaction_state-5 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,450] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-5 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,450] INFO [MergedLog partition=__transaction_state-5, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,453] INFO [MergedLog partition=__transaction_state-38, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,453] INFO Created log for partition __transaction_state-38 in /etc/confluent/kraft-combined-logs/__transaction_state-38 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,454] INFO [Partition __transaction_state-38 broker=1] No checkpointed highwatermark is found for partition __transaction_state-38 (kafka.cluster.Partition)
[2023-08-24 07:23:32,454] INFO [Partition __transaction_state-38 broker=1] Log loaded for partition __transaction_state-38 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,454] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-38 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,454] INFO [MergedLog partition=__transaction_state-38, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-38 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,457] INFO [MergedLog partition=__transaction_state-9, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,458] INFO Created log for partition __transaction_state-9 in /etc/confluent/kraft-combined-logs/__transaction_state-9 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,459] INFO [Partition __transaction_state-9 broker=1] No checkpointed highwatermark is found for partition __transaction_state-9 (kafka.cluster.Partition)
[2023-08-24 07:23:32,459] INFO [Partition __transaction_state-9 broker=1] Log loaded for partition __transaction_state-9 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,460] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-9 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,460] INFO [MergedLog partition=__transaction_state-9, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,463] INFO [MergedLog partition=__transaction_state-26, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,464] INFO Created log for partition __transaction_state-26 in /etc/confluent/kraft-combined-logs/__transaction_state-26 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,464] INFO [Partition __transaction_state-26 broker=1] No checkpointed highwatermark is found for partition __transaction_state-26 (kafka.cluster.Partition)
[2023-08-24 07:23:32,465] INFO [Partition __transaction_state-26 broker=1] Log loaded for partition __transaction_state-26 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,465] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-26 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,465] INFO [MergedLog partition=__transaction_state-26, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-26 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,475] INFO [MergedLog partition=__transaction_state-30, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,476] INFO Created log for partition __transaction_state-30 in /etc/confluent/kraft-combined-logs/__transaction_state-30 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,479] INFO [Partition __transaction_state-30 broker=1] No checkpointed highwatermark is found for partition __transaction_state-30 (kafka.cluster.Partition)
[2023-08-24 07:23:32,479] INFO [Partition __transaction_state-30 broker=1] Log loaded for partition __transaction_state-30 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,480] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-30 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,483] INFO [MergedLog partition=__transaction_state-30, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-30 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,487] INFO [MergedLog partition=__transaction_state-1, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,488] INFO Created log for partition __transaction_state-1 in /etc/confluent/kraft-combined-logs/__transaction_state-1 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,489] INFO [Partition __transaction_state-1 broker=1] No checkpointed highwatermark is found for partition __transaction_state-1 (kafka.cluster.Partition)
[2023-08-24 07:23:32,489] INFO [Partition __transaction_state-1 broker=1] Log loaded for partition __transaction_state-1 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,489] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-1 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,490] INFO [MergedLog partition=__transaction_state-1, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,493] INFO [MergedLog partition=__transaction_state-18, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,494] INFO Created log for partition __transaction_state-18 in /etc/confluent/kraft-combined-logs/__transaction_state-18 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,494] INFO [Partition __transaction_state-18 broker=1] No checkpointed highwatermark is found for partition __transaction_state-18 (kafka.cluster.Partition)
[2023-08-24 07:23:32,495] INFO [Partition __transaction_state-18 broker=1] Log loaded for partition __transaction_state-18 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,495] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-18 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,496] INFO [MergedLog partition=__transaction_state-18, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-18 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,498] INFO [MergedLog partition=__transaction_state-22, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,498] INFO Created log for partition __transaction_state-22 in /etc/confluent/kraft-combined-logs/__transaction_state-22 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,498] INFO [Partition __transaction_state-22 broker=1] No checkpointed highwatermark is found for partition __transaction_state-22 (kafka.cluster.Partition)
[2023-08-24 07:23:32,500] INFO [Partition __transaction_state-22 broker=1] Log loaded for partition __transaction_state-22 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,500] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-22 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,500] INFO [MergedLog partition=__transaction_state-22, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-22 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,504] INFO [MergedLog partition=__transaction_state-12, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,504] INFO Created log for partition __transaction_state-12 in /etc/confluent/kraft-combined-logs/__transaction_state-12 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,505] INFO [Partition __transaction_state-12 broker=1] No checkpointed highwatermark is found for partition __transaction_state-12 (kafka.cluster.Partition)
[2023-08-24 07:23:32,506] INFO [Partition __transaction_state-12 broker=1] Log loaded for partition __transaction_state-12 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,506] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-12 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,506] INFO [MergedLog partition=__transaction_state-12, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-12 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,509] INFO [MergedLog partition=__transaction_state-45, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,510] INFO Created log for partition __transaction_state-45 in /etc/confluent/kraft-combined-logs/__transaction_state-45 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,511] INFO [Partition __transaction_state-45 broker=1] No checkpointed highwatermark is found for partition __transaction_state-45 (kafka.cluster.Partition)
[2023-08-24 07:23:32,511] INFO [Partition __transaction_state-45 broker=1] Log loaded for partition __transaction_state-45 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,512] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-45 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,513] INFO [MergedLog partition=__transaction_state-45, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-45 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,516] INFO [MergedLog partition=__transaction_state-16, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,517] INFO Created log for partition __transaction_state-16 in /etc/confluent/kraft-combined-logs/__transaction_state-16 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,518] INFO [Partition __transaction_state-16 broker=1] No checkpointed highwatermark is found for partition __transaction_state-16 (kafka.cluster.Partition)
[2023-08-24 07:23:32,518] INFO [Partition __transaction_state-16 broker=1] Log loaded for partition __transaction_state-16 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,518] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-16 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,518] INFO [MergedLog partition=__transaction_state-16, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-16 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,521] INFO [MergedLog partition=__transaction_state-49, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,522] INFO Created log for partition __transaction_state-49 in /etc/confluent/kraft-combined-logs/__transaction_state-49 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,522] INFO [Partition __transaction_state-49 broker=1] No checkpointed highwatermark is found for partition __transaction_state-49 (kafka.cluster.Partition)
[2023-08-24 07:23:32,522] INFO [Partition __transaction_state-49 broker=1] Log loaded for partition __transaction_state-49 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,523] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-49 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,523] INFO [MergedLog partition=__transaction_state-49, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-49 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,526] INFO [MergedLog partition=__transaction_state-4, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,526] INFO Created log for partition __transaction_state-4 in /etc/confluent/kraft-combined-logs/__transaction_state-4 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,527] INFO [Partition __transaction_state-4 broker=1] No checkpointed highwatermark is found for partition __transaction_state-4 (kafka.cluster.Partition)
[2023-08-24 07:23:32,527] INFO [Partition __transaction_state-4 broker=1] Log loaded for partition __transaction_state-4 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,527] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-4 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,528] INFO [MergedLog partition=__transaction_state-4, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,530] INFO [MergedLog partition=__transaction_state-37, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,531] INFO Created log for partition __transaction_state-37 in /etc/confluent/kraft-combined-logs/__transaction_state-37 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,531] INFO [Partition __transaction_state-37 broker=1] No checkpointed highwatermark is found for partition __transaction_state-37 (kafka.cluster.Partition)
[2023-08-24 07:23:32,531] INFO [Partition __transaction_state-37 broker=1] Log loaded for partition __transaction_state-37 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,532] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-37 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,532] INFO [MergedLog partition=__transaction_state-37, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-37 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,551] INFO [MergedLog partition=__transaction_state-8, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,551] INFO Created log for partition __transaction_state-8 in /etc/confluent/kraft-combined-logs/__transaction_state-8 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,551] INFO [Partition __transaction_state-8 broker=1] No checkpointed highwatermark is found for partition __transaction_state-8 (kafka.cluster.Partition)
[2023-08-24 07:23:32,552] INFO [Partition __transaction_state-8 broker=1] Log loaded for partition __transaction_state-8 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,552] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-8 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,552] INFO [MergedLog partition=__transaction_state-8, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,558] INFO [MergedLog partition=__transaction_state-41, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,559] INFO Created log for partition __transaction_state-41 in /etc/confluent/kraft-combined-logs/__transaction_state-41 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,566] INFO [Partition __transaction_state-41 broker=1] No checkpointed highwatermark is found for partition __transaction_state-41 (kafka.cluster.Partition)
[2023-08-24 07:23:32,566] INFO [Partition __transaction_state-41 broker=1] Log loaded for partition __transaction_state-41 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,566] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-41 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,567] INFO [MergedLog partition=__transaction_state-41, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-41 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,569] INFO [MergedLog partition=__transaction_state-29, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,570] INFO Created log for partition __transaction_state-29 in /etc/confluent/kraft-combined-logs/__transaction_state-29 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,570] INFO [Partition __transaction_state-29 broker=1] No checkpointed highwatermark is found for partition __transaction_state-29 (kafka.cluster.Partition)
[2023-08-24 07:23:32,571] INFO [Partition __transaction_state-29 broker=1] Log loaded for partition __transaction_state-29 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,572] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-29 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,572] INFO [MergedLog partition=__transaction_state-29, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-29 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,574] INFO [MergedLog partition=__transaction_state-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,575] INFO Created log for partition __transaction_state-0 in /etc/confluent/kraft-combined-logs/__transaction_state-0 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,575] INFO [Partition __transaction_state-0 broker=1] No checkpointed highwatermark is found for partition __transaction_state-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,575] INFO [Partition __transaction_state-0 broker=1] Log loaded for partition __transaction_state-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,577] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,577] INFO [MergedLog partition=__transaction_state-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,579] INFO [MergedLog partition=__transaction_state-33, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,580] INFO Created log for partition __transaction_state-33 in /etc/confluent/kraft-combined-logs/__transaction_state-33 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,580] INFO [Partition __transaction_state-33 broker=1] No checkpointed highwatermark is found for partition __transaction_state-33 (kafka.cluster.Partition)
[2023-08-24 07:23:32,580] INFO [Partition __transaction_state-33 broker=1] Log loaded for partition __transaction_state-33 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,582] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-33 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,582] INFO [MergedLog partition=__transaction_state-33, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-33 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,584] INFO [MergedLog partition=__transaction_state-21, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,585] INFO Created log for partition __transaction_state-21 in /etc/confluent/kraft-combined-logs/__transaction_state-21 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,586] INFO [Partition __transaction_state-21 broker=1] No checkpointed highwatermark is found for partition __transaction_state-21 (kafka.cluster.Partition)
[2023-08-24 07:23:32,586] INFO [Partition __transaction_state-21 broker=1] Log loaded for partition __transaction_state-21 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,587] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-21 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,587] INFO [MergedLog partition=__transaction_state-21, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-21 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,589] INFO [MergedLog partition=__transaction_state-25, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,590] INFO Created log for partition __transaction_state-25 in /etc/confluent/kraft-combined-logs/__transaction_state-25 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,590] INFO [Partition __transaction_state-25 broker=1] No checkpointed highwatermark is found for partition __transaction_state-25 (kafka.cluster.Partition)
[2023-08-24 07:23:32,590] INFO [Partition __transaction_state-25 broker=1] Log loaded for partition __transaction_state-25 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,592] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-25 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,592] INFO [MergedLog partition=__transaction_state-25, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-25 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,597] INFO [MergedLog partition=__transaction_state-11, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,598] INFO Created log for partition __transaction_state-11 in /etc/confluent/kraft-combined-logs/__transaction_state-11 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,599] INFO [Partition __transaction_state-11 broker=1] No checkpointed highwatermark is found for partition __transaction_state-11 (kafka.cluster.Partition)
[2023-08-24 07:23:32,599] INFO [Partition __transaction_state-11 broker=1] Log loaded for partition __transaction_state-11 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,599] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-11 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,599] INFO [MergedLog partition=__transaction_state-11, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,602] INFO [MergedLog partition=__transaction_state-44, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,602] INFO Created log for partition __transaction_state-44 in /etc/confluent/kraft-combined-logs/__transaction_state-44 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,602] INFO [Partition __transaction_state-44 broker=1] No checkpointed highwatermark is found for partition __transaction_state-44 (kafka.cluster.Partition)
[2023-08-24 07:23:32,602] INFO [Partition __transaction_state-44 broker=1] Log loaded for partition __transaction_state-44 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,602] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-44 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,602] INFO [MergedLog partition=__transaction_state-44, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-44 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,607] INFO [MergedLog partition=__transaction_state-15, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,607] INFO Created log for partition __transaction_state-15 in /etc/confluent/kraft-combined-logs/__transaction_state-15 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,608] INFO [Partition __transaction_state-15 broker=1] No checkpointed highwatermark is found for partition __transaction_state-15 (kafka.cluster.Partition)
[2023-08-24 07:23:32,608] INFO [Partition __transaction_state-15 broker=1] Log loaded for partition __transaction_state-15 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,609] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-15 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,609] INFO [MergedLog partition=__transaction_state-15, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-15 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,612] INFO [MergedLog partition=__transaction_state-48, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,612] INFO Created log for partition __transaction_state-48 in /etc/confluent/kraft-combined-logs/__transaction_state-48 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,612] INFO [Partition __transaction_state-48 broker=1] No checkpointed highwatermark is found for partition __transaction_state-48 (kafka.cluster.Partition)
[2023-08-24 07:23:32,612] INFO [Partition __transaction_state-48 broker=1] Log loaded for partition __transaction_state-48 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,613] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-48 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,613] INFO [MergedLog partition=__transaction_state-48, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-48 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,618] INFO [MergedLog partition=__transaction_state-3, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,619] INFO Created log for partition __transaction_state-3 in /etc/confluent/kraft-combined-logs/__transaction_state-3 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,620] INFO [Partition __transaction_state-3 broker=1] No checkpointed highwatermark is found for partition __transaction_state-3 (kafka.cluster.Partition)
[2023-08-24 07:23:32,620] INFO [Partition __transaction_state-3 broker=1] Log loaded for partition __transaction_state-3 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,620] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-3 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,620] INFO [MergedLog partition=__transaction_state-3, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,624] INFO [MergedLog partition=__transaction_state-36, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,625] INFO Created log for partition __transaction_state-36 in /etc/confluent/kraft-combined-logs/__transaction_state-36 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,625] INFO [Partition __transaction_state-36 broker=1] No checkpointed highwatermark is found for partition __transaction_state-36 (kafka.cluster.Partition)
[2023-08-24 07:23:32,625] INFO [Partition __transaction_state-36 broker=1] Log loaded for partition __transaction_state-36 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,625] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-36 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,626] INFO [MergedLog partition=__transaction_state-36, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-36 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,629] INFO [MergedLog partition=__transaction_state-7, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,629] INFO Created log for partition __transaction_state-7 in /etc/confluent/kraft-combined-logs/__transaction_state-7 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,630] INFO [Partition __transaction_state-7 broker=1] No checkpointed highwatermark is found for partition __transaction_state-7 (kafka.cluster.Partition)
[2023-08-24 07:23:32,631] INFO [Partition __transaction_state-7 broker=1] Log loaded for partition __transaction_state-7 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,631] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-7 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,631] INFO [MergedLog partition=__transaction_state-7, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,635] INFO [MergedLog partition=__transaction_state-40, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,636] INFO Created log for partition __transaction_state-40 in /etc/confluent/kraft-combined-logs/__transaction_state-40 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,636] INFO [Partition __transaction_state-40 broker=1] No checkpointed highwatermark is found for partition __transaction_state-40 (kafka.cluster.Partition)
[2023-08-24 07:23:32,636] INFO [Partition __transaction_state-40 broker=1] Log loaded for partition __transaction_state-40 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,637] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-40 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,637] INFO [MergedLog partition=__transaction_state-40, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-40 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,639] INFO [MergedLog partition=__transaction_state-28, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,640] INFO Created log for partition __transaction_state-28 in /etc/confluent/kraft-combined-logs/__transaction_state-28 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,640] INFO [Partition __transaction_state-28 broker=1] No checkpointed highwatermark is found for partition __transaction_state-28 (kafka.cluster.Partition)
[2023-08-24 07:23:32,640] INFO [Partition __transaction_state-28 broker=1] Log loaded for partition __transaction_state-28 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,640] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-28 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,640] INFO [MergedLog partition=__transaction_state-28, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-28 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,642] INFO [MergedLog partition=__transaction_state-32, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,643] INFO Created log for partition __transaction_state-32 in /etc/confluent/kraft-combined-logs/__transaction_state-32 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,644] INFO [Partition __transaction_state-32 broker=1] No checkpointed highwatermark is found for partition __transaction_state-32 (kafka.cluster.Partition)
[2023-08-24 07:23:32,644] INFO [Partition __transaction_state-32 broker=1] Log loaded for partition __transaction_state-32 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,644] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-32 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,644] INFO [MergedLog partition=__transaction_state-32, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-32 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,647] INFO [MergedLog partition=__transaction_state-20, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,648] INFO Created log for partition __transaction_state-20 in /etc/confluent/kraft-combined-logs/__transaction_state-20 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,648] INFO [Partition __transaction_state-20 broker=1] No checkpointed highwatermark is found for partition __transaction_state-20 (kafka.cluster.Partition)
[2023-08-24 07:23:32,648] INFO [Partition __transaction_state-20 broker=1] Log loaded for partition __transaction_state-20 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,648] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-20 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,649] INFO [MergedLog partition=__transaction_state-20, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-20 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,651] INFO [MergedLog partition=__transaction_state-24, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,652] INFO Created log for partition __transaction_state-24 in /etc/confluent/kraft-combined-logs/__transaction_state-24 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,652] INFO [Partition __transaction_state-24 broker=1] No checkpointed highwatermark is found for partition __transaction_state-24 (kafka.cluster.Partition)
[2023-08-24 07:23:32,652] INFO [Partition __transaction_state-24 broker=1] Log loaded for partition __transaction_state-24 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,652] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-24 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,653] INFO [MergedLog partition=__transaction_state-24, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-24 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,655] INFO [MergedLog partition=__transaction_state-10, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,656] INFO Created log for partition __transaction_state-10 in /etc/confluent/kraft-combined-logs/__transaction_state-10 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,656] INFO [Partition __transaction_state-10 broker=1] No checkpointed highwatermark is found for partition __transaction_state-10 (kafka.cluster.Partition)
[2023-08-24 07:23:32,656] INFO [Partition __transaction_state-10 broker=1] Log loaded for partition __transaction_state-10 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,656] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-10 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,657] INFO [MergedLog partition=__transaction_state-10, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,659] INFO [MergedLog partition=__transaction_state-43, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,660] INFO Created log for partition __transaction_state-43 in /etc/confluent/kraft-combined-logs/__transaction_state-43 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,661] INFO [Partition __transaction_state-43 broker=1] No checkpointed highwatermark is found for partition __transaction_state-43 (kafka.cluster.Partition)
[2023-08-24 07:23:32,661] INFO [Partition __transaction_state-43 broker=1] Log loaded for partition __transaction_state-43 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,661] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-43 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,662] INFO [MergedLog partition=__transaction_state-43, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-43 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,664] INFO [MergedLog partition=__transaction_state-14, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,664] INFO Created log for partition __transaction_state-14 in /etc/confluent/kraft-combined-logs/__transaction_state-14 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,664] INFO [Partition __transaction_state-14 broker=1] No checkpointed highwatermark is found for partition __transaction_state-14 (kafka.cluster.Partition)
[2023-08-24 07:23:32,665] INFO [Partition __transaction_state-14 broker=1] Log loaded for partition __transaction_state-14 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,665] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-14 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,665] INFO [MergedLog partition=__transaction_state-14, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-14 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,667] INFO [MergedLog partition=__transaction_state-47, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,669] INFO Created log for partition __transaction_state-47 in /etc/confluent/kraft-combined-logs/__transaction_state-47 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,669] INFO [Partition __transaction_state-47 broker=1] No checkpointed highwatermark is found for partition __transaction_state-47 (kafka.cluster.Partition)
[2023-08-24 07:23:32,669] INFO [Partition __transaction_state-47 broker=1] Log loaded for partition __transaction_state-47 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,669] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-47 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,670] INFO [MergedLog partition=__transaction_state-47, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-47 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,674] INFO [MergedLog partition=__transaction_state-2, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,675] INFO Created log for partition __transaction_state-2 in /etc/confluent/kraft-combined-logs/__transaction_state-2 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,675] INFO [Partition __transaction_state-2 broker=1] No checkpointed highwatermark is found for partition __transaction_state-2 (kafka.cluster.Partition)
[2023-08-24 07:23:32,676] INFO [Partition __transaction_state-2 broker=1] Log loaded for partition __transaction_state-2 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,676] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-2 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,676] INFO [MergedLog partition=__transaction_state-2, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,678] INFO [MergedLog partition=__transaction_state-35, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,679] INFO Created log for partition __transaction_state-35 in /etc/confluent/kraft-combined-logs/__transaction_state-35 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,679] INFO [Partition __transaction_state-35 broker=1] No checkpointed highwatermark is found for partition __transaction_state-35 (kafka.cluster.Partition)
[2023-08-24 07:23:32,680] INFO [Partition __transaction_state-35 broker=1] Log loaded for partition __transaction_state-35 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,680] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-35 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,680] INFO [MergedLog partition=__transaction_state-35, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-35 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,682] INFO [MergedLog partition=__transaction_state-6, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,683] INFO Created log for partition __transaction_state-6 in /etc/confluent/kraft-combined-logs/__transaction_state-6 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,683] INFO [Partition __transaction_state-6 broker=1] No checkpointed highwatermark is found for partition __transaction_state-6 (kafka.cluster.Partition)
[2023-08-24 07:23:32,684] INFO [Partition __transaction_state-6 broker=1] Log loaded for partition __transaction_state-6 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,684] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-6 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,684] INFO [MergedLog partition=__transaction_state-6, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,687] INFO [MergedLog partition=__transaction_state-39, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,687] INFO Created log for partition __transaction_state-39 in /etc/confluent/kraft-combined-logs/__transaction_state-39 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,687] INFO [Partition __transaction_state-39 broker=1] No checkpointed highwatermark is found for partition __transaction_state-39 (kafka.cluster.Partition)
[2023-08-24 07:23:32,688] INFO [Partition __transaction_state-39 broker=1] Log loaded for partition __transaction_state-39 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,688] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-39 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,688] INFO [MergedLog partition=__transaction_state-39, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-39 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,691] INFO [MergedLog partition=__transaction_state-27, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,692] INFO Created log for partition __transaction_state-27 in /etc/confluent/kraft-combined-logs/__transaction_state-27 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,692] INFO [Partition __transaction_state-27 broker=1] No checkpointed highwatermark is found for partition __transaction_state-27 (kafka.cluster.Partition)
[2023-08-24 07:23:32,693] INFO [Partition __transaction_state-27 broker=1] Log loaded for partition __transaction_state-27 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,694] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-27 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,694] INFO [MergedLog partition=__transaction_state-27, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-27 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,696] INFO [MergedLog partition=__transaction_state-31, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,697] INFO Created log for partition __transaction_state-31 in /etc/confluent/kraft-combined-logs/__transaction_state-31 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,697] INFO [Partition __transaction_state-31 broker=1] No checkpointed highwatermark is found for partition __transaction_state-31 (kafka.cluster.Partition)
[2023-08-24 07:23:32,698] INFO [Partition __transaction_state-31 broker=1] Log loaded for partition __transaction_state-31 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,698] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-31 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,698] INFO [MergedLog partition=__transaction_state-31, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-31 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,710] INFO [MergedLog partition=__transaction_state-19, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,710] INFO Created log for partition __transaction_state-19 in /etc/confluent/kraft-combined-logs/__transaction_state-19 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,711] INFO [Partition __transaction_state-19 broker=1] No checkpointed highwatermark is found for partition __transaction_state-19 (kafka.cluster.Partition)
[2023-08-24 07:23:32,711] INFO [Partition __transaction_state-19 broker=1] Log loaded for partition __transaction_state-19 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,711] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-19 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,711] INFO [MergedLog partition=__transaction_state-19, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-19 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,714] INFO [MergedLog partition=__transaction_state-23, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,714] INFO Created log for partition __transaction_state-23 in /etc/confluent/kraft-combined-logs/__transaction_state-23 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-24 07:23:32,715] INFO [Partition __transaction_state-23 broker=1] No checkpointed highwatermark is found for partition __transaction_state-23 (kafka.cluster.Partition)
[2023-08-24 07:23:32,715] INFO [Partition __transaction_state-23 broker=1] Log loaded for partition __transaction_state-23 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,715] INFO Setting topicIdPartition Q-5MPSJjSlWAoDwb3vgx8w:__transaction_state-23 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,715] INFO [MergedLog partition=__transaction_state-23, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-23 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,716] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 42 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,723] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 13 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,723] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 46 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,724] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 17 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,724] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 34 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,724] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 5 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,724] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 38 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,724] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 9 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,724] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 26 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,724] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 30 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,724] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 1 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,724] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 18 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,724] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-42 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,724] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 22 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,724] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 12 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,724] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 45 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,724] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 16 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,724] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 49 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,724] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 4 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,724] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 37 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,724] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 8 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,724] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 41 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,724] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 29 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,724] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 0 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,724] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 33 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,724] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 21 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,724] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 25 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,725] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 11 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,725] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 44 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,725] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 15 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,725] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 48 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,725] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 3 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,725] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 36 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,725] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 7 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,725] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 40 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,725] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 28 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,725] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-42 in 2 milliseconds, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,725] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 32 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,725] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 20 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,725] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 24 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,725] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 10 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,725] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 43 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,726] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 14 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,728] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-42 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,729] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-13 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,729] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-13 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,729] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-13 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,729] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-46 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,729] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-46 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,729] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-46 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,729] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-17 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,729] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-17 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,729] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-17 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,729] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-34 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,729] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-34 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,730] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-34 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,730] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-5 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,730] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-5 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,730] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-5 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,730] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-38 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,730] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-38 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,730] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-38 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,730] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-9 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,730] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-9 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,730] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-9 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,730] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-26 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,730] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-26 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,730] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-26 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,730] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-30 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,730] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-30 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,731] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-30 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,731] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-1 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,731] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-1 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,731] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-1 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,731] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-18 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,731] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-18 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,731] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-18 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,731] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-22 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,731] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-22 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,731] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-22 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,731] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-12 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,731] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-12 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,731] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-12 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,731] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-45 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,732] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-45 in 8 milliseconds, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,732] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-45 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,732] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-16 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,732] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-16 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,732] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-16 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,732] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-49 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,732] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-49 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,732] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-49 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,732] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-4 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,732] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-4 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,732] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-4 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,732] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-37 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,732] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-37 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,732] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-37 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,733] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-8 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,733] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-8 in 9 milliseconds, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,733] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-8 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,733] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-41 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,733] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-41 in 9 milliseconds, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,733] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-41 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,733] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-29 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,734] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-29 in 9 milliseconds, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,734] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-29 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,734] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-0 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,734] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-0 in 10 milliseconds, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,734] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-0 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,734] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-33 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,734] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-33 in 10 milliseconds, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,734] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-33 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,734] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-21 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,734] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-21 in 10 milliseconds, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,734] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-21 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,734] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-25 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,735] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-25 in 11 milliseconds, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,735] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-25 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,735] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-11 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,735] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-11 in 10 milliseconds, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,735] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-11 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,735] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-44 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,735] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-44 in 10 milliseconds, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,735] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-44 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,735] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-15 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,735] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-15 in 10 milliseconds, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,735] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-15 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,736] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-48 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,736] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-48 in 11 milliseconds, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,736] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-48 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,736] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-3 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,736] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-3 in 11 milliseconds, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,736] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-3 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,736] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-36 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,736] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-36 in 11 milliseconds, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,737] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-36 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,737] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-7 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,737] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-7 in 12 milliseconds, of which 12 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,737] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-7 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,737] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-40 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,737] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-40 in 12 milliseconds, of which 12 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,737] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-40 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,737] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-28 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,737] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-28 in 12 milliseconds, of which 12 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,737] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-28 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,737] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-32 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,737] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-32 in 12 milliseconds, of which 12 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,737] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-32 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,737] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-20 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,738] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-20 in 13 milliseconds, of which 12 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,738] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-20 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,738] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-24 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,738] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-24 in 13 milliseconds, of which 13 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,738] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-24 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,738] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 47 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,738] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-10 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,738] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-10 in 13 milliseconds, of which 13 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,738] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-10 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,738] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-43 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,738] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-43 in 12 milliseconds, of which 12 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,738] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-43 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,738] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-14 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,739] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-14 in 0 milliseconds, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,739] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-14 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,739] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 2 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,739] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 35 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,739] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 6 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,739] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 39 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,739] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-47 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,740] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 27 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,740] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 31 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,740] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 19 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,740] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 23 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,741] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic __transaction_state with new configuration : compression.type -> uncompressed,cleanup.policy -> compact,min.insync.replicas -> 1,segment.bytes -> 104857600,confluent.placement.constraints -> ,unclean.leader.election.enable -> false (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:32,745] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-47 in 6 milliseconds, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,745] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-47 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,745] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-2 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,746] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-2 in 7 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,746] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-2 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,746] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-35 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,746] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-35 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,746] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-35 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,746] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-6 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,746] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-6 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,746] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-6 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,746] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-39 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,746] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-39 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,746] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-39 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,746] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-27 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,746] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-27 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,746] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-27 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,746] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-31 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,746] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-31 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,746] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-31 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,746] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-19 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,746] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-19 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,746] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-19 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,746] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-23 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,746] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-23 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,746] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-23 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-24 07:23:32,749] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,754] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,755] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-24 07:23:32,756] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,757] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,757] INFO Setting topicIdPartition zdncAGeVR1CZFwP9vi0zuA:_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,758] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,758] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:32,760] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,765] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,765] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:32,766] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,766] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,766] INFO Setting topicIdPartition 2LJZLTUZSzq3vwqcXFWATQ:_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,766] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,767] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:32,768] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,772] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,772] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-24 07:23:32,773] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,773] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,775] INFO Setting topicIdPartition eH9WDLKGTfazT0GtwMJJow:_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,775] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,777] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:32,778] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,783] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,784] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:32,785] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,785] INFO [TransactionCoordinator id=1] Initialized transactionalId ksqldb-01 with producerId 3 and producer epoch 0 on partition __transaction_state-1 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-24 07:23:32,785] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,786] INFO Setting topicIdPartition D8vJ86g7TTKT6VOdvECAYQ:_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,786] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,787] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:32,788] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-cluster-rekey-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,834] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-cluster-rekey-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,834] INFO Created log for partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-cluster-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
[2023-08-24 07:23:32,838] INFO [Partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,838] INFO [Partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,838] INFO Setting topicIdPartition MEk0eG8hSKixxFAAerW9XA:_confluent-controlcenter-7-4-1-1-cluster-rekey-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,838] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-cluster-rekey-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-cluster-rekey-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,839] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-cluster-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:32,840] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,843] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,843] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:32,844] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,845] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,845] INFO Setting topicIdPartition y_-DkBoMS92-YnrfGh1T1g:_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,845] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,846] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:32,847] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,850] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,851] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:32,851] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,851] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,852] INFO Setting topicIdPartition RQpEX_ZcQA-3onfi3S4h-g:_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,853] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,853] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:32,854] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,857] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,857] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
[2023-08-24 07:23:32,858] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,859] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,859] INFO Setting topicIdPartition QvQn8zr3QMucazZZWNr8OQ:_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,860] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,860] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition with new configuration : cleanup.policy -> delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,retention.ms -> 604800000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:32,863] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,867] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,867] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:32,868] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,869] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,869] INFO Setting topicIdPartition YJK7MeXdSripNn4_QAazsg:_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,869] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,869] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:32,871] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,873] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,874] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:32,874] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,875] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,875] INFO Setting topicIdPartition tM1jBtQWQGSyq75pmMQ9uw:_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,875] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,876] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:32,877] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,880] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,881] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:32,881] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,882] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,883] INFO Setting topicIdPartition a5PvmE_tRhKaPNf0qEp6cA:_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,883] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,883] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:32,886] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,889] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,889] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:32,890] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,891] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,891] INFO Setting topicIdPartition _63z-vG_QhOMT_mHYbVSOg:_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,891] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,891] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:32,896] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,899] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,900] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
[2023-08-24 07:23:32,900] INFO [Partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,900] INFO [Partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,901] INFO Setting topicIdPartition o5T6OB4qStGPbLHi-lVm7Q:_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,901] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,902] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:32,903] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,906] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,907] INFO Created log for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
[2023-08-24 07:23:32,907] INFO [Partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,907] INFO [Partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,907] INFO Setting topicIdPartition PQ8dLWZpT42LAgNupd9JQA:_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,907] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,909] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:32,924] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,928] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,929] INFO Created log for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
[2023-08-24 07:23:32,929] INFO [Partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,930] INFO [Partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,930] INFO Setting topicIdPartition K_0WIVBrQcG8Y3ayDPfvXw:_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,930] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,931] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:32,955] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,959] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,960] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-24 07:23:32,960] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,961] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,961] INFO Setting topicIdPartition cRCSFdOvRa2tr5q3WgiC-w:_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:32,961] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:32,962] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:32,988] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:32,997] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:32,998] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:32,999] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 (kafka.cluster.Partition)
[2023-08-24 07:23:32,999] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,000] INFO Setting topicIdPartition 6ZKHOhEqRtyxEjg4w_RFvg:_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:33,000] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:33,001] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:33,023] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:33,029] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:33,030] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:33,031] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,031] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,031] INFO Setting topicIdPartition RaW0kU6kT_q_jIjUqcmKgg:_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:33,031] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:33,032] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:33,057] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:33,065] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:33,066] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:33,067] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,067] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,067] INFO Setting topicIdPartition cDT5-mcSQpym9L9tevf6jw:_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:33,067] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:33,068] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:33,089] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:33,094] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:33,095] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:33,096] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,096] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,096] INFO Setting topicIdPartition q0MtSx68RfiAeXJxldT5Vg:_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:33,098] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:33,099] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:33,121] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:33,125] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:33,126] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:33,130] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,130] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,130] INFO Setting topicIdPartition qs-02HWnRGKiOCxig8F9oQ:_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:33,130] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:33,131] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:33,152] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:33,157] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:33,161] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:33,163] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,163] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,163] INFO Setting topicIdPartition n3t3pakgQIWz5fZKKeY8LA:_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:33,163] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:33,164] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:33,186] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:33,189] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:33,190] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:33,190] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,190] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,190] INFO Setting topicIdPartition F8syTSP1RhKeJtjncuMBRw:_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:33,190] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:33,191] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:33,219] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:33,223] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:33,223] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:33,225] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,225] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,225] INFO Setting topicIdPartition _V6YXYwkRPSzFy6_omS-tg:_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:33,225] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:33,226] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:33,259] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:33,263] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:33,267] INFO Created log for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
[2023-08-24 07:23:33,268] INFO [Partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,268] INFO [Partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,268] INFO Setting topicIdPartition OwtBS5V0Rt6w_B-rNDGd-w:_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:33,268] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:33,269] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:33,298] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:33,305] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:33,305] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-24 07:23:33,306] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,306] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,306] INFO Setting topicIdPartition On0CeJbITai1Jja775iM7g:_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:33,306] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:33,306] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:33,344] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:33,349] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:33,350] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
[2023-08-24 07:23:33,350] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,351] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,351] INFO Setting topicIdPartition g5LE8Jw0Q062kiFpwxqMkg:_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:33,352] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:33,352] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:33,379] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:33,383] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:33,385] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:33,385] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,386] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,387] INFO Setting topicIdPartition foH66tUSTF2gmzsiDvUkqw:_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:33,387] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:33,388] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:33,416] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:33,427] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:33,428] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:33,428] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,428] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,428] INFO Setting topicIdPartition O6unTSQHSMq02ZpDnILO6g:_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:33,428] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:33,429] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:33,452] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:33,459] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:33,460] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-24 07:23:33,461] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,462] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,462] INFO Setting topicIdPartition aJ9qSRNUQgmimLs6-VUVdQ:_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:33,462] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:33,463] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:33,487] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:33,492] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:33,493] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-24 07:23:33,494] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,495] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,495] INFO Setting topicIdPartition 7JZh7Xl3R1aWnSpCoVd1Dg:_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:33,495] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:33,496] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:33,520] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:33,523] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:33,524] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:33,528] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,528] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,528] INFO Setting topicIdPartition R5WrU_TJQwyxLsZ239F-ZA:_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:33,528] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:33,529] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:33,565] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:33,570] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:33,570] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:33,571] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,572] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,572] INFO Setting topicIdPartition BQyQ91SyRn2Ph2NjEf5fPw:_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:33,572] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:33,573] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:33,595] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:33,599] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:33,599] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-24 07:23:33,600] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,600] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,600] INFO Setting topicIdPartition DYIuWDS2QOK9pb6hopuhww:_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:33,601] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:33,601] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:33,625] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-monitoring-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:33,628] INFO [MergedLog partition=_confluent-monitoring-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:33,629] INFO Created log for partition _confluent-monitoring-0 in /etc/confluent/kraft-combined-logs/_confluent-monitoring-0 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
[2023-08-24 07:23:33,629] INFO [Partition _confluent-monitoring-0 broker=1] No checkpointed highwatermark is found for partition _confluent-monitoring-0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,630] INFO [Partition _confluent-monitoring-0 broker=1] Log loaded for partition _confluent-monitoring-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,630] INFO Setting topicIdPartition JJsY7zmWRwWfrMSUpy1V4g:_confluent-monitoring-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:33,631] INFO [MergedLog partition=_confluent-monitoring-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-monitoring-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:33,631] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-monitoring with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 259200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:33,656] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:33,660] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:33,661] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:33,661] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,661] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,661] INFO Setting topicIdPartition kM4nDtonRt2xRRtWDGeh0Q:_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:33,661] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:33,662] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:33,687] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:33,691] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:33,692] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:33,692] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,693] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,693] INFO Setting topicIdPartition Z6qzFnVeSMmJ7aHvIJt2gQ:_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:33,694] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:33,694] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:33,718] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:33,721] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:33,722] INFO Created log for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-24 07:23:33,723] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,723] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,723] INFO Setting topicIdPartition BG_gpRZUQtGKjjDauoEuOQ:_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:33,723] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:33,724] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:33,749] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:33,753] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:33,753] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-24 07:23:33,754] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,754] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:33,755] INFO Setting topicIdPartition _8bl-sQUSm22ig_t58qJIg:_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:33,755] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:33,755] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:33,986] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1-command in Empty state. Created a new member id _confluent-controlcenter-7-4-1-1-command-6d46022a-d5ef-4ba0-a455-bc2da0b68cee-StreamThread-1-consumer-eb7895f2-3d42-4e50-897a-2380fbbabdad and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:33,990] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-7-4-1-1-command in state PreparingRebalance with old generation 0 (__consumer_offsets-23) (reason: Adding new member _confluent-controlcenter-7-4-1-1-command-6d46022a-d5ef-4ba0-a455-bc2da0b68cee-StreamThread-1-consumer-eb7895f2-3d42-4e50-897a-2380fbbabdad with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:33,996] INFO [GroupCoordinator 1]: Stabilized group _confluent-controlcenter-7-4-1-1-command generation 1 (__consumer_offsets-23) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:34,033] INFO [GroupCoordinator 1]: Assignment received from leader _confluent-controlcenter-7-4-1-1-command-6d46022a-d5ef-4ba0-a455-bc2da0b68cee-StreamThread-1-consumer-eb7895f2-3d42-4e50-897a-2380fbbabdad for group _confluent-controlcenter-7-4-1-1-command for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:36,278] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in Empty state. Created a new member id _confluent-controlcenter-7-4-1-1-05fed4ec-68f6-442f-ba57-23d3af8e5454-StreamThread-6-consumer-2d78e7fb-285d-46a2-b918-c2071fbb6dcb and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:36,283] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in Empty state. Created a new member id _confluent-controlcenter-7-4-1-1-05fed4ec-68f6-442f-ba57-23d3af8e5454-StreamThread-16-consumer-15367129-f9c1-4dbc-a2d9-852eb6767842 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:36,284] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in Empty state. Created a new member id _confluent-controlcenter-7-4-1-1-05fed4ec-68f6-442f-ba57-23d3af8e5454-StreamThread-7-consumer-41e0ec12-5ca2-4318-ac9e-008d83011992 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:36,286] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-7-4-1-1 in state PreparingRebalance with old generation 0 (__consumer_offsets-5) (reason: Adding new member _confluent-controlcenter-7-4-1-1-05fed4ec-68f6-442f-ba57-23d3af8e5454-StreamThread-16-consumer-15367129-f9c1-4dbc-a2d9-852eb6767842 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:36,286] INFO [GroupCoordinator 1]: Stabilized group _confluent-controlcenter-7-4-1-1 generation 1 (__consumer_offsets-5) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:36,288] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in CompletingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-05fed4ec-68f6-442f-ba57-23d3af8e5454-StreamThread-4-consumer-82c8d93b-a8a6-42b0-9a63-144ce5585724 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:36,309] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-7-4-1-1 in state PreparingRebalance with old generation 1 (__consumer_offsets-5) (reason: Adding new member _confluent-controlcenter-7-4-1-1-05fed4ec-68f6-442f-ba57-23d3af8e5454-StreamThread-7-consumer-41e0ec12-5ca2-4318-ac9e-008d83011992 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:36,321] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-05fed4ec-68f6-442f-ba57-23d3af8e5454-StreamThread-5-consumer-a610c179-e2cc-4f54-b92e-680ba84ec3bc and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:36,323] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-05fed4ec-68f6-442f-ba57-23d3af8e5454-StreamThread-15-consumer-5588d54a-5abb-43de-9aa9-decb102c09b9 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:36,324] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-05fed4ec-68f6-442f-ba57-23d3af8e5454-StreamThread-3-consumer-9e3add01-b1ff-4870-b85a-ef4bdf0c5e00 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:36,331] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-05fed4ec-68f6-442f-ba57-23d3af8e5454-StreamThread-1-consumer-70faddab-0a39-4a26-8677-3bee11c23d8f and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:36,334] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-05fed4ec-68f6-442f-ba57-23d3af8e5454-StreamThread-10-consumer-a31268b0-eded-4b63-bdf2-667e948cc3b6 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:36,344] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-05fed4ec-68f6-442f-ba57-23d3af8e5454-StreamThread-13-consumer-404d3d8d-e1fa-42c2-b8e4-7ca3aabca165 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:36,344] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-05fed4ec-68f6-442f-ba57-23d3af8e5454-StreamThread-14-consumer-cf874bf7-93d4-44e2-aa79-5eb0d999098e and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:36,346] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-05fed4ec-68f6-442f-ba57-23d3af8e5454-StreamThread-11-consumer-787562a3-475e-41d6-848a-9ec826b96cf5 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:36,349] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-05fed4ec-68f6-442f-ba57-23d3af8e5454-StreamThread-12-consumer-b343eace-af4c-49bc-9049-fb0a4754b943 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:36,350] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-05fed4ec-68f6-442f-ba57-23d3af8e5454-StreamThread-8-consumer-c7de124f-2667-48c2-92fb-da4d7ccaaef0 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:36,352] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-05fed4ec-68f6-442f-ba57-23d3af8e5454-StreamThread-2-consumer-b6f629e0-a105-482c-8ee0-cd4873006861 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:36,353] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-05fed4ec-68f6-442f-ba57-23d3af8e5454-StreamThread-9-consumer-04f1290f-2060-4447-995b-07fc89f24228 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:36,375] INFO [GroupCoordinator 1]: Stabilized group _confluent-controlcenter-7-4-1-1 generation 2 (__consumer_offsets-5) with 16 members (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:36,401] INFO [GroupCoordinator 1]: Assignment received from leader _confluent-controlcenter-7-4-1-1-05fed4ec-68f6-442f-ba57-23d3af8e5454-StreamThread-16-consumer-15367129-f9c1-4dbc-a2d9-852eb6767842 for group _confluent-controlcenter-7-4-1-1 for generation 2. The group has 16 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:44,559] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(telemetry-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:44,563] INFO [MergedLog partition=telemetry-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:44,564] INFO Created log for partition telemetry-0 in /etc/confluent/kraft-combined-logs/telemetry-0 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-24 07:23:44,565] INFO [Partition telemetry-0 broker=1] No checkpointed highwatermark is found for partition telemetry-0 (kafka.cluster.Partition)
[2023-08-24 07:23:44,565] INFO [Partition telemetry-0 broker=1] Log loaded for partition telemetry-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:44,565] INFO Setting topicIdPartition Mc_7IgQpT2yZQcNgdFyP-w:telemetry-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:44,565] INFO [MergedLog partition=telemetry-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for telemetry-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:44,566] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic telemetry with new configuration : max.message.bytes -> 10485760,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,retention.ms -> 259200000,segment.ms -> 14400000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:44,629] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-telemetry-metrics-3, _confluent-telemetry-metrics-4, _confluent-telemetry-metrics-5, _confluent-telemetry-metrics-6, _confluent-telemetry-metrics-7, _confluent-telemetry-metrics-8, _confluent-telemetry-metrics-9, _confluent-telemetry-metrics-10, _confluent-telemetry-metrics-11, _confluent-telemetry-metrics-0, _confluent-telemetry-metrics-1, _confluent-telemetry-metrics-2) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:44,641] INFO [MergedLog partition=_confluent-telemetry-metrics-3, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:44,642] INFO Created log for partition _confluent-telemetry-metrics-3 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-3 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-24 07:23:44,643] INFO [Partition _confluent-telemetry-metrics-3 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-3 (kafka.cluster.Partition)
[2023-08-24 07:23:44,643] INFO [Partition _confluent-telemetry-metrics-3 broker=1] Log loaded for partition _confluent-telemetry-metrics-3 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:44,643] INFO Setting topicIdPartition JJ8OudbITsuLRR5d_fRPdg:_confluent-telemetry-metrics-3 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:44,643] INFO [MergedLog partition=_confluent-telemetry-metrics-3, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:44,646] INFO [MergedLog partition=_confluent-telemetry-metrics-4, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:44,646] INFO Created log for partition _confluent-telemetry-metrics-4 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-4 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-24 07:23:44,646] INFO [Partition _confluent-telemetry-metrics-4 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-4 (kafka.cluster.Partition)
[2023-08-24 07:23:44,646] INFO [Partition _confluent-telemetry-metrics-4 broker=1] Log loaded for partition _confluent-telemetry-metrics-4 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:44,646] INFO Setting topicIdPartition JJ8OudbITsuLRR5d_fRPdg:_confluent-telemetry-metrics-4 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:44,647] INFO [MergedLog partition=_confluent-telemetry-metrics-4, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:44,649] INFO [MergedLog partition=_confluent-telemetry-metrics-5, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:44,650] INFO Created log for partition _confluent-telemetry-metrics-5 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-5 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-24 07:23:44,650] INFO [Partition _confluent-telemetry-metrics-5 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-5 (kafka.cluster.Partition)
[2023-08-24 07:23:44,650] INFO [Partition _confluent-telemetry-metrics-5 broker=1] Log loaded for partition _confluent-telemetry-metrics-5 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:44,650] INFO Setting topicIdPartition JJ8OudbITsuLRR5d_fRPdg:_confluent-telemetry-metrics-5 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:44,651] INFO [MergedLog partition=_confluent-telemetry-metrics-5, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:44,653] INFO [MergedLog partition=_confluent-telemetry-metrics-6, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:44,654] INFO Created log for partition _confluent-telemetry-metrics-6 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-6 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-24 07:23:44,654] INFO [Partition _confluent-telemetry-metrics-6 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-6 (kafka.cluster.Partition)
[2023-08-24 07:23:44,654] INFO [Partition _confluent-telemetry-metrics-6 broker=1] Log loaded for partition _confluent-telemetry-metrics-6 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:44,655] INFO Setting topicIdPartition JJ8OudbITsuLRR5d_fRPdg:_confluent-telemetry-metrics-6 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:44,655] INFO [MergedLog partition=_confluent-telemetry-metrics-6, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:44,658] INFO [MergedLog partition=_confluent-telemetry-metrics-7, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:44,658] INFO Created log for partition _confluent-telemetry-metrics-7 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-7 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-24 07:23:44,658] INFO [Partition _confluent-telemetry-metrics-7 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-7 (kafka.cluster.Partition)
[2023-08-24 07:23:44,658] INFO [Partition _confluent-telemetry-metrics-7 broker=1] Log loaded for partition _confluent-telemetry-metrics-7 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:44,659] INFO Setting topicIdPartition JJ8OudbITsuLRR5d_fRPdg:_confluent-telemetry-metrics-7 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:44,659] INFO [MergedLog partition=_confluent-telemetry-metrics-7, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:44,661] INFO [MergedLog partition=_confluent-telemetry-metrics-8, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:44,662] INFO Created log for partition _confluent-telemetry-metrics-8 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-8 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-24 07:23:44,662] INFO [Partition _confluent-telemetry-metrics-8 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-8 (kafka.cluster.Partition)
[2023-08-24 07:23:44,662] INFO [Partition _confluent-telemetry-metrics-8 broker=1] Log loaded for partition _confluent-telemetry-metrics-8 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:44,662] INFO Setting topicIdPartition JJ8OudbITsuLRR5d_fRPdg:_confluent-telemetry-metrics-8 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:44,663] INFO [MergedLog partition=_confluent-telemetry-metrics-8, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:44,665] INFO [MergedLog partition=_confluent-telemetry-metrics-9, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:44,666] INFO Created log for partition _confluent-telemetry-metrics-9 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-9 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-24 07:23:44,667] INFO [Partition _confluent-telemetry-metrics-9 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-9 (kafka.cluster.Partition)
[2023-08-24 07:23:44,668] INFO [Partition _confluent-telemetry-metrics-9 broker=1] Log loaded for partition _confluent-telemetry-metrics-9 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:44,668] INFO Setting topicIdPartition JJ8OudbITsuLRR5d_fRPdg:_confluent-telemetry-metrics-9 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:44,668] INFO [MergedLog partition=_confluent-telemetry-metrics-9, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:44,670] INFO [MergedLog partition=_confluent-telemetry-metrics-10, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:44,671] INFO Created log for partition _confluent-telemetry-metrics-10 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-10 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-24 07:23:44,672] INFO [Partition _confluent-telemetry-metrics-10 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-10 (kafka.cluster.Partition)
[2023-08-24 07:23:44,672] INFO [Partition _confluent-telemetry-metrics-10 broker=1] Log loaded for partition _confluent-telemetry-metrics-10 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:44,673] INFO Setting topicIdPartition JJ8OudbITsuLRR5d_fRPdg:_confluent-telemetry-metrics-10 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:44,673] INFO [MergedLog partition=_confluent-telemetry-metrics-10, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:44,675] INFO [MergedLog partition=_confluent-telemetry-metrics-11, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:44,676] INFO Created log for partition _confluent-telemetry-metrics-11 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-11 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-24 07:23:44,676] INFO [Partition _confluent-telemetry-metrics-11 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-11 (kafka.cluster.Partition)
[2023-08-24 07:23:44,676] INFO [Partition _confluent-telemetry-metrics-11 broker=1] Log loaded for partition _confluent-telemetry-metrics-11 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:44,676] INFO Setting topicIdPartition JJ8OudbITsuLRR5d_fRPdg:_confluent-telemetry-metrics-11 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:44,677] INFO [MergedLog partition=_confluent-telemetry-metrics-11, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:44,679] INFO [MergedLog partition=_confluent-telemetry-metrics-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:44,680] INFO Created log for partition _confluent-telemetry-metrics-0 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-0 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-24 07:23:44,680] INFO [Partition _confluent-telemetry-metrics-0 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-0 (kafka.cluster.Partition)
[2023-08-24 07:23:44,680] INFO [Partition _confluent-telemetry-metrics-0 broker=1] Log loaded for partition _confluent-telemetry-metrics-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:44,680] INFO Setting topicIdPartition JJ8OudbITsuLRR5d_fRPdg:_confluent-telemetry-metrics-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:44,680] INFO [MergedLog partition=_confluent-telemetry-metrics-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:44,684] INFO [MergedLog partition=_confluent-telemetry-metrics-1, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:44,684] INFO Created log for partition _confluent-telemetry-metrics-1 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-1 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-24 07:23:44,685] INFO [Partition _confluent-telemetry-metrics-1 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-1 (kafka.cluster.Partition)
[2023-08-24 07:23:44,685] INFO [Partition _confluent-telemetry-metrics-1 broker=1] Log loaded for partition _confluent-telemetry-metrics-1 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:44,686] INFO Setting topicIdPartition JJ8OudbITsuLRR5d_fRPdg:_confluent-telemetry-metrics-1 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:44,686] INFO [MergedLog partition=_confluent-telemetry-metrics-1, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:44,688] INFO [MergedLog partition=_confluent-telemetry-metrics-2, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:44,689] INFO Created log for partition _confluent-telemetry-metrics-2 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-2 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-24 07:23:44,689] INFO [Partition _confluent-telemetry-metrics-2 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-2 (kafka.cluster.Partition)
[2023-08-24 07:23:44,690] INFO [Partition _confluent-telemetry-metrics-2 broker=1] Log loaded for partition _confluent-telemetry-metrics-2 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:44,690] INFO Setting topicIdPartition JJ8OudbITsuLRR5d_fRPdg:_confluent-telemetry-metrics-2 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:44,690] INFO [MergedLog partition=_confluent-telemetry-metrics-2, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:44,691] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-telemetry-metrics with new configuration : max.message.bytes -> 10485760,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,retention.ms -> 259200000,segment.ms -> 14400000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:50,287] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent_balancer_api_state-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:23:50,290] INFO [MergedLog partition=_confluent_balancer_api_state-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:23:50,290] INFO Created log for partition _confluent_balancer_api_state-0 in /etc/confluent/kraft-combined-logs/_confluent_balancer_api_state-0 with properties {cleanup.policy=compact, retention.ms=-1} (kafka.log.LogManager)
[2023-08-24 07:23:50,291] INFO [Partition _confluent_balancer_api_state-0 broker=1] No checkpointed highwatermark is found for partition _confluent_balancer_api_state-0 (kafka.cluster.Partition)
[2023-08-24 07:23:50,291] INFO [Partition _confluent_balancer_api_state-0 broker=1] Log loaded for partition _confluent_balancer_api_state-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:23:50,291] INFO Setting topicIdPartition QPHS9GIFR7ScJkLF2JfmJg:_confluent_balancer_api_state-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:23:50,292] INFO [MergedLog partition=_confluent_balancer_api_state-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent_balancer_api_state-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:23:50,292] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent_balancer_api_state with new configuration : cleanup.policy -> compact,retention.ms -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:23:51,696] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group ConfluentTelemetryReporterSampler-3440767994714366978 in Empty state. Created a new member id kafka-cruise-control-00fad80b-050b-407c-83cb-0ca2e5e0c373 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:51,699] INFO [GroupCoordinator 1]: Preparing to rebalance group ConfluentTelemetryReporterSampler-3440767994714366978 in state PreparingRebalance with old generation 0 (__consumer_offsets-21) (reason: Adding new member kafka-cruise-control-00fad80b-050b-407c-83cb-0ca2e5e0c373 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:51,701] INFO [GroupCoordinator 1]: Stabilized group ConfluentTelemetryReporterSampler-3440767994714366978 generation 1 (__consumer_offsets-21) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:23:51,712] INFO [GroupCoordinator 1]: Assignment received from leader kafka-cruise-control-00fad80b-050b-407c-83cb-0ca2e5e0c373 for group ConfluentTelemetryReporterSampler-3440767994714366978 for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:24:25,103] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(connect-offsets-10, connect-offsets-8, connect-offsets-14, connect-offsets-12, connect-offsets-2, connect-offsets-0, connect-offsets-6, connect-offsets-4, connect-offsets-24, connect-offsets-18, connect-offsets-16, connect-offsets-22, connect-offsets-20, connect-offsets-9, connect-offsets-7, connect-offsets-13, connect-offsets-11, connect-offsets-1, connect-offsets-5, connect-offsets-3, connect-offsets-23, connect-offsets-17, connect-offsets-15, connect-offsets-21, connect-offsets-19) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:24:25,121] INFO [MergedLog partition=connect-offsets-10, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,122] INFO Created log for partition connect-offsets-10 in /etc/confluent/kraft-combined-logs/connect-offsets-10 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,122] INFO [Partition connect-offsets-10 broker=1] No checkpointed highwatermark is found for partition connect-offsets-10 (kafka.cluster.Partition)
[2023-08-24 07:24:25,123] INFO [Partition connect-offsets-10 broker=1] Log loaded for partition connect-offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,123] INFO Setting topicIdPartition PRKNJquVSUSRiDTunZXoCw:connect-offsets-10 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,123] INFO [MergedLog partition=connect-offsets-10, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,125] INFO [MergedLog partition=connect-offsets-8, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,125] INFO Created log for partition connect-offsets-8 in /etc/confluent/kraft-combined-logs/connect-offsets-8 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,125] INFO [Partition connect-offsets-8 broker=1] No checkpointed highwatermark is found for partition connect-offsets-8 (kafka.cluster.Partition)
[2023-08-24 07:24:25,125] INFO [Partition connect-offsets-8 broker=1] Log loaded for partition connect-offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,125] INFO Setting topicIdPartition PRKNJquVSUSRiDTunZXoCw:connect-offsets-8 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,126] INFO [MergedLog partition=connect-offsets-8, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,128] INFO [MergedLog partition=connect-offsets-14, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,128] INFO Created log for partition connect-offsets-14 in /etc/confluent/kraft-combined-logs/connect-offsets-14 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,128] INFO [Partition connect-offsets-14 broker=1] No checkpointed highwatermark is found for partition connect-offsets-14 (kafka.cluster.Partition)
[2023-08-24 07:24:25,128] INFO [Partition connect-offsets-14 broker=1] Log loaded for partition connect-offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,128] INFO Setting topicIdPartition PRKNJquVSUSRiDTunZXoCw:connect-offsets-14 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,129] INFO [MergedLog partition=connect-offsets-14, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-14 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,131] INFO [MergedLog partition=connect-offsets-12, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,131] INFO Created log for partition connect-offsets-12 in /etc/confluent/kraft-combined-logs/connect-offsets-12 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,131] INFO [Partition connect-offsets-12 broker=1] No checkpointed highwatermark is found for partition connect-offsets-12 (kafka.cluster.Partition)
[2023-08-24 07:24:25,131] INFO [Partition connect-offsets-12 broker=1] Log loaded for partition connect-offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,131] INFO Setting topicIdPartition PRKNJquVSUSRiDTunZXoCw:connect-offsets-12 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,131] INFO [MergedLog partition=connect-offsets-12, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-12 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,133] INFO [MergedLog partition=connect-offsets-2, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,134] INFO Created log for partition connect-offsets-2 in /etc/confluent/kraft-combined-logs/connect-offsets-2 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,134] INFO [Partition connect-offsets-2 broker=1] No checkpointed highwatermark is found for partition connect-offsets-2 (kafka.cluster.Partition)
[2023-08-24 07:24:25,134] INFO [Partition connect-offsets-2 broker=1] Log loaded for partition connect-offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,134] INFO Setting topicIdPartition PRKNJquVSUSRiDTunZXoCw:connect-offsets-2 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,134] INFO [MergedLog partition=connect-offsets-2, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,137] INFO [MergedLog partition=connect-offsets-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,137] INFO Created log for partition connect-offsets-0 in /etc/confluent/kraft-combined-logs/connect-offsets-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,137] INFO [Partition connect-offsets-0 broker=1] No checkpointed highwatermark is found for partition connect-offsets-0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,137] INFO [Partition connect-offsets-0 broker=1] Log loaded for partition connect-offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,137] INFO Setting topicIdPartition PRKNJquVSUSRiDTunZXoCw:connect-offsets-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,138] INFO [MergedLog partition=connect-offsets-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,140] INFO [MergedLog partition=connect-offsets-6, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,140] INFO Created log for partition connect-offsets-6 in /etc/confluent/kraft-combined-logs/connect-offsets-6 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,140] INFO [Partition connect-offsets-6 broker=1] No checkpointed highwatermark is found for partition connect-offsets-6 (kafka.cluster.Partition)
[2023-08-24 07:24:25,140] INFO [Partition connect-offsets-6 broker=1] Log loaded for partition connect-offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,140] INFO Setting topicIdPartition PRKNJquVSUSRiDTunZXoCw:connect-offsets-6 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,140] INFO [MergedLog partition=connect-offsets-6, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,143] INFO [MergedLog partition=connect-offsets-4, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,143] INFO Created log for partition connect-offsets-4 in /etc/confluent/kraft-combined-logs/connect-offsets-4 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,144] INFO [Partition connect-offsets-4 broker=1] No checkpointed highwatermark is found for partition connect-offsets-4 (kafka.cluster.Partition)
[2023-08-24 07:24:25,144] INFO [Partition connect-offsets-4 broker=1] Log loaded for partition connect-offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,145] INFO Setting topicIdPartition PRKNJquVSUSRiDTunZXoCw:connect-offsets-4 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,145] INFO [MergedLog partition=connect-offsets-4, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,147] INFO [MergedLog partition=connect-offsets-24, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,147] INFO Created log for partition connect-offsets-24 in /etc/confluent/kraft-combined-logs/connect-offsets-24 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,148] INFO [Partition connect-offsets-24 broker=1] No checkpointed highwatermark is found for partition connect-offsets-24 (kafka.cluster.Partition)
[2023-08-24 07:24:25,148] INFO [Partition connect-offsets-24 broker=1] Log loaded for partition connect-offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,149] INFO Setting topicIdPartition PRKNJquVSUSRiDTunZXoCw:connect-offsets-24 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,149] INFO [MergedLog partition=connect-offsets-24, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-24 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,151] INFO [MergedLog partition=connect-offsets-18, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,151] INFO Created log for partition connect-offsets-18 in /etc/confluent/kraft-combined-logs/connect-offsets-18 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,151] INFO [Partition connect-offsets-18 broker=1] No checkpointed highwatermark is found for partition connect-offsets-18 (kafka.cluster.Partition)
[2023-08-24 07:24:25,152] INFO [Partition connect-offsets-18 broker=1] Log loaded for partition connect-offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,152] INFO Setting topicIdPartition PRKNJquVSUSRiDTunZXoCw:connect-offsets-18 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,152] INFO [MergedLog partition=connect-offsets-18, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-18 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,154] INFO [MergedLog partition=connect-offsets-16, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,155] INFO Created log for partition connect-offsets-16 in /etc/confluent/kraft-combined-logs/connect-offsets-16 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,155] INFO [Partition connect-offsets-16 broker=1] No checkpointed highwatermark is found for partition connect-offsets-16 (kafka.cluster.Partition)
[2023-08-24 07:24:25,155] INFO [Partition connect-offsets-16 broker=1] Log loaded for partition connect-offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,155] INFO Setting topicIdPartition PRKNJquVSUSRiDTunZXoCw:connect-offsets-16 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,156] INFO [MergedLog partition=connect-offsets-16, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-16 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,158] INFO [MergedLog partition=connect-offsets-22, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,158] INFO Created log for partition connect-offsets-22 in /etc/confluent/kraft-combined-logs/connect-offsets-22 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,158] INFO [Partition connect-offsets-22 broker=1] No checkpointed highwatermark is found for partition connect-offsets-22 (kafka.cluster.Partition)
[2023-08-24 07:24:25,158] INFO [Partition connect-offsets-22 broker=1] Log loaded for partition connect-offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,158] INFO Setting topicIdPartition PRKNJquVSUSRiDTunZXoCw:connect-offsets-22 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,158] INFO [MergedLog partition=connect-offsets-22, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-22 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,160] INFO [MergedLog partition=connect-offsets-20, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,161] INFO Created log for partition connect-offsets-20 in /etc/confluent/kraft-combined-logs/connect-offsets-20 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,161] INFO [Partition connect-offsets-20 broker=1] No checkpointed highwatermark is found for partition connect-offsets-20 (kafka.cluster.Partition)
[2023-08-24 07:24:25,161] INFO [Partition connect-offsets-20 broker=1] Log loaded for partition connect-offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,161] INFO Setting topicIdPartition PRKNJquVSUSRiDTunZXoCw:connect-offsets-20 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,161] INFO [MergedLog partition=connect-offsets-20, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-20 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,163] INFO [MergedLog partition=connect-offsets-9, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,164] INFO Created log for partition connect-offsets-9 in /etc/confluent/kraft-combined-logs/connect-offsets-9 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,164] INFO [Partition connect-offsets-9 broker=1] No checkpointed highwatermark is found for partition connect-offsets-9 (kafka.cluster.Partition)
[2023-08-24 07:24:25,164] INFO [Partition connect-offsets-9 broker=1] Log loaded for partition connect-offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,164] INFO Setting topicIdPartition PRKNJquVSUSRiDTunZXoCw:connect-offsets-9 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,164] INFO [MergedLog partition=connect-offsets-9, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,166] INFO [MergedLog partition=connect-offsets-7, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,167] INFO Created log for partition connect-offsets-7 in /etc/confluent/kraft-combined-logs/connect-offsets-7 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,167] INFO [Partition connect-offsets-7 broker=1] No checkpointed highwatermark is found for partition connect-offsets-7 (kafka.cluster.Partition)
[2023-08-24 07:24:25,167] INFO [Partition connect-offsets-7 broker=1] Log loaded for partition connect-offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,167] INFO Setting topicIdPartition PRKNJquVSUSRiDTunZXoCw:connect-offsets-7 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,167] INFO [MergedLog partition=connect-offsets-7, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,170] INFO [MergedLog partition=connect-offsets-13, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,170] INFO Created log for partition connect-offsets-13 in /etc/confluent/kraft-combined-logs/connect-offsets-13 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,170] INFO [Partition connect-offsets-13 broker=1] No checkpointed highwatermark is found for partition connect-offsets-13 (kafka.cluster.Partition)
[2023-08-24 07:24:25,170] INFO [Partition connect-offsets-13 broker=1] Log loaded for partition connect-offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,171] INFO Setting topicIdPartition PRKNJquVSUSRiDTunZXoCw:connect-offsets-13 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,171] INFO [MergedLog partition=connect-offsets-13, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-13 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,173] INFO [MergedLog partition=connect-offsets-11, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,173] INFO Created log for partition connect-offsets-11 in /etc/confluent/kraft-combined-logs/connect-offsets-11 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,174] INFO [Partition connect-offsets-11 broker=1] No checkpointed highwatermark is found for partition connect-offsets-11 (kafka.cluster.Partition)
[2023-08-24 07:24:25,174] INFO [Partition connect-offsets-11 broker=1] Log loaded for partition connect-offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,174] INFO Setting topicIdPartition PRKNJquVSUSRiDTunZXoCw:connect-offsets-11 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,175] INFO [MergedLog partition=connect-offsets-11, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,178] INFO [MergedLog partition=connect-offsets-1, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,178] INFO Created log for partition connect-offsets-1 in /etc/confluent/kraft-combined-logs/connect-offsets-1 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,179] INFO [Partition connect-offsets-1 broker=1] No checkpointed highwatermark is found for partition connect-offsets-1 (kafka.cluster.Partition)
[2023-08-24 07:24:25,179] INFO [Partition connect-offsets-1 broker=1] Log loaded for partition connect-offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,179] INFO Setting topicIdPartition PRKNJquVSUSRiDTunZXoCw:connect-offsets-1 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,179] INFO [MergedLog partition=connect-offsets-1, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,181] INFO [MergedLog partition=connect-offsets-5, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,181] INFO Created log for partition connect-offsets-5 in /etc/confluent/kraft-combined-logs/connect-offsets-5 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,181] INFO [Partition connect-offsets-5 broker=1] No checkpointed highwatermark is found for partition connect-offsets-5 (kafka.cluster.Partition)
[2023-08-24 07:24:25,182] INFO [Partition connect-offsets-5 broker=1] Log loaded for partition connect-offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,182] INFO Setting topicIdPartition PRKNJquVSUSRiDTunZXoCw:connect-offsets-5 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,182] INFO [MergedLog partition=connect-offsets-5, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,184] INFO [MergedLog partition=connect-offsets-3, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,184] INFO Created log for partition connect-offsets-3 in /etc/confluent/kraft-combined-logs/connect-offsets-3 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,185] INFO [Partition connect-offsets-3 broker=1] No checkpointed highwatermark is found for partition connect-offsets-3 (kafka.cluster.Partition)
[2023-08-24 07:24:25,185] INFO [Partition connect-offsets-3 broker=1] Log loaded for partition connect-offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,185] INFO Setting topicIdPartition PRKNJquVSUSRiDTunZXoCw:connect-offsets-3 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,185] INFO [MergedLog partition=connect-offsets-3, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,187] INFO [MergedLog partition=connect-offsets-23, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,187] INFO Created log for partition connect-offsets-23 in /etc/confluent/kraft-combined-logs/connect-offsets-23 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,187] INFO [Partition connect-offsets-23 broker=1] No checkpointed highwatermark is found for partition connect-offsets-23 (kafka.cluster.Partition)
[2023-08-24 07:24:25,188] INFO [Partition connect-offsets-23 broker=1] Log loaded for partition connect-offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,188] INFO Setting topicIdPartition PRKNJquVSUSRiDTunZXoCw:connect-offsets-23 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,188] INFO [MergedLog partition=connect-offsets-23, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-23 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,190] INFO [MergedLog partition=connect-offsets-17, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,190] INFO Created log for partition connect-offsets-17 in /etc/confluent/kraft-combined-logs/connect-offsets-17 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,190] INFO [Partition connect-offsets-17 broker=1] No checkpointed highwatermark is found for partition connect-offsets-17 (kafka.cluster.Partition)
[2023-08-24 07:24:25,190] INFO [Partition connect-offsets-17 broker=1] Log loaded for partition connect-offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,190] INFO Setting topicIdPartition PRKNJquVSUSRiDTunZXoCw:connect-offsets-17 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,190] INFO [MergedLog partition=connect-offsets-17, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-17 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,193] INFO [MergedLog partition=connect-offsets-15, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,193] INFO Created log for partition connect-offsets-15 in /etc/confluent/kraft-combined-logs/connect-offsets-15 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,193] INFO [Partition connect-offsets-15 broker=1] No checkpointed highwatermark is found for partition connect-offsets-15 (kafka.cluster.Partition)
[2023-08-24 07:24:25,193] INFO [Partition connect-offsets-15 broker=1] Log loaded for partition connect-offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,193] INFO Setting topicIdPartition PRKNJquVSUSRiDTunZXoCw:connect-offsets-15 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,193] INFO [MergedLog partition=connect-offsets-15, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-15 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,196] INFO [MergedLog partition=connect-offsets-21, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,196] INFO Created log for partition connect-offsets-21 in /etc/confluent/kraft-combined-logs/connect-offsets-21 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,196] INFO [Partition connect-offsets-21 broker=1] No checkpointed highwatermark is found for partition connect-offsets-21 (kafka.cluster.Partition)
[2023-08-24 07:24:25,197] INFO [Partition connect-offsets-21 broker=1] Log loaded for partition connect-offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,197] INFO Setting topicIdPartition PRKNJquVSUSRiDTunZXoCw:connect-offsets-21 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,197] INFO [MergedLog partition=connect-offsets-21, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-21 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,199] INFO [MergedLog partition=connect-offsets-19, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,200] INFO Created log for partition connect-offsets-19 in /etc/confluent/kraft-combined-logs/connect-offsets-19 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,200] INFO [Partition connect-offsets-19 broker=1] No checkpointed highwatermark is found for partition connect-offsets-19 (kafka.cluster.Partition)
[2023-08-24 07:24:25,200] INFO [Partition connect-offsets-19 broker=1] Log loaded for partition connect-offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,200] INFO Setting topicIdPartition PRKNJquVSUSRiDTunZXoCw:connect-offsets-19 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,200] INFO [MergedLog partition=connect-offsets-19, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-19 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,200] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic connect-offsets with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:24:25,294] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(connect-statuses-0, connect-statuses-3, connect-statuses-4, connect-statuses-1, connect-statuses-2) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:24:25,301] INFO [MergedLog partition=connect-statuses-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,301] INFO Created log for partition connect-statuses-0 in /etc/confluent/kraft-combined-logs/connect-statuses-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,302] INFO [Partition connect-statuses-0 broker=1] No checkpointed highwatermark is found for partition connect-statuses-0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,303] INFO [Partition connect-statuses-0 broker=1] Log loaded for partition connect-statuses-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,303] INFO Setting topicIdPartition cNLKjpesSAyHJB-P0uE3nQ:connect-statuses-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,303] INFO [MergedLog partition=connect-statuses-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-statuses-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,305] INFO [MergedLog partition=connect-statuses-3, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,307] INFO Created log for partition connect-statuses-3 in /etc/confluent/kraft-combined-logs/connect-statuses-3 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,307] INFO [Partition connect-statuses-3 broker=1] No checkpointed highwatermark is found for partition connect-statuses-3 (kafka.cluster.Partition)
[2023-08-24 07:24:25,308] INFO [Partition connect-statuses-3 broker=1] Log loaded for partition connect-statuses-3 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,308] INFO Setting topicIdPartition cNLKjpesSAyHJB-P0uE3nQ:connect-statuses-3 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,308] INFO [MergedLog partition=connect-statuses-3, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-statuses-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,310] INFO [MergedLog partition=connect-statuses-4, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,311] INFO Created log for partition connect-statuses-4 in /etc/confluent/kraft-combined-logs/connect-statuses-4 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,312] INFO [Partition connect-statuses-4 broker=1] No checkpointed highwatermark is found for partition connect-statuses-4 (kafka.cluster.Partition)
[2023-08-24 07:24:25,312] INFO [Partition connect-statuses-4 broker=1] Log loaded for partition connect-statuses-4 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,312] INFO Setting topicIdPartition cNLKjpesSAyHJB-P0uE3nQ:connect-statuses-4 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,312] INFO [MergedLog partition=connect-statuses-4, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-statuses-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,314] INFO [MergedLog partition=connect-statuses-1, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,314] INFO Created log for partition connect-statuses-1 in /etc/confluent/kraft-combined-logs/connect-statuses-1 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,315] INFO [Partition connect-statuses-1 broker=1] No checkpointed highwatermark is found for partition connect-statuses-1 (kafka.cluster.Partition)
[2023-08-24 07:24:25,315] INFO [Partition connect-statuses-1 broker=1] Log loaded for partition connect-statuses-1 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,315] INFO Setting topicIdPartition cNLKjpesSAyHJB-P0uE3nQ:connect-statuses-1 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,315] INFO [MergedLog partition=connect-statuses-1, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-statuses-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,318] INFO [MergedLog partition=connect-statuses-2, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,318] INFO Created log for partition connect-statuses-2 in /etc/confluent/kraft-combined-logs/connect-statuses-2 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,318] INFO [Partition connect-statuses-2 broker=1] No checkpointed highwatermark is found for partition connect-statuses-2 (kafka.cluster.Partition)
[2023-08-24 07:24:25,319] INFO [Partition connect-statuses-2 broker=1] Log loaded for partition connect-statuses-2 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,319] INFO Setting topicIdPartition cNLKjpesSAyHJB-P0uE3nQ:connect-statuses-2 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,319] INFO [MergedLog partition=connect-statuses-2, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-statuses-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,319] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic connect-statuses with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:24:25,365] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(connect-configs-0) (kafka.server.ReplicaFetcherManager)
[2023-08-24 07:24:25,371] INFO [MergedLog partition=connect-configs-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-24 07:24:25,371] INFO Created log for partition connect-configs-0 in /etc/confluent/kraft-combined-logs/connect-configs-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-24 07:24:25,373] INFO [Partition connect-configs-0 broker=1] No checkpointed highwatermark is found for partition connect-configs-0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,373] INFO [Partition connect-configs-0 broker=1] Log loaded for partition connect-configs-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-24 07:24:25,373] INFO Setting topicIdPartition nbVhO8lgReGwoGB6riEFXw:connect-configs-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-24 07:24:25,374] INFO [MergedLog partition=connect-configs-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-configs-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-24 07:24:25,374] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic connect-configs with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-24 07:24:25,429] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group kafka-connect in Empty state. Created a new member id connect-1-01c53243-0c4e-47fd-b414-782590469941 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:24:25,432] INFO [GroupCoordinator 1]: Preparing to rebalance group kafka-connect in state PreparingRebalance with old generation 0 (__consumer_offsets-11) (reason: Adding new member connect-1-01c53243-0c4e-47fd-b414-782590469941 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:24:25,433] INFO [GroupCoordinator 1]: Stabilized group kafka-connect generation 1 (__consumer_offsets-11) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-08-24 07:24:25,452] INFO [GroupCoordinator 1]: Assignment received from leader connect-1-01c53243-0c4e-47fd-b414-782590469941 for group kafka-connect for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
