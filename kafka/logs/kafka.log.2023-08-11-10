[2023-08-11 10:41:49,677] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2023-08-11 10:41:51,168] INFO KafkaConfig values: 
	advertised.listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = zhRYba3JTRSe5KTHHN-Hnw
	client.quota.callback.class = null
	compression.type = producer
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.demotion.support.enabled = false
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = true
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = ANY_UNEVEN_LOAD
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = false
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.resource.utilization.detector.enabled = false
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.partition.maximum.movements = 5
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.suspension.ms = 10800000
	confluent.balancer.topic.replication.factor = 1
	confluent.balancer.triggering.goals = []
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.kraft.mitigation.enabled = false
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.max.topics.per.snapshot = 5000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud
	confluent.cdc.api.keys.topic = 
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 1
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.metadata.load.threads = 32
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.throttle.ms = null
	confluent.metadata.active.encryptor = null
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-01:19092
	confluent.min.acks = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.prefer.tier.fetch.ms = -1
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.dynamic.reporting.min.usage = 102400
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = false
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = http://schema-registry:8081
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.telemetry.enabled = true
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.producer.enable.idempotence = false
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.partition.default.placement = 2
	confluent.topic.replica.assignor.builder.class = 
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id = 
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.valid.broker.rack.set = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-01:29092]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = INTERNAL
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
	listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092,CONTROLLER://kafka-01:29092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dirs = /etc/confluent/kraft-combined-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 2000000
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter, io.confluent.telemetry.reporter.TelemetryReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = [hidden]
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.id.quota.window.num = 11
	producer.id.quota.window.size.seconds = 1
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 2097152
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.acl.change.notification.expiration.ms = 900000
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2023-08-11 10:41:51,197] INFO KafkaConfig values: 
	advertised.listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = zhRYba3JTRSe5KTHHN-Hnw
	client.quota.callback.class = null
	compression.type = producer
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.demotion.support.enabled = false
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = true
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = ANY_UNEVEN_LOAD
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = false
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.resource.utilization.detector.enabled = false
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.partition.maximum.movements = 5
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.suspension.ms = 10800000
	confluent.balancer.topic.replication.factor = 1
	confluent.balancer.triggering.goals = []
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.kraft.mitigation.enabled = false
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.max.topics.per.snapshot = 5000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud
	confluent.cdc.api.keys.topic = 
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 1
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.metadata.load.threads = 32
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.throttle.ms = null
	confluent.metadata.active.encryptor = null
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-01:19092
	confluent.min.acks = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.prefer.tier.fetch.ms = -1
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.dynamic.reporting.min.usage = 102400
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = false
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = http://schema-registry:8081
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.telemetry.enabled = true
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.producer.enable.idempotence = false
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.partition.default.placement = 2
	confluent.topic.replica.assignor.builder.class = 
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id = 
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.valid.broker.rack.set = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-01:29092]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = INTERNAL
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
	listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092,CONTROLLER://kafka-01:29092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dirs = /etc/confluent/kraft-combined-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 2000000
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter, io.confluent.telemetry.reporter.TelemetryReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = [hidden]
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.id.quota.window.num = 11
	producer.id.quota.window.size.seconds = 1
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 2097152
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.acl.change.notification.expiration.ms = 900000
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2023-08-11 10:41:51,208] INFO Starting controller (kafka.server.ControllerServer)
[2023-08-11 10:41:51,535] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas)
[2023-08-11 10:41:51,547] INFO Awaiting socket connections on kafka-01:29092. (kafka.network.DataPlaneAcceptor)
[2023-08-11 10:41:51,601] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)
[2023-08-11 10:41:51,612] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer)
[2023-08-11 10:41:51,782] INFO [MergedLog partition=__cluster_metadata-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:41:51,782] INFO [MergedLog partition=__cluster_metadata-0, dir=/etc/confluent/kraft-combined-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.MergedLog$)
[2023-08-11 10:41:51,783] INFO [MergedLog partition=__cluster_metadata-0, dir=/etc/confluent/kraft-combined-logs] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.MergedLog$)
[2023-08-11 10:41:51,801] INFO Initialized snapshots with IDs SortedSet() from /etc/confluent/kraft-combined-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
[2023-08-11 10:41:51,826] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
[2023-08-11 10:41:51,922] INFO [kafka-raft-outbound-request-thread]: Starting (kafka.raft.RaftSendThread)
[2023-08-11 10:41:51,922] INFO [kafka-raft-io-thread]: Starting (kafka.raft.KafkaRaftManager$RaftIoThread)
[2023-08-11 10:41:52,030] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-11 10:41:52,032] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2023-08-11 10:41:52,033] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-11 10:41:52,048] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2023-08-11 10:41:52,049] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-11 10:41:52,051] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2023-08-11 10:41:52,053] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-11 10:41:52,075] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-11 10:41:52,089] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
[2023-08-11 10:41:52,097] INFO [BrokerServer id=1] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
[2023-08-11 10:41:52,098] INFO [BrokerServer id=1] Starting broker (kafka.server.BrokerServer)
[2023-08-11 10:41:52,128] INFO [BrokerServer id=1] FIPS mode enabled: false (kafka.server.BrokerServer)
[2023-08-11 10:41:52,145] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2023-08-11 10:41:52,146] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-11 10:41:52,147] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-11 10:41:52,148] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2023-08-11 10:41:52,153] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[2023-08-11 10:41:52,154] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-11 10:41:52,155] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[2023-08-11 10:41:52,208] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread)
[2023-08-11 10:41:52,210] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Recorded new controller, from now on will use node kafka-01:29092 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2023-08-11 10:41:52,279] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas)
[2023-08-11 10:41:52,280] INFO Awaiting socket connections on kafka-01:19092. (kafka.network.DataPlaneAcceptor)
[2023-08-11 10:41:52,289] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(INTERNAL) (kafka.network.SocketServer)
[2023-08-11 10:41:52,291] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas)
[2023-08-11 10:41:52,291] INFO Awaiting socket connections on kafka-01:9092. (kafka.network.DataPlaneAcceptor)
[2023-08-11 10:41:52,300] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2023-08-11 10:41:52,326] INFO [BrokerToControllerChannelManager broker=1 name=alterPartition]: Starting (kafka.server.BrokerToControllerRequestThread)
[2023-08-11 10:41:52,327] INFO [BrokerToControllerChannelManager broker=1 name=alterPartition]: Recorded new controller, from now on will use node kafka-01:29092 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2023-08-11 10:41:52,341] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-11 10:41:52,341] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-11 10:41:52,342] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-11 10:41:52,344] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-11 10:41:52,345] INFO [ExpirationReaper-1-ListOffsets]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-11 10:41:52,379] INFO [BrokerHealthManager]: Starting (kafka.availability.BrokerHealthManager)
[2023-08-11 10:41:52,385] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-11 10:41:52,392] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-11 10:41:54,474] INFO [BrokerLifecycleManager id=1] Incarnation XiToQRkgT-OjwgsNXCU1yQ of broker 1 in cluster MkU3OEVBNTcwNTJENDM2Qg is now STARTING. (kafka.server.BrokerLifecycleManager)
[2023-08-11 10:41:54,475] INFO [BrokerToControllerChannelManager broker=1 name=heartbeat]: Starting (kafka.server.BrokerToControllerRequestThread)
[2023-08-11 10:41:54,476] INFO [BrokerToControllerChannelManager broker=1 name=heartbeat]: Recorded new controller, from now on will use node kafka-01:29092 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2023-08-11 10:41:54,552] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2023-08-11 10:41:54,569] INFO [BrokerServer id=1] Waiting for broker metadata to catch up. (kafka.server.BrokerServer)
[2023-08-11 10:41:54,609] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 7 (kafka.server.BrokerLifecycleManager)
[2023-08-11 10:41:54,619] INFO [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
[2023-08-11 10:41:54,627] INFO [BrokerMetadataListener id=1] Starting to publish metadata events at offset 7. (kafka.server.metadata.BrokerMetadataListener)
[2023-08-11 10:41:54,634] INFO [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=7, epoch=1) with metadata.version 3.4-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
[2023-08-11 10:41:54,635] INFO Loading logs from log dirs ArraySeq(/etc/confluent/kraft-combined-logs) (kafka.log.LogManager)
[2023-08-11 10:41:54,637] INFO [BrokerLifecycleManager id=1] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
[2023-08-11 10:41:54,638] INFO Attempting recovery for all logs in /etc/confluent/kraft-combined-logs since no clean shutdown file was found (kafka.log.LogManager)
[2023-08-11 10:41:54,647] INFO Loaded 0 logs in 11ms. (kafka.log.LogManager)
[2023-08-11 10:41:54,647] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2023-08-11 10:41:54,650] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2023-08-11 10:41:54,679] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2023-08-11 10:41:54,679] INFO Starting the log cleaner (kafka.log.LogCleaner)
[2023-08-11 10:41:54,775] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner)
[2023-08-11 10:41:54,775] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner)
[2023-08-11 10:41:54,781] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2023-08-11 10:41:54,782] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:41:54,785] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:41:54,787] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:41:54,793] INFO [Transaction Marker Channel Manager 1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2023-08-11 10:41:54,793] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:41:54,796] INFO [BrokerMetadataPublisher id=1] Updating metadata.version to 3.4-IV0 at offset OffsetAndEpoch(offset=7, epoch=1). (kafka.server.metadata.BrokerMetadataPublisher)
[2023-08-11 10:41:54,820] INFO KafkaConfig values: 
	advertised.listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = zhRYba3JTRSe5KTHHN-Hnw
	client.quota.callback.class = null
	compression.type = producer
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.demotion.support.enabled = false
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = true
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = ANY_UNEVEN_LOAD
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = false
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.resource.utilization.detector.enabled = false
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.partition.maximum.movements = 5
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.suspension.ms = 10800000
	confluent.balancer.topic.replication.factor = 1
	confluent.balancer.triggering.goals = []
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.kraft.mitigation.enabled = false
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.max.topics.per.snapshot = 5000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud
	confluent.cdc.api.keys.topic = 
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 1
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.metadata.load.threads = 32
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.throttle.ms = null
	confluent.metadata.active.encryptor = null
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-01:19092
	confluent.min.acks = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.prefer.tier.fetch.ms = -1
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.dynamic.reporting.min.usage = 102400
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = false
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = http://schema-registry:8081
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.telemetry.enabled = true
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.producer.enable.idempotence = false
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.partition.default.placement = 2
	confluent.topic.replica.assignor.builder.class = 
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id = 
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.valid.broker.rack.set = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-01:29092]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = INTERNAL
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
	listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092,CONTROLLER://kafka-01:29092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dirs = /etc/confluent/kraft-combined-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 2000000
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter, io.confluent.telemetry.reporter.TelemetryReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = [hidden]
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.id.quota.window.num = 11
	producer.id.quota.window.size.seconds = 1
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 2097152
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.acl.change.notification.expiration.ms = 900000
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2023-08-11 10:41:54,838] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
[2023-08-11 10:41:54,926] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
[2023-08-11 10:41:55,105] INFO KafkaConfig values: 
	advertised.listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.include.jmx.reporter = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 1
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	broker.session.timeout.ms = 9000
	broker.session.uuid = zhRYba3JTRSe5KTHHN-Hnw
	client.quota.callback.class = null
	compression.type = producer
	confluent.alter.broker.health.max.demoted.brokers = 2147483647
	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
	confluent.balancer.demotion.support.enabled = false
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.disk.min.free.space.gb = 0
	confluent.balancer.disk.utilization.detector.duration.ms = 600000
	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
	confluent.balancer.enable = true
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = ANY_UNEVEN_LOAD
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = false
	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
	confluent.balancer.incremental.balancing.enabled = false
	confluent.balancer.incremental.balancing.goals = []
	confluent.balancer.incremental.balancing.lower.bound = 0.02
	confluent.balancer.incremental.balancing.step.ratio = 0.2
	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.rebalancing.goals = []
	confluent.balancer.resource.utilization.detector.enabled = false
	confluent.balancer.resource.utilization.detector.interval.ms = 60000
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.partition.maximum.movements = 5
	confluent.balancer.topic.partition.movement.expiration.ms = 3600000
	confluent.balancer.topic.partition.suspension.ms = 10800000
	confluent.balancer.topic.replication.factor = 1
	confluent.balancer.triggering.goals = []
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
	confluent.bearer.auth.client.id = null
	confluent.bearer.auth.client.secret = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.identity.pool.id = null
	confluent.bearer.auth.issuer.endpoint.url = null
	confluent.bearer.auth.logical.cluster = null
	confluent.bearer.auth.scope = null
	confluent.bearer.auth.scope.claim.name = scope
	confluent.bearer.auth.sub.claim.name = sub
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.kraft.mitigation.enabled = false
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
	confluent.catalog.collector.enable = false
	confluent.catalog.collector.max.topics.per.snapshot = 5000
	confluent.catalog.collector.max.topics.process = 500
	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
	confluent.catalog.collector.snapshot.init.delay.sec = 60
	confluent.catalog.collector.snapshot.interval.sec = 300
	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud
	confluent.cdc.api.keys.topic = 
	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
	confluent.cdc.client.quotas.enable = false
	confluent.cdc.client.quotas.topic.name = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.cdc.user.metadata.enable = false
	confluent.cdc.user.metadata.topic = _confluent-user_metadata
	confluent.cells.default.size = 15
	confluent.cells.enable = false
	confluent.cells.implicit.creation.enable = false
	confluent.cells.max.size = 15
	confluent.cells.min.size = 6
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.clm.max.backup.days = 3
	confluent.clm.min.delay.in.minutes = 30
	confluent.clm.thread.pool.size = 2
	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = false
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 1
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.compacted.topic.prefer.tier.fetch.ms = -1
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
	confluent.durability.audit.enable = false
	confluent.durability.audit.idempotent.producer = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.io.bytes.per.sec = 10485760
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.from.follower.require.leader.epoch.enable = false
	confluent.fetch.partition.pruning.enable = true
	confluent.group.coordinator.offsets.batching.enable = false
	confluent.group.coordinator.offsets.writer.threads = 2
	confluent.group.metadata.load.threads = 32
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
	confluent.max.connection.throttle.ms = null
	confluent.metadata.active.encryptor = null
	confluent.metadata.encryptor.classes = null
	confluent.metadata.encryptor.secrets = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.metrics.reporter.bootstrap.servers = kafka-01:19092
	confluent.min.acks = 0
	confluent.min.segment.ms = 1
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.network.health.manager.enabled = false
	confluent.network.health.manager.min.healthy.network.samples = 3
	confluent.network.health.manager.mitigation.enabled = false
	confluent.network.health.manager.network.sample.window.size = 120
	confluent.network.health.manager.sample.duration.ms = 1000
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = false
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
	confluent.prefer.tier.fetch.ms = -1
	confluent.producer.id.cache.limit = 2147483647
	confluent.producer.id.quota.manager.enable = false
	confluent.producer.id.throttle.enable = false
	confluent.producer.id.throttle.enable.threshold.percentage = 100
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.enable = false
	confluent.quota.dynamic.publishing.interval.ms = 60000
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.dynamic.reporting.min.usage = 102400
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.backoff.max.ms = 1000
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.request.pipelining.enable = false
	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = http://schema-registry:8081
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.storage.probe.slow.write.threshold.ms = 5000
	confluent.stray.log.delete.delay.ms = 604800000
	confluent.stray.log.max.deletions.per.run = 72
	confluent.telemetry.enabled = true
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.bucket.probe.period.ms = -1
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.dual.compaction = false
	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
	confluent.tier.cleaner.dual.compaction.validation.percent = 0
	confluent.tier.cleaner.enable = false
	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.metadata.snapshots.enable = false
	confluent.tier.metadata.snapshots.interval.ms = 86400000
	confluent.tier.metadata.snapshots.retention.days = 7
	confluent.tier.metadata.snapshots.threads = 2
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
	confluent.tier.partition.state.cleanup.enable = false
	confluent.tier.partition.state.cleanup.interval.ms = 86400000
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.tier.topic.producer.enable.idempotence = false
	confluent.tier.topic.snapshots.enable = false
	confluent.tier.topic.snapshots.interval.ms = 300000
	confluent.tier.topic.snapshots.max.records = 100000
	confluent.tier.topic.snapshots.retention.hours = 168
	confluent.topic.partition.default.placement = 2
	confluent.topic.replica.assignor.builder.class = 
	confluent.traffic.cdc.network.id.routes.enable = false
	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
	confluent.traffic.network.id = 
	confluent.transaction.logging.verbosity = 0
	confluent.transaction.state.log.placement.constraints = 
	confluent.valid.broker.rack.set = null
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = CONTROLLER
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = [1@kafka-01:29092]
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.cluster.link.policy.class.name = null
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	early.start.listeners = null
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = INTERNAL
	inter.broker.protocol.version = 3.4-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
	listeners = INTERNAL://kafka-01:19092,PLAINTEXT://kafka-01:9092,CONTROLLER://kafka-01:29092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.hash.algorithm = MD5
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
	log.dir = /tmp/kafka-logs
	log.dirs = /etc/confluent/kraft-combined-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 1.7976931348623157E308
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 2000000
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.max.snapshot.interval.ms = 3600000
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.idle.interval.ms = 500
	metadata.max.retention.bytes = 104857600
	metadata.max.retention.ms = 604800000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter, io.confluent.telemetry.reporter.TelemetryReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.check.ms = 120000
	multitenant.tenant.delete.delay = 604800000
	node.id = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = [hidden]
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = [broker, controller]
	producer.id.expiration.check.interval.ms = 600000
	producer.id.expiration.ms = 86400000
	producer.id.quota.window.num = 11
	producer.id.quota.window.size.seconds = 1
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.consumption.expiration.time.ms = 600000
	quotas.expiration.interval.ms = 3600000
	quotas.expiration.time.ms = 604800000
	quotas.lazy.evaluation.threshold = 0.5
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 2097152
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.authn.async.enable = false
	sasl.server.authn.async.max.threads = 1
	sasl.server.authn.async.timeout.ms = 30000
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.acl.change.notification.expiration.ms = 900000
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.metadata.migration.enable = false
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
 (kafka.server.KafkaConfig)
[2023-08-11 10:41:59,096] INFO [BrokerServer id=1] Skipping durability audit instantiation (kafka.server.BrokerServer)
[2023-08-11 10:41:59,511] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-command-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:41:59,667] INFO [MergedLog partition=_confluent-command-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:41:59,686] INFO Created log for partition _confluent-command-0 in /etc/confluent/kraft-combined-logs/_confluent-command-0 with properties {cleanup.policy=compact, min.insync.replicas=1} (kafka.log.LogManager)
[2023-08-11 10:41:59,695] INFO [Partition _confluent-command-0 broker=1] No checkpointed highwatermark is found for partition _confluent-command-0 (kafka.cluster.Partition)
[2023-08-11 10:41:59,733] INFO [Partition _confluent-command-0 broker=1] Log loaded for partition _confluent-command-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:41:59,736] INFO Setting topicIdPartition 8d3c9wqZRYOG10xzYRq6zw:_confluent-command-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:41:59,747] INFO [MergedLog partition=_confluent-command-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-command-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:41:59,884] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-command with new configuration : cleanup.policy -> compact,min.insync.replicas -> 1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:41:59,943] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:41:59,969] INFO [MergedLog partition=_schemas-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:00,067] INFO Created log for partition _schemas-0 in /etc/confluent/kraft-combined-logs/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:00,091] INFO [Partition _schemas-0 broker=1] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[2023-08-11 10:42:00,092] INFO [Partition _schemas-0 broker=1] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:00,092] INFO Setting topicIdPartition PO_BOmdHSQ2btOk0igtYpg:_schemas-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:00,092] INFO [MergedLog partition=_schemas-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _schemas-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:00,094] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _schemas with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:00,980] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2023-08-11 10:42:01,051] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:01,216] INFO Starting delay for broker load metric (kafka.metrics.BrokerLoad)
[2023-08-11 10:42:01,219] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer)
[2023-08-11 10:42:01,222] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer)
[2023-08-11 10:42:01,231] INFO [MergedLog partition=__consumer_offsets-13, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,233] INFO Created log for partition __consumer_offsets-13 in /etc/confluent/kraft-combined-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,239] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2023-08-11 10:42:01,239] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,239] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-13 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,240] INFO [MergedLog partition=__consumer_offsets-13, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-13 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,247] INFO [MergedLog partition=__consumer_offsets-46, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,250] INFO Created log for partition __consumer_offsets-46 in /etc/confluent/kraft-combined-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,250] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2023-08-11 10:42:01,250] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,251] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-46 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,257] INFO [MergedLog partition=__consumer_offsets-46, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-46 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,286] INFO [MergedLog partition=__consumer_offsets-9, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,288] INFO Created log for partition __consumer_offsets-9 in /etc/confluent/kraft-combined-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,288] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2023-08-11 10:42:01,288] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,289] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-9 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,289] INFO [MergedLog partition=__consumer_offsets-9, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,369] INFO [MergedLog partition=__consumer_offsets-42, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,370] INFO Created log for partition __consumer_offsets-42 in /etc/confluent/kraft-combined-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,370] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2023-08-11 10:42:01,371] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,371] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-42 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,371] INFO [MergedLog partition=__consumer_offsets-42, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-42 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,395] INFO [MergedLog partition=__consumer_offsets-21, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,397] INFO Created log for partition __consumer_offsets-21 in /etc/confluent/kraft-combined-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,398] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2023-08-11 10:42:01,398] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,399] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-21 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,399] INFO [MergedLog partition=__consumer_offsets-21, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-21 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,415] INFO [MergedLog partition=__consumer_offsets-17, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,441] INFO Created log for partition __consumer_offsets-17 in /etc/confluent/kraft-combined-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,441] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2023-08-11 10:42:01,441] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,441] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-17 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,441] INFO [MergedLog partition=__consumer_offsets-17, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-17 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,462] INFO [MergedLog partition=__consumer_offsets-30, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,463] INFO Created log for partition __consumer_offsets-30 in /etc/confluent/kraft-combined-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,470] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2023-08-11 10:42:01,470] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,472] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-30 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,473] INFO [MergedLog partition=__consumer_offsets-30, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-30 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,493] INFO [MergedLog partition=__consumer_offsets-26, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,495] INFO Created log for partition __consumer_offsets-26 in /etc/confluent/kraft-combined-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,504] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2023-08-11 10:42:01,505] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,505] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-26 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,505] INFO [MergedLog partition=__consumer_offsets-26, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-26 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,509] INFO [MergedLog partition=__consumer_offsets-5, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,524] INFO Created log for partition __consumer_offsets-5 in /etc/confluent/kraft-combined-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,524] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2023-08-11 10:42:01,525] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,525] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-5 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,525] INFO [MergedLog partition=__consumer_offsets-5, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,529] INFO [MergedLog partition=__consumer_offsets-38, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,578] INFO Created log for partition __consumer_offsets-38 in /etc/confluent/kraft-combined-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,578] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2023-08-11 10:42:01,578] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,578] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-38 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,597] INFO [MergedLog partition=__consumer_offsets-38, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-38 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,625] INFO [MergedLog partition=__consumer_offsets-1, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,628] INFO Created log for partition __consumer_offsets-1 in /etc/confluent/kraft-combined-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,628] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2023-08-11 10:42:01,628] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,628] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-1 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,629] INFO [MergedLog partition=__consumer_offsets-1, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,648] INFO [MergedLog partition=__consumer_offsets-34, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,649] INFO Created log for partition __consumer_offsets-34 in /etc/confluent/kraft-combined-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,649] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2023-08-11 10:42:01,649] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,649] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-34 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,649] INFO [MergedLog partition=__consumer_offsets-34, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-34 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,656] INFO [MergedLog partition=__consumer_offsets-16, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,657] INFO Created log for partition __consumer_offsets-16 in /etc/confluent/kraft-combined-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,657] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2023-08-11 10:42:01,657] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,657] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-16 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,658] INFO [MergedLog partition=__consumer_offsets-16, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-16 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,669] INFO [MergedLog partition=__consumer_offsets-45, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,670] INFO Created log for partition __consumer_offsets-45 in /etc/confluent/kraft-combined-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,671] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2023-08-11 10:42:01,671] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,672] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-45 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,672] INFO [MergedLog partition=__consumer_offsets-45, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-45 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,676] INFO [MergedLog partition=__consumer_offsets-12, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,684] INFO Created log for partition __consumer_offsets-12 in /etc/confluent/kraft-combined-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,684] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2023-08-11 10:42:01,685] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,685] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-12 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,686] INFO [MergedLog partition=__consumer_offsets-12, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-12 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,690] INFO [MergedLog partition=__consumer_offsets-41, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,691] INFO Created log for partition __consumer_offsets-41 in /etc/confluent/kraft-combined-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,691] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2023-08-11 10:42:01,691] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,692] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-41 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,692] INFO [MergedLog partition=__consumer_offsets-41, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-41 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,700] INFO [MergedLog partition=__consumer_offsets-24, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,701] INFO Created log for partition __consumer_offsets-24 in /etc/confluent/kraft-combined-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,735] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2023-08-11 10:42:01,735] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,736] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-24 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,736] INFO [MergedLog partition=__consumer_offsets-24, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-24 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,753] INFO [MergedLog partition=__consumer_offsets-20, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,754] INFO Created log for partition __consumer_offsets-20 in /etc/confluent/kraft-combined-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,755] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2023-08-11 10:42:01,755] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,755] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-20 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,756] INFO [MergedLog partition=__consumer_offsets-20, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-20 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,768] INFO [MergedLog partition=__consumer_offsets-49, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,770] INFO Created log for partition __consumer_offsets-49 in /etc/confluent/kraft-combined-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,771] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2023-08-11 10:42:01,771] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,771] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-49 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,771] INFO [MergedLog partition=__consumer_offsets-49, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-49 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,774] INFO [MergedLog partition=__consumer_offsets-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,780] INFO Created log for partition __consumer_offsets-0 in /etc/confluent/kraft-combined-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,781] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,784] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,785] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,785] INFO [MergedLog partition=__consumer_offsets-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,789] INFO [MergedLog partition=__consumer_offsets-29, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,791] INFO Created log for partition __consumer_offsets-29 in /etc/confluent/kraft-combined-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,799] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2023-08-11 10:42:01,799] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,800] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-29 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,800] INFO [MergedLog partition=__consumer_offsets-29, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-29 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,806] INFO [MergedLog partition=__consumer_offsets-25, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,811] INFO Created log for partition __consumer_offsets-25 in /etc/confluent/kraft-combined-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,811] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2023-08-11 10:42:01,811] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,812] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-25 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,813] INFO [MergedLog partition=__consumer_offsets-25, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-25 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,823] INFO [MergedLog partition=__consumer_offsets-8, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,824] INFO Created log for partition __consumer_offsets-8 in /etc/confluent/kraft-combined-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,839] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2023-08-11 10:42:01,840] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,840] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-8 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,840] INFO [MergedLog partition=__consumer_offsets-8, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,844] INFO [MergedLog partition=__consumer_offsets-37, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,852] INFO Created log for partition __consumer_offsets-37 in /etc/confluent/kraft-combined-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,852] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2023-08-11 10:42:01,853] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,857] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-37 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,857] INFO [MergedLog partition=__consumer_offsets-37, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-37 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,861] INFO [MergedLog partition=__consumer_offsets-4, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,862] INFO Created log for partition __consumer_offsets-4 in /etc/confluent/kraft-combined-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,869] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2023-08-11 10:42:01,873] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,873] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-4 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,876] INFO [MergedLog partition=__consumer_offsets-4, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,887] INFO [MergedLog partition=__consumer_offsets-33, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,888] INFO Created log for partition __consumer_offsets-33 in /etc/confluent/kraft-combined-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,890] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2023-08-11 10:42:01,890] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,890] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-33 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,891] INFO [MergedLog partition=__consumer_offsets-33, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-33 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,926] INFO [MergedLog partition=__consumer_offsets-15, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,928] INFO Created log for partition __consumer_offsets-15 in /etc/confluent/kraft-combined-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,929] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2023-08-11 10:42:01,929] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,929] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-15 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,929] INFO [MergedLog partition=__consumer_offsets-15, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-15 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,932] INFO [MergedLog partition=__consumer_offsets-48, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,933] INFO Created log for partition __consumer_offsets-48 in /etc/confluent/kraft-combined-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,933] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2023-08-11 10:42:01,933] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,933] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-48 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,933] INFO [MergedLog partition=__consumer_offsets-48, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-48 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,964] INFO [MergedLog partition=__consumer_offsets-11, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,966] INFO Created log for partition __consumer_offsets-11 in /etc/confluent/kraft-combined-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,966] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2023-08-11 10:42:01,966] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,966] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-11 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,966] INFO [MergedLog partition=__consumer_offsets-11, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,970] INFO [MergedLog partition=__consumer_offsets-44, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,970] INFO Created log for partition __consumer_offsets-44 in /etc/confluent/kraft-combined-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,970] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2023-08-11 10:42:01,970] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,971] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-44 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,971] INFO [MergedLog partition=__consumer_offsets-44, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-44 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,982] INFO [MergedLog partition=__consumer_offsets-23, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,986] INFO Created log for partition __consumer_offsets-23 in /etc/confluent/kraft-combined-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,986] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2023-08-11 10:42:01,986] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,986] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-23 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,987] INFO [MergedLog partition=__consumer_offsets-23, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-23 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:01,990] INFO [MergedLog partition=__consumer_offsets-19, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:01,996] INFO Created log for partition __consumer_offsets-19 in /etc/confluent/kraft-combined-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:01,996] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2023-08-11 10:42:01,996] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:01,996] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-19 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:01,996] INFO [MergedLog partition=__consumer_offsets-19, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-19 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:02,005] INFO [MergedLog partition=__consumer_offsets-32, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:02,007] INFO Created log for partition __consumer_offsets-32 in /etc/confluent/kraft-combined-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:02,007] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2023-08-11 10:42:02,007] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:02,008] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-32 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:02,009] INFO [MergedLog partition=__consumer_offsets-32, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-32 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:02,016] INFO [MergedLog partition=__consumer_offsets-28, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:02,017] INFO Created log for partition __consumer_offsets-28 in /etc/confluent/kraft-combined-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:02,018] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2023-08-11 10:42:02,018] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:02,018] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-28 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:02,019] INFO [MergedLog partition=__consumer_offsets-28, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-28 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:02,022] INFO [MergedLog partition=__consumer_offsets-7, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:02,023] INFO Created log for partition __consumer_offsets-7 in /etc/confluent/kraft-combined-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:02,025] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2023-08-11 10:42:02,025] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:02,026] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-7 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:02,026] INFO [MergedLog partition=__consumer_offsets-7, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:02,038] INFO [MergedLog partition=__consumer_offsets-40, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:02,039] INFO Created log for partition __consumer_offsets-40 in /etc/confluent/kraft-combined-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:02,042] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2023-08-11 10:42:02,042] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:02,042] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-40 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:02,043] INFO [MergedLog partition=__consumer_offsets-40, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-40 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:02,046] INFO [MergedLog partition=__consumer_offsets-3, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:02,047] INFO Created log for partition __consumer_offsets-3 in /etc/confluent/kraft-combined-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:02,049] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2023-08-11 10:42:02,049] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:02,049] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-3 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:02,050] INFO [MergedLog partition=__consumer_offsets-3, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:02,061] INFO [MergedLog partition=__consumer_offsets-36, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:02,071] INFO Created log for partition __consumer_offsets-36 in /etc/confluent/kraft-combined-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:02,071] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2023-08-11 10:42:02,071] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:02,071] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-36 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:02,072] INFO [MergedLog partition=__consumer_offsets-36, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-36 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:02,076] INFO [MergedLog partition=__consumer_offsets-47, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:02,087] INFO Created log for partition __consumer_offsets-47 in /etc/confluent/kraft-combined-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:02,087] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2023-08-11 10:42:02,087] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:02,088] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-47 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:02,088] INFO [MergedLog partition=__consumer_offsets-47, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-47 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:02,093] INFO [MergedLog partition=__consumer_offsets-14, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:02,094] INFO Created log for partition __consumer_offsets-14 in /etc/confluent/kraft-combined-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:02,099] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2023-08-11 10:42:02,099] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:02,099] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-14 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:02,107] INFO [MergedLog partition=__consumer_offsets-14, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-14 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:02,111] INFO [MergedLog partition=__consumer_offsets-43, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:02,118] INFO Created log for partition __consumer_offsets-43 in /etc/confluent/kraft-combined-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:02,118] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2023-08-11 10:42:02,118] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:02,119] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-43 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:02,127] INFO [MergedLog partition=__consumer_offsets-43, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-43 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:02,133] INFO [MergedLog partition=__consumer_offsets-10, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:02,134] INFO Created log for partition __consumer_offsets-10 in /etc/confluent/kraft-combined-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:02,135] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2023-08-11 10:42:02,135] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:02,149] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-10 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:02,150] INFO [MergedLog partition=__consumer_offsets-10, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:02,153] INFO [MergedLog partition=__consumer_offsets-22, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:02,154] INFO Created log for partition __consumer_offsets-22 in /etc/confluent/kraft-combined-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:02,155] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2023-08-11 10:42:02,159] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:02,159] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-22 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:02,160] INFO [MergedLog partition=__consumer_offsets-22, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-22 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:02,172] INFO [MergedLog partition=__consumer_offsets-18, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:02,180] INFO Created log for partition __consumer_offsets-18 in /etc/confluent/kraft-combined-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:02,183] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2023-08-11 10:42:02,183] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:02,184] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-18 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:02,184] INFO [MergedLog partition=__consumer_offsets-18, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-18 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:02,194] ERROR [KafkaApi-1] Unexpected error handling request RequestHeader(apiKey=INIT_PRODUCER_ID, apiVersion=4, clientId=confluent-control-center-heartbeat-sender-1, correlationId=2, headerVersion=2) -- InitProducerIdRequestData(transactionalId=null, transactionTimeoutMs=2147483647, producerId=-1, producerEpoch=-1) with context RequestContext(header=RequestHeader(apiKey=INIT_PRODUCER_ID, apiVersion=4, clientId=confluent-control-center-heartbeat-sender-1, correlationId=2, headerVersion=2), connectionId='172.20.0.2:19092-172.20.0.6:57776-12', clientAddress=/172.20.0.6, principal=User:ANONYMOUS, listenerName=ListenerName(INTERNAL), securityProtocol=PLAINTEXT, clientInformation=ClientInformation(softwareName=apache-kafka-java, softwareVersion=7.4.1-ce), fromPrivilegedListener=true, principalSerde=Optional[org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder@7f17036e]) (kafka.server.KafkaApis)
org.apache.kafka.common.errors.CoordinatorLoadInProgressException: Timed out waiting for next producer ID block
[2023-08-11 10:42:02,195] INFO [MergedLog partition=__consumer_offsets-31, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:02,196] INFO Created log for partition __consumer_offsets-31 in /etc/confluent/kraft-combined-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:02,196] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2023-08-11 10:42:02,197] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:02,197] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-31 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:02,200] INFO [MergedLog partition=__consumer_offsets-31, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-31 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:02,213] INFO [MergedLog partition=__consumer_offsets-27, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:02,214] INFO Created log for partition __consumer_offsets-27 in /etc/confluent/kraft-combined-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:02,214] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2023-08-11 10:42:02,214] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:02,214] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-27 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:02,214] INFO [MergedLog partition=__consumer_offsets-27, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-27 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:02,218] INFO [MergedLog partition=__consumer_offsets-39, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:02,228] INFO Created log for partition __consumer_offsets-39 in /etc/confluent/kraft-combined-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:02,228] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2023-08-11 10:42:02,228] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:02,228] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-39 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:02,228] INFO [MergedLog partition=__consumer_offsets-39, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-39 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:02,233] INFO [MergedLog partition=__consumer_offsets-6, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:02,240] INFO Created log for partition __consumer_offsets-6 in /etc/confluent/kraft-combined-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:02,240] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2023-08-11 10:42:02,240] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:02,240] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-6 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:02,240] INFO [MergedLog partition=__consumer_offsets-6, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:02,252] INFO [MergedLog partition=__consumer_offsets-35, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:02,253] INFO Created log for partition __consumer_offsets-35 in /etc/confluent/kraft-combined-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:02,256] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2023-08-11 10:42:02,257] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:02,257] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-35 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:02,257] INFO [MergedLog partition=__consumer_offsets-35, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-35 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:02,261] INFO [MergedLog partition=__consumer_offsets-2, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:02,271] INFO Created log for partition __consumer_offsets-2 in /etc/confluent/kraft-combined-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600} (kafka.log.LogManager)
[2023-08-11 10:42:02,271] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2023-08-11 10:42:02,271] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:02,272] INFO Setting topicIdPartition SmZQLwtCQbqAKk1ytF2fDw:__consumer_offsets-2 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:02,272] INFO [MergedLog partition=__consumer_offsets-2, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:02,274] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,278] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,287] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,307] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,318] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,319] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,327] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,327] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,328] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,328] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,331] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,331] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,335] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,335] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,359] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,359] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,359] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,311] INFO The cleaning for partition __consumer_offsets-13 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,421] INFO The cleaning for partition __consumer_offsets-21 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,421] INFO The cleaning for partition __consumer_offsets-17 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,419] INFO The cleaning for partition __consumer_offsets-42 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,419] INFO The cleaning for partition __consumer_offsets-9 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,414] INFO The cleaning for partition __consumer_offsets-26 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,414] INFO The cleaning for partition __consumer_offsets-46 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,437] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 130 milliseconds for epoch 0, of which 130 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,407] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,444] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 117 milliseconds for epoch 0, of which 108 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,403] INFO The cleaning for partition __consumer_offsets-30 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,428] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 99 milliseconds for epoch 0, of which 98 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,436] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 105 milliseconds for epoch 0, of which 92 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,445] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 110 milliseconds for epoch 0, of which 110 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,435] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 156 milliseconds for epoch 0, of which 143 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,445] INFO Cleaning for partition __consumer_offsets-46 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,445] INFO The cleaning for partition __consumer_offsets-5 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,446] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 2 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,446] INFO Cleaning for partition __consumer_offsets-5 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,435] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 116 milliseconds for epoch 0, of which 103 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,472] INFO Cleaning for partition __consumer_offsets-9 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,455] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 96 milliseconds for epoch 0, of which 77 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,473] INFO Cleaning for partition __consumer_offsets-26 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,445] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,473] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,476] INFO Cleaning for partition __consumer_offsets-42 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,476] INFO Cleaning for partition __consumer_offsets-17 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,476] INFO Cleaning for partition __consumer_offsets-30 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,477] INFO Cleaning for partition __consumer_offsets-21 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,477] INFO Cleaning for partition __consumer_offsets-13 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,479] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,479] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,479] INFO The cleaning for partition __consumer_offsets-38 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,480] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,480] INFO Cleaning for partition __consumer_offsets-38 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,495] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,495] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,496] INFO The cleaning for partition __consumer_offsets-1 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,496] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 17 milliseconds for epoch 0, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,497] INFO Cleaning for partition __consumer_offsets-1 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,497] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,497] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,497] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,497] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,498] INFO The cleaning for partition __consumer_offsets-16 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,498] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,498] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,498] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,498] INFO Cleaning for partition __consumer_offsets-16 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,498] INFO The cleaning for partition __consumer_offsets-45 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,499] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 1 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,499] INFO Cleaning for partition __consumer_offsets-45 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,499] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,499] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,499] INFO The cleaning for partition __consumer_offsets-12 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,500] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 2 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,500] INFO Cleaning for partition __consumer_offsets-12 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,503] INFO The cleaning for partition __consumer_offsets-34 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,503] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,503] INFO Cleaning for partition __consumer_offsets-34 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,507] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,507] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,507] INFO The cleaning for partition __consumer_offsets-41 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,508] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 9 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,508] INFO Cleaning for partition __consumer_offsets-41 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,508] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,508] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,508] INFO The cleaning for partition __consumer_offsets-24 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,508] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,508] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,509] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,509] INFO Cleaning for partition __consumer_offsets-24 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,509] INFO The cleaning for partition __consumer_offsets-20 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,509] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,509] INFO Cleaning for partition __consumer_offsets-20 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,509] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,509] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,510] INFO The cleaning for partition __consumer_offsets-49 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,510] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,510] INFO Cleaning for partition __consumer_offsets-49 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,510] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,510] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,510] INFO The cleaning for partition __consumer_offsets-0 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,510] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,511] INFO Cleaning for partition __consumer_offsets-0 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,527] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,528] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,528] INFO The cleaning for partition __consumer_offsets-29 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,528] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 18 milliseconds for epoch 0, of which 18 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,528] INFO Cleaning for partition __consumer_offsets-29 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,528] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,528] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,528] INFO The cleaning for partition __consumer_offsets-25 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,529] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,529] INFO Cleaning for partition __consumer_offsets-25 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,533] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,533] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,539] INFO The cleaning for partition __consumer_offsets-8 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,540] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 12 milliseconds for epoch 0, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,541] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,545] INFO The cleaning for partition __consumer_offsets-37 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,549] INFO Cleaning for partition __consumer_offsets-8 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,549] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,550] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 17 milliseconds for epoch 0, of which 17 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,551] INFO Cleaning for partition __consumer_offsets-37 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,551] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,551] INFO The cleaning for partition __consumer_offsets-4 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,551] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,551] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 1 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,552] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,553] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,553] INFO The cleaning for partition __consumer_offsets-33 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,553] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,553] INFO Cleaning for partition __consumer_offsets-33 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,553] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,554] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,554] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,554] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,553] INFO Cleaning for partition __consumer_offsets-4 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,554] INFO The cleaning for partition __consumer_offsets-15 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,555] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,555] INFO Cleaning for partition __consumer_offsets-15 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,555] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,555] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,556] INFO The cleaning for partition __consumer_offsets-48 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,556] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,556] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,556] INFO The cleaning for partition __consumer_offsets-11 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,556] INFO The cleaning for partition __consumer_offsets-44 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,556] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,559] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,559] INFO The cleaning for partition __consumer_offsets-23 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,567] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 13 milliseconds for epoch 0, of which 13 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,568] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,568] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 12 milliseconds for epoch 0, of which 12 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,568] INFO Cleaning for partition __consumer_offsets-23 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,571] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,571] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 16 milliseconds for epoch 0, of which 16 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,571] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,571] INFO Cleaning for partition __consumer_offsets-44 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,571] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,571] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,571] INFO Cleaning for partition __consumer_offsets-48 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,571] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,571] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,571] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,571] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,571] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,571] INFO The cleaning for partition __consumer_offsets-32 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,571] INFO Cleaning for partition __consumer_offsets-11 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,571] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,571] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,571] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,571] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,572] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,572] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,572] INFO The cleaning for partition __consumer_offsets-19 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,572] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 1 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,572] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,572] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,572] INFO The cleaning for partition __consumer_offsets-28 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,572] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,572] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,572] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,572] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,572] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,572] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,572] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,572] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,572] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,572] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,572] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,572] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,572] INFO The cleaning for partition __consumer_offsets-7 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,572] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,572] INFO Cleaning for partition __consumer_offsets-32 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,572] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,573] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,573] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,573] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,573] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,573] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,573] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,574] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,segment.bytes -> 104857600,confluent.placement.constraints ->  (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:02,575] INFO The cleaning for partition __consumer_offsets-40 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,575] INFO The cleaning for partition __consumer_offsets-3 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,575] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,575] INFO Cleaning for partition __consumer_offsets-28 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,575] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,575] INFO Cleaning for partition __consumer_offsets-3 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,575] INFO The cleaning for partition __consumer_offsets-36 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,575] INFO The cleaning for partition __consumer_offsets-47 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,576] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 5 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,576] INFO Cleaning for partition __consumer_offsets-36 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,576] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 4 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,576] INFO Cleaning for partition __consumer_offsets-47 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,576] INFO The cleaning for partition __consumer_offsets-14 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,576] INFO The cleaning for partition __consumer_offsets-43 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,576] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,576] INFO Cleaning for partition __consumer_offsets-14 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,576] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,576] INFO Cleaning for partition __consumer_offsets-43 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,576] INFO The cleaning for partition __consumer_offsets-10 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,576] INFO The cleaning for partition __consumer_offsets-22 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,576] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,576] INFO Cleaning for partition __consumer_offsets-10 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,576] INFO The cleaning for partition __consumer_offsets-31 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,576] INFO The cleaning for partition __consumer_offsets-18 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,576] INFO The cleaning for partition __consumer_offsets-27 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,577] INFO The cleaning for partition __consumer_offsets-39 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,577] INFO The cleaning for partition __consumer_offsets-6 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,577] INFO The cleaning for partition __consumer_offsets-35 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,575] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,576] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 4 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,578] INFO The cleaning for partition __consumer_offsets-2 is aborted and paused (kafka.log.LogManager)
[2023-08-11 10:42:02,580] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,584] INFO Cleaning for partition __consumer_offsets-31 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,587] INFO Cleaning for partition __consumer_offsets-19 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,587] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 14 milliseconds for epoch 0, of which 14 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,587] INFO Cleaning for partition __consumer_offsets-2 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,587] INFO Cleaning for partition __consumer_offsets-7 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,591] INFO Cleaning for partition __consumer_offsets-22 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,592] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 19 milliseconds for epoch 0, of which 18 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,592] INFO Cleaning for partition __consumer_offsets-35 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,592] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 19 milliseconds for epoch 0, of which 19 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,592] INFO Cleaning for partition __consumer_offsets-6 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,592] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 19 milliseconds for epoch 0, of which 19 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,592] INFO Cleaning for partition __consumer_offsets-39 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,592] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 20 milliseconds for epoch 0, of which 20 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,592] INFO Cleaning for partition __consumer_offsets-27 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,595] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 23 milliseconds for epoch 0, of which 23 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,595] INFO Cleaning for partition __consumer_offsets-18 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,595] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 24 milliseconds for epoch 0, of which 24 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[2023-08-11 10:42:02,595] INFO Cleaning for partition __consumer_offsets-40 is resumed (kafka.log.LogManager)
[2023-08-11 10:42:02,665] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-c3a77635-510d-4df6-a54b-471d1261ab8e and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,694] INFO [GroupCoordinator 1]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 0 (__consumer_offsets-29) (reason: Adding new member sr-1-c3a77635-510d-4df6-a54b-471d1261ab8e with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,722] INFO [GroupCoordinator 1]: Stabilized group schema-registry generation 1 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:02,812] INFO [GroupCoordinator 1]: Assignment received from leader sr-1-c3a77635-510d-4df6-a54b-471d1261ab8e for group schema-registry for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:07,914] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:07,920] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:07,921] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:07,922] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 (kafka.cluster.Partition)
[2023-08-11 10:42:07,922] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:07,923] INFO Setting topicIdPartition yVQCuXyHTlaLJOdRqyMzwg:_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:07,923] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:07,923] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerActionsStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:07,952] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:07,957] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:07,958] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:07,960] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
[2023-08-11 10:42:07,960] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:07,960] INFO Setting topicIdPartition 8I_LsOqdQya6IU3VdeyKxg:_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:07,961] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:07,961] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:07,988] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:07,996] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:07,997] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:07,999] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 (kafka.cluster.Partition)
[2023-08-11 10:42:07,999] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:07,999] INFO Setting topicIdPartition xU7mZ680QFyu5_6-r_PJ_A:_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:07,999] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,000] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerEventsStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:08,024] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:08,029] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,030] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:08,031] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,031] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,031] INFO Setting topicIdPartition dUepquCISKuTshdUITPXAA:_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,032] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,032] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:08,055] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:08,060] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,061] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:08,062] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,062] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,062] INFO Setting topicIdPartition XGY26_VTSd2_B2a872KbQg:_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,063] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,064] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:08,089] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:08,093] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,094] INFO Created log for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:08,096] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,096] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,097] INFO Setting topicIdPartition 1vMZcKMYRZ23q2sHYn2txQ:_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,097] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,097] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-AlertHistoryStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:08,133] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:08,138] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,139] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:08,141] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,141] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,141] INFO Setting topicIdPartition YrptvBcIT7O1zFD32BBvNQ:_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,141] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,142] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:08,171] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:08,177] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,179] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:08,180] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,180] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,180] INFO Setting topicIdPartition 2wS7b0qHQBON2GlcqX_Gcg:_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,180] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,181] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:08,206] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:08,210] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,212] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
[2023-08-11 10:42:08,214] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,214] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,214] INFO Setting topicIdPartition j-kKXrSeRJ6Mnb1HybJl-A:_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,215] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,218] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-monitoring-message-rekey-store with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:08,249] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:08,255] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,256] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-11 10:42:08,260] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,260] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,260] INFO Setting topicIdPartition 3-KPtevIRMKUJf8NAwWCHA:_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,260] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,261] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:08,284] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:08,290] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,291] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:08,293] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,293] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,293] INFO Setting topicIdPartition 2VXz-bhiRLeVvbh_1QjUSQ:_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,293] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,294] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:08,325] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:08,341] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,344] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-11 10:42:08,344] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,345] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,345] INFO Setting topicIdPartition ool9GKP8TX6rMfnKAu_qdQ:_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,345] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,346] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:08,371] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-metrics-11, _confluent-metrics-9, _confluent-metrics-10, _confluent-metrics-7, _confluent-metrics-8, _confluent-metrics-5, _confluent-metrics-6, _confluent-metrics-3, _confluent-metrics-4, _confluent-metrics-1, _confluent-metrics-2, _confluent-metrics-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:08,387] INFO [MergedLog partition=_confluent-metrics-11, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,388] INFO Created log for partition _confluent-metrics-11 in /etc/confluent/kraft-combined-logs/_confluent-metrics-11 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
[2023-08-11 10:42:08,389] INFO [Partition _confluent-metrics-11 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-11 (kafka.cluster.Partition)
[2023-08-11 10:42:08,392] INFO [Partition _confluent-metrics-11 broker=1] Log loaded for partition _confluent-metrics-11 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,392] INFO Setting topicIdPartition 007rNpncSVaTDHyLNOd9Mg:_confluent-metrics-11 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,392] INFO [MergedLog partition=_confluent-metrics-11, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,397] INFO [MergedLog partition=_confluent-metrics-9, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,398] INFO Created log for partition _confluent-metrics-9 in /etc/confluent/kraft-combined-logs/_confluent-metrics-9 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
[2023-08-11 10:42:08,400] INFO [Partition _confluent-metrics-9 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-9 (kafka.cluster.Partition)
[2023-08-11 10:42:08,400] INFO [Partition _confluent-metrics-9 broker=1] Log loaded for partition _confluent-metrics-9 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,400] INFO Setting topicIdPartition 007rNpncSVaTDHyLNOd9Mg:_confluent-metrics-9 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,400] INFO [MergedLog partition=_confluent-metrics-9, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,403] INFO [MergedLog partition=_confluent-metrics-10, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,409] INFO Created log for partition _confluent-metrics-10 in /etc/confluent/kraft-combined-logs/_confluent-metrics-10 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
[2023-08-11 10:42:08,420] INFO [Partition _confluent-metrics-10 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-10 (kafka.cluster.Partition)
[2023-08-11 10:42:08,420] INFO [Partition _confluent-metrics-10 broker=1] Log loaded for partition _confluent-metrics-10 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,421] INFO Setting topicIdPartition 007rNpncSVaTDHyLNOd9Mg:_confluent-metrics-10 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,421] INFO [MergedLog partition=_confluent-metrics-10, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,424] INFO [MergedLog partition=_confluent-metrics-7, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,425] INFO Created log for partition _confluent-metrics-7 in /etc/confluent/kraft-combined-logs/_confluent-metrics-7 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
[2023-08-11 10:42:08,427] INFO [Partition _confluent-metrics-7 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-7 (kafka.cluster.Partition)
[2023-08-11 10:42:08,427] INFO [Partition _confluent-metrics-7 broker=1] Log loaded for partition _confluent-metrics-7 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,427] INFO Setting topicIdPartition 007rNpncSVaTDHyLNOd9Mg:_confluent-metrics-7 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,428] INFO [MergedLog partition=_confluent-metrics-7, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,431] INFO [MergedLog partition=_confluent-metrics-8, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,432] INFO Created log for partition _confluent-metrics-8 in /etc/confluent/kraft-combined-logs/_confluent-metrics-8 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
[2023-08-11 10:42:08,432] INFO [Partition _confluent-metrics-8 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-8 (kafka.cluster.Partition)
[2023-08-11 10:42:08,432] INFO [Partition _confluent-metrics-8 broker=1] Log loaded for partition _confluent-metrics-8 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,433] INFO Setting topicIdPartition 007rNpncSVaTDHyLNOd9Mg:_confluent-metrics-8 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,434] INFO [MergedLog partition=_confluent-metrics-8, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,440] INFO [MergedLog partition=_confluent-metrics-5, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,442] INFO Created log for partition _confluent-metrics-5 in /etc/confluent/kraft-combined-logs/_confluent-metrics-5 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
[2023-08-11 10:42:08,442] INFO [Partition _confluent-metrics-5 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-5 (kafka.cluster.Partition)
[2023-08-11 10:42:08,442] INFO [Partition _confluent-metrics-5 broker=1] Log loaded for partition _confluent-metrics-5 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,442] INFO Setting topicIdPartition 007rNpncSVaTDHyLNOd9Mg:_confluent-metrics-5 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,443] INFO [MergedLog partition=_confluent-metrics-5, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,451] INFO [MergedLog partition=_confluent-metrics-6, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,452] INFO Created log for partition _confluent-metrics-6 in /etc/confluent/kraft-combined-logs/_confluent-metrics-6 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
[2023-08-11 10:42:08,452] INFO [Partition _confluent-metrics-6 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-6 (kafka.cluster.Partition)
[2023-08-11 10:42:08,452] INFO [Partition _confluent-metrics-6 broker=1] Log loaded for partition _confluent-metrics-6 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,452] INFO Setting topicIdPartition 007rNpncSVaTDHyLNOd9Mg:_confluent-metrics-6 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,467] INFO [MergedLog partition=_confluent-metrics-6, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,471] INFO [MergedLog partition=_confluent-metrics-3, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,472] INFO Created log for partition _confluent-metrics-3 in /etc/confluent/kraft-combined-logs/_confluent-metrics-3 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
[2023-08-11 10:42:08,474] INFO [Partition _confluent-metrics-3 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-3 (kafka.cluster.Partition)
[2023-08-11 10:42:08,474] INFO [Partition _confluent-metrics-3 broker=1] Log loaded for partition _confluent-metrics-3 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,474] INFO Setting topicIdPartition 007rNpncSVaTDHyLNOd9Mg:_confluent-metrics-3 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,474] INFO [MergedLog partition=_confluent-metrics-3, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,477] INFO [MergedLog partition=_confluent-metrics-4, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,478] INFO Created log for partition _confluent-metrics-4 in /etc/confluent/kraft-combined-logs/_confluent-metrics-4 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
[2023-08-11 10:42:08,478] INFO [Partition _confluent-metrics-4 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-4 (kafka.cluster.Partition)
[2023-08-11 10:42:08,479] INFO [Partition _confluent-metrics-4 broker=1] Log loaded for partition _confluent-metrics-4 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,480] INFO Setting topicIdPartition 007rNpncSVaTDHyLNOd9Mg:_confluent-metrics-4 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,480] INFO [MergedLog partition=_confluent-metrics-4, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,483] INFO [MergedLog partition=_confluent-metrics-1, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,487] INFO Created log for partition _confluent-metrics-1 in /etc/confluent/kraft-combined-logs/_confluent-metrics-1 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
[2023-08-11 10:42:08,487] INFO [Partition _confluent-metrics-1 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-1 (kafka.cluster.Partition)
[2023-08-11 10:42:08,487] INFO [Partition _confluent-metrics-1 broker=1] Log loaded for partition _confluent-metrics-1 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,487] INFO Setting topicIdPartition 007rNpncSVaTDHyLNOd9Mg:_confluent-metrics-1 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,488] INFO [MergedLog partition=_confluent-metrics-1, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,492] INFO [MergedLog partition=_confluent-metrics-2, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,492] INFO Created log for partition _confluent-metrics-2 in /etc/confluent/kraft-combined-logs/_confluent-metrics-2 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
[2023-08-11 10:42:08,493] INFO [Partition _confluent-metrics-2 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-2 (kafka.cluster.Partition)
[2023-08-11 10:42:08,493] INFO [Partition _confluent-metrics-2 broker=1] Log loaded for partition _confluent-metrics-2 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,493] INFO Setting topicIdPartition 007rNpncSVaTDHyLNOd9Mg:_confluent-metrics-2 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,494] INFO [MergedLog partition=_confluent-metrics-2, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,510] INFO [MergedLog partition=_confluent-metrics-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,511] INFO Created log for partition _confluent-metrics-0 in /etc/confluent/kraft-combined-logs/_confluent-metrics-0 with properties {cleanup.policy=delete, max.message.bytes=10485760, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
[2023-08-11 10:42:08,511] INFO [Partition _confluent-metrics-0 broker=1] No checkpointed highwatermark is found for partition _confluent-metrics-0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,511] INFO [Partition _confluent-metrics-0 broker=1] Log loaded for partition _confluent-metrics-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,511] INFO Setting topicIdPartition 007rNpncSVaTDHyLNOd9Mg:_confluent-metrics-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,512] INFO [MergedLog partition=_confluent-metrics-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-metrics-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,512] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-metrics with new configuration : cleanup.policy -> delete,max.message.bytes -> 10485760,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,retention.ms -> 259200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:08,514] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:08,524] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,525] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:08,529] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,529] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,529] INFO Setting topicIdPartition K_duKxwPTuS_S6oKDnuv6A:_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,529] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,530] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:08,532] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-cluster-rekey-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:08,537] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-cluster-rekey-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,539] INFO Created log for partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-cluster-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
[2023-08-11 10:42:08,540] INFO [Partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,540] INFO [Partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-cluster-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,540] INFO Setting topicIdPartition SGGI84n7RmmJHgIn07mZoA:_confluent-controlcenter-7-4-1-1-cluster-rekey-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,540] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-cluster-rekey-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-cluster-rekey-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,541] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-cluster-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:08,545] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:08,551] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,552] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:08,553] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,554] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,554] INFO Setting topicIdPartition MSkROhNRTZKmq4Y3dI9Dug:_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,554] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,555] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:08,557] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:08,561] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,562] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:08,563] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,563] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,563] INFO Setting topicIdPartition tk3R9AJLS0itLXKU6VuxPw:_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,563] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,564] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTERTHIS-0000000105-store-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:08,580] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:08,588] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,589] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
[2023-08-11 10:42:08,589] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,589] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,590] INFO Setting topicIdPartition zY4jLUzqTMC1WDV6c4LLfA:_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,590] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,591] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-repartition with new configuration : cleanup.policy -> delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,retention.ms -> 604800000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:08,616] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:08,621] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,622] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:08,623] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,623] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,624] INFO Setting topicIdPartition 6pn9VlXVRMKqzm38fktQCA:_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,624] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,625] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringTriggerStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:08,652] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:08,658] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,660] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:08,661] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,661] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,661] INFO Setting topicIdPartition gF9OxYHlRyevDbWTN6WWmQ:_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,662] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,662] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:08,692] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:08,699] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,699] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:08,701] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,701] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,701] INFO Setting topicIdPartition a5otQTD3RD6-8DeSXcTdaA:_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,701] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,702] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:08,721] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:08,725] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,726] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:08,726] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,727] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,727] INFO Setting topicIdPartition 9y3MHy1aTFKKy3L-SIrDXQ:_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,728] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,732] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:08,759] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:08,763] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,764] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
[2023-08-11 10:42:08,767] INFO [Partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,767] INFO [Partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,767] INFO Setting topicIdPartition YPv1UBv3TvCpczrZN1geGw:_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,767] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,768] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-stream-extension-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:08,795] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:08,807] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,807] INFO Created log for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
[2023-08-11 10:42:08,808] INFO [Partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,809] INFO [Partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,809] INFO Setting topicIdPartition K6uvig7QQEu9K-HewcN4zQ:_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,809] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,810] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-expected-group-consumption-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:08,828] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:08,832] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,833] INFO Created log for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
[2023-08-11 10:42:08,834] INFO [Partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,834] INFO [Partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,834] INFO Setting topicIdPartition MsX7MhJBRICfuyChlC7i7w:_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,835] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,835] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-metrics-trigger-measurement-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:08,862] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:08,868] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,869] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-11 10:42:08,870] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,870] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,870] INFO Setting topicIdPartition gMxDvQQzSnKcaDqOL59UGQ:_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,871] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,871] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:08,896] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:08,900] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,900] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:08,901] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,902] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,902] INFO Setting topicIdPartition eSETxjekTLGrcwV2JU2BKg:_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,902] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,903] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerEventsStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:08,929] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:08,934] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,935] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:08,937] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,937] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,937] INFO Setting topicIdPartition FCjDGvtyRweR8gueSiUBIA:_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,937] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,938] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringStream-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:08,970] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:08,974] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:08,975] INFO Created log for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:08,977] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,977] INFO [Partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:08,977] INFO Setting topicIdPartition pauUl88tR8-xLYSd24hnKw:_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:08,977] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:08,978] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-TriggerActionsStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:09,005] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:09,014] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:09,015] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:09,015] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,015] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,015] INFO Setting topicIdPartition 9pnb-0P0RGebm7axJfQXiw:_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:09,016] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:09,017] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:09,041] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:09,045] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:09,045] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:09,046] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,047] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,047] INFO Setting topicIdPartition MODe_hf6STqHOHweehIihA:_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:09,048] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:09,048] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:09,073] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-ksql-ksqldb-01_command_topic-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:09,086] INFO [MergedLog partition=_confluent-ksql-ksqldb-01_command_topic-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:09,087] INFO Created log for partition _confluent-ksql-ksqldb-01_command_topic-0 in /etc/confluent/kraft-combined-logs/_confluent-ksql-ksqldb-01_command_topic-0 with properties {cleanup.policy=delete, min.insync.replicas=1, retention.ms=-1, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:09,087] INFO [Partition _confluent-ksql-ksqldb-01_command_topic-0 broker=1] No checkpointed highwatermark is found for partition _confluent-ksql-ksqldb-01_command_topic-0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,090] INFO [Partition _confluent-ksql-ksqldb-01_command_topic-0 broker=1] Log loaded for partition _confluent-ksql-ksqldb-01_command_topic-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,090] INFO Setting topicIdPartition 5CRoaIZkSwy_rtSWFSRDWQ:_confluent-ksql-ksqldb-01_command_topic-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:09,091] INFO [MergedLog partition=_confluent-ksql-ksqldb-01_command_topic-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-ksql-ksqldb-01_command_topic-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:09,091] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-ksql-ksqldb-01_command_topic with new configuration : cleanup.policy -> delete,min.insync.replicas -> 1,retention.ms -> -1,unclean.leader.election.enable -> false (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:09,103] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:09,108] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:09,109] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:09,110] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,110] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,111] INFO Setting topicIdPartition y2jjo1epTCaPL9bItSdUcQ:_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:09,111] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:09,112] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:09,134] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(ksqldb-01ksql_processing_log-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:09,139] INFO [MergedLog partition=ksqldb-01ksql_processing_log-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:09,140] INFO Created log for partition ksqldb-01ksql_processing_log-0 in /etc/confluent/kraft-combined-logs/ksqldb-01ksql_processing_log-0 with properties {} (kafka.log.LogManager)
[2023-08-11 10:42:09,141] INFO [Partition ksqldb-01ksql_processing_log-0 broker=1] No checkpointed highwatermark is found for partition ksqldb-01ksql_processing_log-0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,142] INFO [Partition ksqldb-01ksql_processing_log-0 broker=1] Log loaded for partition ksqldb-01ksql_processing_log-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,142] INFO Setting topicIdPartition ogItNxBrSTayOo119m7eKw:ksqldb-01ksql_processing_log-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:09,142] INFO [MergedLog partition=ksqldb-01ksql_processing_log-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for ksqldb-01ksql_processing_log-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:09,167] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:09,172] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:09,173] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:09,173] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,174] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,175] INFO Setting topicIdPartition N657tlX5RzG1PJaV8zulSw:_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:09,175] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:09,175] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:09,199] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:09,206] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:09,207] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:09,208] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,208] INFO [Partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,208] INFO Setting topicIdPartition 3NWAfxBwS4yu3ifqfxzLjg:_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:09,209] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:09,209] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MetricsAggregateStore-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:09,233] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:09,237] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:09,238] INFO Created log for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
[2023-08-11 10:42:09,239] INFO [Partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,240] INFO [Partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,240] INFO Setting topicIdPartition JcxuirabQfWfbPE4Htm2pg:_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:09,240] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:09,241] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-actual-group-consumption-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:09,266] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:09,270] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:09,271] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-11 10:42:09,271] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,271] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,272] INFO Setting topicIdPartition PUUJsd7PQtu76hmnbMNyBQ:_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:09,272] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:09,272] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:09,297] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:09,302] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:09,302] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=604800000} (kafka.log.LogManager)
[2023-08-11 10:42:09,303] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,303] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,304] INFO Setting topicIdPartition q4yyoarHQUS4xdb6dSCpiA:_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:09,304] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:09,304] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-monitoring-trigger-event-rekey with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 604800000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:09,328] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:09,333] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:09,334] INFO Created log for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:09,334] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,335] INFO [Partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,335] INFO Setting topicIdPartition e8vg2bXsTC6YAwMPTh16yQ:_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:09,335] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:09,336] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-KSTREAM-OUTEROTHER-0000000106-store-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:09,362] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:09,367] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:09,367] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:09,368] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,370] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,371] INFO Setting topicIdPartition 6PSjIs8lT7eqYbuge-QrtQ:_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:09,371] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:09,372] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:09,397] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:09,403] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:09,404] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-11 10:42:09,404] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,405] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,406] INFO Setting topicIdPartition BZmd6N1UQPmESPhGqsz4IA:_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:09,406] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:09,406] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:09,430] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:09,434] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:09,435] INFO Created log for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-11 10:42:09,435] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,437] INFO [Partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,437] INFO Setting topicIdPartition l8Wd5SHOSxCLRQY7Gsf_wA:_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:09,437] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:09,438] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-monitoring-aggregate-rekey-store-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:09,470] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:09,476] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:09,476] INFO Created log for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:09,477] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,478] INFO [Partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,478] INFO Setting topicIdPartition 5dzBWdLkTIq_Ky1fT4zkTg:_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:09,478] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:09,479] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-Group-THREE_HOURS-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:09,504] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:09,511] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:09,512] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:09,512] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,513] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,514] INFO Setting topicIdPartition A-dzTkyjSc-005NI0hjzvQ:_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:09,514] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:09,517] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:09,540] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:09,547] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:09,548] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-11 10:42:09,550] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,551] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,551] INFO Setting topicIdPartition 884Iq712T62HfeRdtajwJw:_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:09,552] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:09,553] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregate-topic-partition-store-changelog with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:09,574] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-monitoring-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:09,578] INFO [MergedLog partition=_confluent-monitoring-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:09,579] INFO Created log for partition _confluent-monitoring-0 in /etc/confluent/kraft-combined-logs/_confluent-monitoring-0 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000} (kafka.log.LogManager)
[2023-08-11 10:42:09,581] INFO [Partition _confluent-monitoring-0 broker=1] No checkpointed highwatermark is found for partition _confluent-monitoring-0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,581] INFO [Partition _confluent-monitoring-0 broker=1] Log loaded for partition _confluent-monitoring-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,581] INFO Setting topicIdPartition xUHLfQudRSeH0PDs0HK2Gw:_confluent-monitoring-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:09,581] INFO [MergedLog partition=_confluent-monitoring-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-monitoring-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:09,582] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-monitoring with new configuration : cleanup.policy -> delete,message.timestamp.type -> LogAppendTime,min.insync.replicas -> 1,retention.ms -> 259200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:09,616] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:09,628] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:09,634] INFO Created log for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:09,634] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,634] INFO [Partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,634] INFO Setting topicIdPartition QTS69uhpRPu5YroNHugXsw:_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:09,635] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:09,636] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-MonitoringVerifierStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:09,671] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:09,677] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:09,678] INFO Created log for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:09,678] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,678] INFO [Partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,679] INFO Setting topicIdPartition 8gU1leK-QsKCh2TxxxPAuQ:_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:09,679] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:09,681] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition with new configuration : cleanup.policy -> compact,delete,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 60566400000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 60566400000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:09,716] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:09,725] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:09,726] INFO Created log for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728} (kafka.log.LogManager)
[2023-08-11 10:42:09,727] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,729] INFO [Partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,730] INFO Setting topicIdPartition 9T8787AnROqLLsZR4wGFnQ:_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:09,731] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:09,732] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-AlertHistoryStore-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 134217728,retention.ms -> 691200000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 691200000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:09,752] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:09,756] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:09,760] INFO Created log for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 in /etc/confluent/kraft-combined-logs/_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864} (kafka.log.LogManager)
[2023-08-11 10:42:09,761] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 broker=1] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,761] INFO [Partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 broker=1] Log loaded for partition _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,761] INFO Setting topicIdPartition xAhXWTQxSCGx7Lad4TpCBQ:_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:09,762] INFO [MergedLog partition=_confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:09,762] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-controlcenter-7-4-1-1-group-aggregate-store-ONE_MINUTE-repartition with new configuration : cleanup.policy -> compact,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,segment.bytes -> 67108864,retention.ms -> 432000000,message.timestamp.difference.max.ms -> 9223372036854775807,retention.bytes -> -1,delete.retention.ms -> 432000000 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:09,900] INFO Sent auto-creation request for Set(__transaction_state) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
[2023-08-11 10:42:09,936] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__transaction_state-42, __transaction_state-13, __transaction_state-46, __transaction_state-17, __transaction_state-34, __transaction_state-5, __transaction_state-38, __transaction_state-9, __transaction_state-26, __transaction_state-30, __transaction_state-1, __transaction_state-18, __transaction_state-22, __transaction_state-12, __transaction_state-45, __transaction_state-16, __transaction_state-49, __transaction_state-4, __transaction_state-37, __transaction_state-8, __transaction_state-41, __transaction_state-29, __transaction_state-0, __transaction_state-33, __transaction_state-21, __transaction_state-25, __transaction_state-11, __transaction_state-44, __transaction_state-15, __transaction_state-48, __transaction_state-3, __transaction_state-36, __transaction_state-7, __transaction_state-40, __transaction_state-28, __transaction_state-32, __transaction_state-20, __transaction_state-24, __transaction_state-10, __transaction_state-43, __transaction_state-14, __transaction_state-47, __transaction_state-2, __transaction_state-35, __transaction_state-6, __transaction_state-39, __transaction_state-27, __transaction_state-31, __transaction_state-19, __transaction_state-23) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:09,982] INFO [MergedLog partition=__transaction_state-42, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:09,982] INFO Created log for partition __transaction_state-42 in /etc/confluent/kraft-combined-logs/__transaction_state-42 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:09,983] INFO [Partition __transaction_state-42 broker=1] No checkpointed highwatermark is found for partition __transaction_state-42 (kafka.cluster.Partition)
[2023-08-11 10:42:09,984] INFO [Partition __transaction_state-42 broker=1] Log loaded for partition __transaction_state-42 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,984] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-42 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:09,984] INFO [MergedLog partition=__transaction_state-42, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-42 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:09,987] INFO [MergedLog partition=__transaction_state-13, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:09,989] INFO Created log for partition __transaction_state-13 in /etc/confluent/kraft-combined-logs/__transaction_state-13 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:09,989] INFO [Partition __transaction_state-13 broker=1] No checkpointed highwatermark is found for partition __transaction_state-13 (kafka.cluster.Partition)
[2023-08-11 10:42:09,989] INFO [Partition __transaction_state-13 broker=1] Log loaded for partition __transaction_state-13 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,989] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-13 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:09,990] INFO [MergedLog partition=__transaction_state-13, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-13 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:09,993] INFO [MergedLog partition=__transaction_state-46, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:09,993] INFO Created log for partition __transaction_state-46 in /etc/confluent/kraft-combined-logs/__transaction_state-46 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:09,995] INFO [Partition __transaction_state-46 broker=1] No checkpointed highwatermark is found for partition __transaction_state-46 (kafka.cluster.Partition)
[2023-08-11 10:42:09,995] INFO [Partition __transaction_state-46 broker=1] Log loaded for partition __transaction_state-46 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:09,995] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-46 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:09,995] INFO [MergedLog partition=__transaction_state-46, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-46 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:09,998] INFO [MergedLog partition=__transaction_state-17, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:09,999] INFO Created log for partition __transaction_state-17 in /etc/confluent/kraft-combined-logs/__transaction_state-17 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:09,999] INFO [Partition __transaction_state-17 broker=1] No checkpointed highwatermark is found for partition __transaction_state-17 (kafka.cluster.Partition)
[2023-08-11 10:42:09,999] INFO [Partition __transaction_state-17 broker=1] Log loaded for partition __transaction_state-17 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,000] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-17 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,000] INFO [MergedLog partition=__transaction_state-17, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-17 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,003] INFO [MergedLog partition=__transaction_state-34, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,003] INFO Created log for partition __transaction_state-34 in /etc/confluent/kraft-combined-logs/__transaction_state-34 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,004] INFO [Partition __transaction_state-34 broker=1] No checkpointed highwatermark is found for partition __transaction_state-34 (kafka.cluster.Partition)
[2023-08-11 10:42:10,004] INFO [Partition __transaction_state-34 broker=1] Log loaded for partition __transaction_state-34 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,004] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-34 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,004] INFO [MergedLog partition=__transaction_state-34, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-34 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,009] INFO [MergedLog partition=__transaction_state-5, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,009] INFO Created log for partition __transaction_state-5 in /etc/confluent/kraft-combined-logs/__transaction_state-5 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,009] INFO [Partition __transaction_state-5 broker=1] No checkpointed highwatermark is found for partition __transaction_state-5 (kafka.cluster.Partition)
[2023-08-11 10:42:10,009] INFO [Partition __transaction_state-5 broker=1] Log loaded for partition __transaction_state-5 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,010] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-5 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,011] INFO [MergedLog partition=__transaction_state-5, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,014] INFO [MergedLog partition=__transaction_state-38, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,015] INFO Created log for partition __transaction_state-38 in /etc/confluent/kraft-combined-logs/__transaction_state-38 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,015] INFO [Partition __transaction_state-38 broker=1] No checkpointed highwatermark is found for partition __transaction_state-38 (kafka.cluster.Partition)
[2023-08-11 10:42:10,015] INFO [Partition __transaction_state-38 broker=1] Log loaded for partition __transaction_state-38 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,015] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-38 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,016] INFO [MergedLog partition=__transaction_state-38, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-38 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,018] INFO [MergedLog partition=__transaction_state-9, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,019] INFO Created log for partition __transaction_state-9 in /etc/confluent/kraft-combined-logs/__transaction_state-9 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,019] INFO [Partition __transaction_state-9 broker=1] No checkpointed highwatermark is found for partition __transaction_state-9 (kafka.cluster.Partition)
[2023-08-11 10:42:10,019] INFO [Partition __transaction_state-9 broker=1] Log loaded for partition __transaction_state-9 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,019] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-9 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,019] INFO [MergedLog partition=__transaction_state-9, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,022] INFO [MergedLog partition=__transaction_state-26, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,022] INFO Created log for partition __transaction_state-26 in /etc/confluent/kraft-combined-logs/__transaction_state-26 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,022] INFO [Partition __transaction_state-26 broker=1] No checkpointed highwatermark is found for partition __transaction_state-26 (kafka.cluster.Partition)
[2023-08-11 10:42:10,022] INFO [Partition __transaction_state-26 broker=1] Log loaded for partition __transaction_state-26 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,022] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-26 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,022] INFO [MergedLog partition=__transaction_state-26, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-26 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,025] INFO [MergedLog partition=__transaction_state-30, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,026] INFO Created log for partition __transaction_state-30 in /etc/confluent/kraft-combined-logs/__transaction_state-30 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,026] INFO [Partition __transaction_state-30 broker=1] No checkpointed highwatermark is found for partition __transaction_state-30 (kafka.cluster.Partition)
[2023-08-11 10:42:10,026] INFO [Partition __transaction_state-30 broker=1] Log loaded for partition __transaction_state-30 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,026] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-30 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,026] INFO [MergedLog partition=__transaction_state-30, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-30 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,028] INFO [MergedLog partition=__transaction_state-1, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,029] INFO Created log for partition __transaction_state-1 in /etc/confluent/kraft-combined-logs/__transaction_state-1 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,029] INFO [Partition __transaction_state-1 broker=1] No checkpointed highwatermark is found for partition __transaction_state-1 (kafka.cluster.Partition)
[2023-08-11 10:42:10,029] INFO [Partition __transaction_state-1 broker=1] Log loaded for partition __transaction_state-1 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,029] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-1 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,029] INFO [MergedLog partition=__transaction_state-1, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,032] INFO [MergedLog partition=__transaction_state-18, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,032] INFO Created log for partition __transaction_state-18 in /etc/confluent/kraft-combined-logs/__transaction_state-18 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,032] INFO [Partition __transaction_state-18 broker=1] No checkpointed highwatermark is found for partition __transaction_state-18 (kafka.cluster.Partition)
[2023-08-11 10:42:10,032] INFO [Partition __transaction_state-18 broker=1] Log loaded for partition __transaction_state-18 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,032] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-18 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,033] INFO [MergedLog partition=__transaction_state-18, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-18 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,036] INFO [MergedLog partition=__transaction_state-22, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,036] INFO Created log for partition __transaction_state-22 in /etc/confluent/kraft-combined-logs/__transaction_state-22 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,037] INFO [Partition __transaction_state-22 broker=1] No checkpointed highwatermark is found for partition __transaction_state-22 (kafka.cluster.Partition)
[2023-08-11 10:42:10,037] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1-command in Empty state. Created a new member id _confluent-controlcenter-7-4-1-1-command-36af446a-ed64-431e-9928-2d17544bc31e-StreamThread-1-consumer-540880c6-8d86-4a85-b46a-16aa56398832 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:10,038] INFO [Partition __transaction_state-22 broker=1] Log loaded for partition __transaction_state-22 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,038] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-22 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,038] INFO [MergedLog partition=__transaction_state-22, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-22 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,040] INFO [MergedLog partition=__transaction_state-12, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,041] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-7-4-1-1-command in state PreparingRebalance with old generation 0 (__consumer_offsets-23) (reason: Adding new member _confluent-controlcenter-7-4-1-1-command-36af446a-ed64-431e-9928-2d17544bc31e-StreamThread-1-consumer-540880c6-8d86-4a85-b46a-16aa56398832 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:10,041] INFO Created log for partition __transaction_state-12 in /etc/confluent/kraft-combined-logs/__transaction_state-12 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,041] INFO [Partition __transaction_state-12 broker=1] No checkpointed highwatermark is found for partition __transaction_state-12 (kafka.cluster.Partition)
[2023-08-11 10:42:10,042] INFO [Partition __transaction_state-12 broker=1] Log loaded for partition __transaction_state-12 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,042] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-12 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,042] INFO [MergedLog partition=__transaction_state-12, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-12 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,044] INFO [MergedLog partition=__transaction_state-45, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,045] INFO Created log for partition __transaction_state-45 in /etc/confluent/kraft-combined-logs/__transaction_state-45 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,045] INFO [Partition __transaction_state-45 broker=1] No checkpointed highwatermark is found for partition __transaction_state-45 (kafka.cluster.Partition)
[2023-08-11 10:42:10,045] INFO [Partition __transaction_state-45 broker=1] Log loaded for partition __transaction_state-45 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,045] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-45 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,046] INFO [MergedLog partition=__transaction_state-45, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-45 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,047] INFO [GroupCoordinator 1]: Stabilized group _confluent-controlcenter-7-4-1-1-command generation 1 (__consumer_offsets-23) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:10,048] INFO [MergedLog partition=__transaction_state-16, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,049] INFO Created log for partition __transaction_state-16 in /etc/confluent/kraft-combined-logs/__transaction_state-16 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,049] INFO [Partition __transaction_state-16 broker=1] No checkpointed highwatermark is found for partition __transaction_state-16 (kafka.cluster.Partition)
[2023-08-11 10:42:10,049] INFO [Partition __transaction_state-16 broker=1] Log loaded for partition __transaction_state-16 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,049] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-16 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,049] INFO [MergedLog partition=__transaction_state-16, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-16 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,052] INFO [MergedLog partition=__transaction_state-49, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,053] INFO Created log for partition __transaction_state-49 in /etc/confluent/kraft-combined-logs/__transaction_state-49 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,053] INFO [Partition __transaction_state-49 broker=1] No checkpointed highwatermark is found for partition __transaction_state-49 (kafka.cluster.Partition)
[2023-08-11 10:42:10,053] INFO [Partition __transaction_state-49 broker=1] Log loaded for partition __transaction_state-49 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,053] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-49 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,053] INFO [MergedLog partition=__transaction_state-49, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-49 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,056] INFO [MergedLog partition=__transaction_state-4, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,057] INFO Created log for partition __transaction_state-4 in /etc/confluent/kraft-combined-logs/__transaction_state-4 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,057] INFO [Partition __transaction_state-4 broker=1] No checkpointed highwatermark is found for partition __transaction_state-4 (kafka.cluster.Partition)
[2023-08-11 10:42:10,057] INFO [Partition __transaction_state-4 broker=1] Log loaded for partition __transaction_state-4 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,057] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-4 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,058] INFO [MergedLog partition=__transaction_state-4, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,060] INFO [MergedLog partition=__transaction_state-37, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,060] INFO Created log for partition __transaction_state-37 in /etc/confluent/kraft-combined-logs/__transaction_state-37 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,060] INFO [Partition __transaction_state-37 broker=1] No checkpointed highwatermark is found for partition __transaction_state-37 (kafka.cluster.Partition)
[2023-08-11 10:42:10,060] INFO [Partition __transaction_state-37 broker=1] Log loaded for partition __transaction_state-37 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,061] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-37 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,061] INFO [MergedLog partition=__transaction_state-37, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-37 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,064] INFO [MergedLog partition=__transaction_state-8, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,064] INFO Created log for partition __transaction_state-8 in /etc/confluent/kraft-combined-logs/__transaction_state-8 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,064] INFO [Partition __transaction_state-8 broker=1] No checkpointed highwatermark is found for partition __transaction_state-8 (kafka.cluster.Partition)
[2023-08-11 10:42:10,064] INFO [Partition __transaction_state-8 broker=1] Log loaded for partition __transaction_state-8 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,065] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-8 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,065] INFO [MergedLog partition=__transaction_state-8, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,067] INFO [MergedLog partition=__transaction_state-41, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,068] INFO Created log for partition __transaction_state-41 in /etc/confluent/kraft-combined-logs/__transaction_state-41 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,068] INFO [Partition __transaction_state-41 broker=1] No checkpointed highwatermark is found for partition __transaction_state-41 (kafka.cluster.Partition)
[2023-08-11 10:42:10,068] INFO [Partition __transaction_state-41 broker=1] Log loaded for partition __transaction_state-41 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,068] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-41 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,068] INFO [MergedLog partition=__transaction_state-41, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-41 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,071] INFO [MergedLog partition=__transaction_state-29, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,071] INFO Created log for partition __transaction_state-29 in /etc/confluent/kraft-combined-logs/__transaction_state-29 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,071] INFO [Partition __transaction_state-29 broker=1] No checkpointed highwatermark is found for partition __transaction_state-29 (kafka.cluster.Partition)
[2023-08-11 10:42:10,071] INFO [Partition __transaction_state-29 broker=1] Log loaded for partition __transaction_state-29 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,071] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-29 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,072] INFO [MergedLog partition=__transaction_state-29, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-29 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,074] INFO [MergedLog partition=__transaction_state-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,074] INFO Created log for partition __transaction_state-0 in /etc/confluent/kraft-combined-logs/__transaction_state-0 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,074] INFO [Partition __transaction_state-0 broker=1] No checkpointed highwatermark is found for partition __transaction_state-0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,074] INFO [Partition __transaction_state-0 broker=1] Log loaded for partition __transaction_state-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,074] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,075] INFO [MergedLog partition=__transaction_state-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,077] INFO [MergedLog partition=__transaction_state-33, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,077] INFO Created log for partition __transaction_state-33 in /etc/confluent/kraft-combined-logs/__transaction_state-33 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,077] INFO [Partition __transaction_state-33 broker=1] No checkpointed highwatermark is found for partition __transaction_state-33 (kafka.cluster.Partition)
[2023-08-11 10:42:10,077] INFO [Partition __transaction_state-33 broker=1] Log loaded for partition __transaction_state-33 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,077] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-33 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,078] INFO [MergedLog partition=__transaction_state-33, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-33 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,080] INFO [MergedLog partition=__transaction_state-21, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,080] INFO Created log for partition __transaction_state-21 in /etc/confluent/kraft-combined-logs/__transaction_state-21 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,080] INFO [Partition __transaction_state-21 broker=1] No checkpointed highwatermark is found for partition __transaction_state-21 (kafka.cluster.Partition)
[2023-08-11 10:42:10,081] INFO [Partition __transaction_state-21 broker=1] Log loaded for partition __transaction_state-21 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,081] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-21 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,081] INFO [MergedLog partition=__transaction_state-21, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-21 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,082] INFO [GroupCoordinator 1]: Assignment received from leader _confluent-controlcenter-7-4-1-1-command-36af446a-ed64-431e-9928-2d17544bc31e-StreamThread-1-consumer-540880c6-8d86-4a85-b46a-16aa56398832 for group _confluent-controlcenter-7-4-1-1-command for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:10,085] INFO [MergedLog partition=__transaction_state-25, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,085] INFO Created log for partition __transaction_state-25 in /etc/confluent/kraft-combined-logs/__transaction_state-25 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,085] INFO [Partition __transaction_state-25 broker=1] No checkpointed highwatermark is found for partition __transaction_state-25 (kafka.cluster.Partition)
[2023-08-11 10:42:10,085] INFO [Partition __transaction_state-25 broker=1] Log loaded for partition __transaction_state-25 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,085] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-25 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,085] INFO [MergedLog partition=__transaction_state-25, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-25 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,088] INFO [MergedLog partition=__transaction_state-11, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,088] INFO Created log for partition __transaction_state-11 in /etc/confluent/kraft-combined-logs/__transaction_state-11 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,089] INFO [Partition __transaction_state-11 broker=1] No checkpointed highwatermark is found for partition __transaction_state-11 (kafka.cluster.Partition)
[2023-08-11 10:42:10,089] INFO [Partition __transaction_state-11 broker=1] Log loaded for partition __transaction_state-11 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,089] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-11 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,089] INFO [MergedLog partition=__transaction_state-11, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,091] INFO [MergedLog partition=__transaction_state-44, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,092] INFO Created log for partition __transaction_state-44 in /etc/confluent/kraft-combined-logs/__transaction_state-44 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,092] INFO [Partition __transaction_state-44 broker=1] No checkpointed highwatermark is found for partition __transaction_state-44 (kafka.cluster.Partition)
[2023-08-11 10:42:10,092] INFO [Partition __transaction_state-44 broker=1] Log loaded for partition __transaction_state-44 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,092] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-44 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,092] INFO [MergedLog partition=__transaction_state-44, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-44 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,095] INFO [MergedLog partition=__transaction_state-15, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,096] INFO Created log for partition __transaction_state-15 in /etc/confluent/kraft-combined-logs/__transaction_state-15 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,097] INFO [Partition __transaction_state-15 broker=1] No checkpointed highwatermark is found for partition __transaction_state-15 (kafka.cluster.Partition)
[2023-08-11 10:42:10,097] INFO [Partition __transaction_state-15 broker=1] Log loaded for partition __transaction_state-15 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,097] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-15 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,097] INFO [MergedLog partition=__transaction_state-15, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-15 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,099] INFO [MergedLog partition=__transaction_state-48, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,100] INFO Created log for partition __transaction_state-48 in /etc/confluent/kraft-combined-logs/__transaction_state-48 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,100] INFO [Partition __transaction_state-48 broker=1] No checkpointed highwatermark is found for partition __transaction_state-48 (kafka.cluster.Partition)
[2023-08-11 10:42:10,100] INFO [Partition __transaction_state-48 broker=1] Log loaded for partition __transaction_state-48 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,101] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-48 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,101] INFO [MergedLog partition=__transaction_state-48, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-48 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,103] INFO [MergedLog partition=__transaction_state-3, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,104] INFO Created log for partition __transaction_state-3 in /etc/confluent/kraft-combined-logs/__transaction_state-3 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,104] INFO [Partition __transaction_state-3 broker=1] No checkpointed highwatermark is found for partition __transaction_state-3 (kafka.cluster.Partition)
[2023-08-11 10:42:10,104] INFO [Partition __transaction_state-3 broker=1] Log loaded for partition __transaction_state-3 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,104] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-3 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,104] INFO [MergedLog partition=__transaction_state-3, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,107] INFO [MergedLog partition=__transaction_state-36, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,107] INFO Created log for partition __transaction_state-36 in /etc/confluent/kraft-combined-logs/__transaction_state-36 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,107] INFO [Partition __transaction_state-36 broker=1] No checkpointed highwatermark is found for partition __transaction_state-36 (kafka.cluster.Partition)
[2023-08-11 10:42:10,108] INFO [Partition __transaction_state-36 broker=1] Log loaded for partition __transaction_state-36 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,108] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-36 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,108] INFO [MergedLog partition=__transaction_state-36, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-36 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,110] INFO [MergedLog partition=__transaction_state-7, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,111] INFO Created log for partition __transaction_state-7 in /etc/confluent/kraft-combined-logs/__transaction_state-7 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,111] INFO [Partition __transaction_state-7 broker=1] No checkpointed highwatermark is found for partition __transaction_state-7 (kafka.cluster.Partition)
[2023-08-11 10:42:10,111] INFO [Partition __transaction_state-7 broker=1] Log loaded for partition __transaction_state-7 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,111] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-7 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,111] INFO [MergedLog partition=__transaction_state-7, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,115] INFO [MergedLog partition=__transaction_state-40, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,115] INFO Created log for partition __transaction_state-40 in /etc/confluent/kraft-combined-logs/__transaction_state-40 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,116] INFO [Partition __transaction_state-40 broker=1] No checkpointed highwatermark is found for partition __transaction_state-40 (kafka.cluster.Partition)
[2023-08-11 10:42:10,116] INFO [Partition __transaction_state-40 broker=1] Log loaded for partition __transaction_state-40 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,116] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-40 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,117] INFO [MergedLog partition=__transaction_state-40, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-40 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,119] INFO [MergedLog partition=__transaction_state-28, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,120] INFO Created log for partition __transaction_state-28 in /etc/confluent/kraft-combined-logs/__transaction_state-28 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,120] INFO [Partition __transaction_state-28 broker=1] No checkpointed highwatermark is found for partition __transaction_state-28 (kafka.cluster.Partition)
[2023-08-11 10:42:10,120] INFO [Partition __transaction_state-28 broker=1] Log loaded for partition __transaction_state-28 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,120] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-28 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,120] INFO [MergedLog partition=__transaction_state-28, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-28 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,123] INFO [MergedLog partition=__transaction_state-32, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,124] INFO Created log for partition __transaction_state-32 in /etc/confluent/kraft-combined-logs/__transaction_state-32 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,124] INFO [Partition __transaction_state-32 broker=1] No checkpointed highwatermark is found for partition __transaction_state-32 (kafka.cluster.Partition)
[2023-08-11 10:42:10,124] INFO [Partition __transaction_state-32 broker=1] Log loaded for partition __transaction_state-32 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,124] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-32 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,124] INFO [MergedLog partition=__transaction_state-32, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-32 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,127] INFO [MergedLog partition=__transaction_state-20, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,127] INFO Created log for partition __transaction_state-20 in /etc/confluent/kraft-combined-logs/__transaction_state-20 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,127] INFO [Partition __transaction_state-20 broker=1] No checkpointed highwatermark is found for partition __transaction_state-20 (kafka.cluster.Partition)
[2023-08-11 10:42:10,128] INFO [Partition __transaction_state-20 broker=1] Log loaded for partition __transaction_state-20 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,128] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-20 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,128] INFO [MergedLog partition=__transaction_state-20, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-20 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,131] INFO [MergedLog partition=__transaction_state-24, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,131] INFO Created log for partition __transaction_state-24 in /etc/confluent/kraft-combined-logs/__transaction_state-24 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,131] INFO [Partition __transaction_state-24 broker=1] No checkpointed highwatermark is found for partition __transaction_state-24 (kafka.cluster.Partition)
[2023-08-11 10:42:10,132] INFO [Partition __transaction_state-24 broker=1] Log loaded for partition __transaction_state-24 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,132] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-24 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,132] INFO [MergedLog partition=__transaction_state-24, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-24 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,135] INFO [MergedLog partition=__transaction_state-10, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,135] INFO Created log for partition __transaction_state-10 in /etc/confluent/kraft-combined-logs/__transaction_state-10 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,135] INFO [Partition __transaction_state-10 broker=1] No checkpointed highwatermark is found for partition __transaction_state-10 (kafka.cluster.Partition)
[2023-08-11 10:42:10,136] INFO [Partition __transaction_state-10 broker=1] Log loaded for partition __transaction_state-10 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,137] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-10 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,137] INFO [MergedLog partition=__transaction_state-10, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,139] INFO [MergedLog partition=__transaction_state-43, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,139] INFO Created log for partition __transaction_state-43 in /etc/confluent/kraft-combined-logs/__transaction_state-43 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,140] INFO [Partition __transaction_state-43 broker=1] No checkpointed highwatermark is found for partition __transaction_state-43 (kafka.cluster.Partition)
[2023-08-11 10:42:10,140] INFO [Partition __transaction_state-43 broker=1] Log loaded for partition __transaction_state-43 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,140] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-43 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,140] INFO [MergedLog partition=__transaction_state-43, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-43 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,143] INFO [MergedLog partition=__transaction_state-14, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,144] INFO Created log for partition __transaction_state-14 in /etc/confluent/kraft-combined-logs/__transaction_state-14 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,144] INFO [Partition __transaction_state-14 broker=1] No checkpointed highwatermark is found for partition __transaction_state-14 (kafka.cluster.Partition)
[2023-08-11 10:42:10,144] INFO [Partition __transaction_state-14 broker=1] Log loaded for partition __transaction_state-14 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,144] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-14 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,144] INFO [MergedLog partition=__transaction_state-14, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-14 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,147] INFO [MergedLog partition=__transaction_state-47, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,148] INFO Created log for partition __transaction_state-47 in /etc/confluent/kraft-combined-logs/__transaction_state-47 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,148] INFO [Partition __transaction_state-47 broker=1] No checkpointed highwatermark is found for partition __transaction_state-47 (kafka.cluster.Partition)
[2023-08-11 10:42:10,148] INFO [Partition __transaction_state-47 broker=1] Log loaded for partition __transaction_state-47 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,148] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-47 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,148] INFO [MergedLog partition=__transaction_state-47, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-47 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,151] INFO [MergedLog partition=__transaction_state-2, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,151] INFO Created log for partition __transaction_state-2 in /etc/confluent/kraft-combined-logs/__transaction_state-2 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,151] INFO [Partition __transaction_state-2 broker=1] No checkpointed highwatermark is found for partition __transaction_state-2 (kafka.cluster.Partition)
[2023-08-11 10:42:10,151] INFO [Partition __transaction_state-2 broker=1] Log loaded for partition __transaction_state-2 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,152] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-2 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,152] INFO [MergedLog partition=__transaction_state-2, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,155] INFO [MergedLog partition=__transaction_state-35, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,156] INFO Created log for partition __transaction_state-35 in /etc/confluent/kraft-combined-logs/__transaction_state-35 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,156] INFO [Partition __transaction_state-35 broker=1] No checkpointed highwatermark is found for partition __transaction_state-35 (kafka.cluster.Partition)
[2023-08-11 10:42:10,157] INFO [Partition __transaction_state-35 broker=1] Log loaded for partition __transaction_state-35 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,158] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-35 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,158] INFO [MergedLog partition=__transaction_state-35, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-35 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,160] INFO [MergedLog partition=__transaction_state-6, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,161] INFO Created log for partition __transaction_state-6 in /etc/confluent/kraft-combined-logs/__transaction_state-6 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,161] INFO [Partition __transaction_state-6 broker=1] No checkpointed highwatermark is found for partition __transaction_state-6 (kafka.cluster.Partition)
[2023-08-11 10:42:10,161] INFO [Partition __transaction_state-6 broker=1] Log loaded for partition __transaction_state-6 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,161] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-6 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,162] INFO [MergedLog partition=__transaction_state-6, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,165] INFO [MergedLog partition=__transaction_state-39, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,165] INFO Created log for partition __transaction_state-39 in /etc/confluent/kraft-combined-logs/__transaction_state-39 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,165] INFO [Partition __transaction_state-39 broker=1] No checkpointed highwatermark is found for partition __transaction_state-39 (kafka.cluster.Partition)
[2023-08-11 10:42:10,165] INFO [Partition __transaction_state-39 broker=1] Log loaded for partition __transaction_state-39 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,165] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-39 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,166] INFO [MergedLog partition=__transaction_state-39, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-39 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,168] INFO [MergedLog partition=__transaction_state-27, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,169] INFO Created log for partition __transaction_state-27 in /etc/confluent/kraft-combined-logs/__transaction_state-27 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,169] INFO [Partition __transaction_state-27 broker=1] No checkpointed highwatermark is found for partition __transaction_state-27 (kafka.cluster.Partition)
[2023-08-11 10:42:10,169] INFO [Partition __transaction_state-27 broker=1] Log loaded for partition __transaction_state-27 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,169] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-27 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,171] INFO [MergedLog partition=__transaction_state-27, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-27 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,173] INFO [MergedLog partition=__transaction_state-31, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,174] INFO Created log for partition __transaction_state-31 in /etc/confluent/kraft-combined-logs/__transaction_state-31 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,174] INFO [Partition __transaction_state-31 broker=1] No checkpointed highwatermark is found for partition __transaction_state-31 (kafka.cluster.Partition)
[2023-08-11 10:42:10,174] INFO [Partition __transaction_state-31 broker=1] Log loaded for partition __transaction_state-31 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,174] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-31 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,174] INFO [MergedLog partition=__transaction_state-31, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-31 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,177] INFO [MergedLog partition=__transaction_state-19, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,178] INFO Created log for partition __transaction_state-19 in /etc/confluent/kraft-combined-logs/__transaction_state-19 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,178] INFO [Partition __transaction_state-19 broker=1] No checkpointed highwatermark is found for partition __transaction_state-19 (kafka.cluster.Partition)
[2023-08-11 10:42:10,178] INFO [Partition __transaction_state-19 broker=1] Log loaded for partition __transaction_state-19 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,178] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-19 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,179] INFO [MergedLog partition=__transaction_state-19, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-19 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,181] INFO [MergedLog partition=__transaction_state-23, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:10,182] INFO Created log for partition __transaction_state-23 in /etc/confluent/kraft-combined-logs/__transaction_state-23 with properties {cleanup.policy=compact, compression.type="uncompressed", confluent.placement.constraints="", min.insync.replicas=1, segment.bytes=104857600, unclean.leader.election.enable=false} (kafka.log.LogManager)
[2023-08-11 10:42:10,182] INFO [Partition __transaction_state-23 broker=1] No checkpointed highwatermark is found for partition __transaction_state-23 (kafka.cluster.Partition)
[2023-08-11 10:42:10,182] INFO [Partition __transaction_state-23 broker=1] Log loaded for partition __transaction_state-23 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:10,182] INFO Setting topicIdPartition 77HVS_69TM2gIASYgs--8w:__transaction_state-23 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:10,183] INFO [MergedLog partition=__transaction_state-23, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for __transaction_state-23 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:10,184] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 42 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,191] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 13 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 46 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 17 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 34 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 5 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 38 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 9 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 26 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 30 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 1 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 18 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 22 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 12 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 45 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 16 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-42 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 49 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 4 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 37 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 8 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 41 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 29 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 0 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 33 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 21 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 25 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 11 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 44 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 15 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 48 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 3 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 36 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 7 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,192] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 40 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,193] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 28 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,193] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 32 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,193] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 20 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,193] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 24 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,193] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 10 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,193] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 43 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,193] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 14 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,193] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 47 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,193] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 2 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,193] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 35 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,193] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 6 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,193] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 39 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,193] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 27 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,193] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 31 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,193] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-42 in 2 milliseconds, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,193] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 19 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,193] INFO [TransactionCoordinator id=1] Elected as the txn coordinator for partition 23 at epoch 0 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:10,193] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic __transaction_state with new configuration : compression.type -> uncompressed,cleanup.policy -> compact,min.insync.replicas -> 1,segment.bytes -> 104857600,confluent.placement.constraints -> ,unclean.leader.election.enable -> false (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:10,197] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-42 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,197] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-13 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,197] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-13 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,197] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-13 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,197] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-46 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,197] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-46 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,197] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-46 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,197] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-17 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,197] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-17 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,197] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-17 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,197] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-34 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,197] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-34 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,197] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-34 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,197] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-5 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,197] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-5 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,197] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-5 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,197] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-38 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,197] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-38 in 5 milliseconds, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,197] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-38 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-9 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-9 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-9 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-26 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-26 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-26 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-30 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-30 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-30 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-1 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-1 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-1 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-18 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-18 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-18 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-22 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-22 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-22 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-12 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-12 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-12 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-45 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-45 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-45 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-16 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-16 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-16 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-49 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-49 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-49 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-4 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,198] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-4 in 6 milliseconds, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-4 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-37 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-37 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-37 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-8 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-8 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-8 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-41 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-41 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-41 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-29 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-29 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-29 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-0 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-0 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-0 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-33 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-33 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-33 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-21 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-21 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-21 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-25 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-25 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-25 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-11 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,199] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-11 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,200] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-11 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,200] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-44 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,200] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-44 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,200] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-44 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,200] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-15 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,200] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-15 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,200] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-15 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,200] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-48 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,200] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-48 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,200] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-48 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,200] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-3 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,200] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-3 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,200] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-3 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,200] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-36 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,200] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-36 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,200] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-36 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,200] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-7 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,200] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-7 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,200] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-7 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,200] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-40 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,200] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-40 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,200] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-40 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,200] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-28 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,200] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-28 in 7 milliseconds, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,200] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-28 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,200] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-32 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,201] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-32 in 8 milliseconds, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,201] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-32 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,201] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-20 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,201] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-20 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,201] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-20 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,201] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-24 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,201] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-24 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,201] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-24 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,201] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-10 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,201] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-10 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,201] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-10 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,201] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-43 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,201] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-43 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,201] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-43 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,201] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-14 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,201] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-14 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,201] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-14 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,201] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-47 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,201] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-47 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,201] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-47 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,201] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-2 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,201] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-2 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,201] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-2 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,201] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-35 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,201] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-35 in 8 milliseconds, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,201] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-35 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,202] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-6 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,202] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-6 in 9 milliseconds, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,202] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-6 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,202] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-39 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,202] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-39 in 9 milliseconds, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,202] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-39 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,202] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-27 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,202] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-27 in 9 milliseconds, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,202] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-27 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,202] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-31 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,202] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-31 in 9 milliseconds, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,202] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-31 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,202] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-19 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,202] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-19 in 9 milliseconds, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,202] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-19 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,202] INFO [Transaction State Manager 1]: Loading transaction metadata from __transaction_state-23 at epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,202] INFO [Transaction State Manager 1]: Finished loading 0 transaction metadata from __transaction_state-23 in 9 milliseconds, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,202] INFO [Transaction State Manager 1]: Completed loading transaction metadata from __transaction_state-23 for coordinator epoch 0 (kafka.coordinator.transaction.TransactionStateManager)
[2023-08-11 10:42:10,238] INFO [TransactionCoordinator id=1] Initialized transactionalId ksqldb-01 with producerId 3 and producer epoch 0 on partition __transaction_state-1 (kafka.coordinator.transaction.TransactionCoordinator)
[2023-08-11 10:42:12,299] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in Empty state. Created a new member id _confluent-controlcenter-7-4-1-1-14be7b5c-9263-4110-8ce4-d5a82b56e18d-StreamThread-7-consumer-70ace708-c718-4c47-9d5e-58d1b811685b and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:12,303] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-7-4-1-1 in state PreparingRebalance with old generation 0 (__consumer_offsets-5) (reason: Adding new member _confluent-controlcenter-7-4-1-1-14be7b5c-9263-4110-8ce4-d5a82b56e18d-StreamThread-7-consumer-70ace708-c718-4c47-9d5e-58d1b811685b with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:12,304] INFO [GroupCoordinator 1]: Stabilized group _confluent-controlcenter-7-4-1-1 generation 1 (__consumer_offsets-5) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:12,315] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in CompletingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-14be7b5c-9263-4110-8ce4-d5a82b56e18d-StreamThread-2-consumer-7696ca93-aefe-4358-9c0a-4c88361d15e0 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:12,321] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in CompletingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-14be7b5c-9263-4110-8ce4-d5a82b56e18d-StreamThread-5-consumer-bd2e2aff-410a-4c9e-8f14-56c9fcb341bb and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:12,326] INFO [GroupCoordinator 1]: Preparing to rebalance group _confluent-controlcenter-7-4-1-1 in state PreparingRebalance with old generation 1 (__consumer_offsets-5) (reason: Adding new member _confluent-controlcenter-7-4-1-1-14be7b5c-9263-4110-8ce4-d5a82b56e18d-StreamThread-5-consumer-bd2e2aff-410a-4c9e-8f14-56c9fcb341bb with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:12,327] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-14be7b5c-9263-4110-8ce4-d5a82b56e18d-StreamThread-4-consumer-93a7397d-5c5f-4ec5-91b4-c9a41592eb6c and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:12,328] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-14be7b5c-9263-4110-8ce4-d5a82b56e18d-StreamThread-8-consumer-d881f857-def6-4f19-85d8-64a8cd1b7ddf and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:12,329] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-14be7b5c-9263-4110-8ce4-d5a82b56e18d-StreamThread-11-consumer-af6a8d54-4cac-4ae2-96e3-724e9a760a09 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:12,331] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-14be7b5c-9263-4110-8ce4-d5a82b56e18d-StreamThread-1-consumer-fa9655c0-5ec2-417e-8696-291e1d9f164b and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:12,331] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-14be7b5c-9263-4110-8ce4-d5a82b56e18d-StreamThread-10-consumer-94616d4b-f6c5-48e4-b29f-b0620621036e and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:12,342] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-14be7b5c-9263-4110-8ce4-d5a82b56e18d-StreamThread-16-consumer-c1592650-1b32-428a-ad53-b5fe5bfbfc50 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:12,342] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-14be7b5c-9263-4110-8ce4-d5a82b56e18d-StreamThread-12-consumer-d8c37768-2355-48db-864c-f108676c48d5 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:12,343] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-14be7b5c-9263-4110-8ce4-d5a82b56e18d-StreamThread-14-consumer-b02af97f-1141-4811-9b4e-e5aeeacf8bc2 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:12,343] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-14be7b5c-9263-4110-8ce4-d5a82b56e18d-StreamThread-13-consumer-6c7430ad-a170-42d1-82d1-22149d6e568c and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:12,344] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-14be7b5c-9263-4110-8ce4-d5a82b56e18d-StreamThread-6-consumer-2c77b0f3-bdbd-4d30-94c1-61901fc685f8 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:12,351] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-14be7b5c-9263-4110-8ce4-d5a82b56e18d-StreamThread-3-consumer-d530f569-6394-4d67-8afd-5b8a070bd5e3 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:12,353] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-14be7b5c-9263-4110-8ce4-d5a82b56e18d-StreamThread-15-consumer-c206dba4-a134-4e2b-999d-49208af7a507 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:12,353] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-4-1-1 in PreparingRebalance state. Created a new member id _confluent-controlcenter-7-4-1-1-14be7b5c-9263-4110-8ce4-d5a82b56e18d-StreamThread-9-consumer-273f15f9-ae18-41f9-96a6-8164fd605abd and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:12,358] INFO [GroupCoordinator 1]: Stabilized group _confluent-controlcenter-7-4-1-1 generation 2 (__consumer_offsets-5) with 16 members (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:12,384] INFO [GroupCoordinator 1]: Assignment received from leader _confluent-controlcenter-7-4-1-1-14be7b5c-9263-4110-8ce4-d5a82b56e18d-StreamThread-7-consumer-70ace708-c718-4c47-9d5e-58d1b811685b for group _confluent-controlcenter-7-4-1-1 for generation 2. The group has 16 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:24,452] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(telemetry-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:24,456] INFO [MergedLog partition=telemetry-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:24,456] INFO Created log for partition telemetry-0 in /etc/confluent/kraft-combined-logs/telemetry-0 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-11 10:42:24,458] INFO [Partition telemetry-0 broker=1] No checkpointed highwatermark is found for partition telemetry-0 (kafka.cluster.Partition)
[2023-08-11 10:42:24,460] INFO [Partition telemetry-0 broker=1] Log loaded for partition telemetry-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:24,464] INFO Setting topicIdPartition oIiO4W4sRV6E0OKkCX3psw:telemetry-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:24,464] INFO [MergedLog partition=telemetry-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for telemetry-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:24,464] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic telemetry with new configuration : max.message.bytes -> 10485760,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,retention.ms -> 259200000,segment.ms -> 14400000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:24,750] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-telemetry-metrics-3, _confluent-telemetry-metrics-4, _confluent-telemetry-metrics-5, _confluent-telemetry-metrics-6, _confluent-telemetry-metrics-7, _confluent-telemetry-metrics-8, _confluent-telemetry-metrics-9, _confluent-telemetry-metrics-10, _confluent-telemetry-metrics-11, _confluent-telemetry-metrics-0, _confluent-telemetry-metrics-1, _confluent-telemetry-metrics-2) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:24,804] INFO [MergedLog partition=_confluent-telemetry-metrics-3, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:24,806] INFO Created log for partition _confluent-telemetry-metrics-3 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-3 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-11 10:42:24,807] INFO [Partition _confluent-telemetry-metrics-3 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-3 (kafka.cluster.Partition)
[2023-08-11 10:42:24,807] INFO [Partition _confluent-telemetry-metrics-3 broker=1] Log loaded for partition _confluent-telemetry-metrics-3 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:24,808] INFO Setting topicIdPartition pOivpVk8S9yMe4qcDbzGKw:_confluent-telemetry-metrics-3 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:24,808] INFO [MergedLog partition=_confluent-telemetry-metrics-3, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:24,812] INFO [MergedLog partition=_confluent-telemetry-metrics-4, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:24,813] INFO Created log for partition _confluent-telemetry-metrics-4 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-4 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-11 10:42:24,813] INFO [Partition _confluent-telemetry-metrics-4 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-4 (kafka.cluster.Partition)
[2023-08-11 10:42:24,813] INFO [Partition _confluent-telemetry-metrics-4 broker=1] Log loaded for partition _confluent-telemetry-metrics-4 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:24,814] INFO Setting topicIdPartition pOivpVk8S9yMe4qcDbzGKw:_confluent-telemetry-metrics-4 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:24,815] INFO [MergedLog partition=_confluent-telemetry-metrics-4, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:24,817] INFO [MergedLog partition=_confluent-telemetry-metrics-5, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:24,823] INFO Created log for partition _confluent-telemetry-metrics-5 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-5 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-11 10:42:24,824] INFO [Partition _confluent-telemetry-metrics-5 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-5 (kafka.cluster.Partition)
[2023-08-11 10:42:24,824] INFO [Partition _confluent-telemetry-metrics-5 broker=1] Log loaded for partition _confluent-telemetry-metrics-5 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:24,824] INFO Setting topicIdPartition pOivpVk8S9yMe4qcDbzGKw:_confluent-telemetry-metrics-5 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:24,825] INFO [MergedLog partition=_confluent-telemetry-metrics-5, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:24,827] INFO [MergedLog partition=_confluent-telemetry-metrics-6, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:24,831] INFO Created log for partition _confluent-telemetry-metrics-6 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-6 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-11 10:42:24,832] INFO [Partition _confluent-telemetry-metrics-6 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-6 (kafka.cluster.Partition)
[2023-08-11 10:42:24,832] INFO [Partition _confluent-telemetry-metrics-6 broker=1] Log loaded for partition _confluent-telemetry-metrics-6 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:24,832] INFO Setting topicIdPartition pOivpVk8S9yMe4qcDbzGKw:_confluent-telemetry-metrics-6 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:24,835] INFO [MergedLog partition=_confluent-telemetry-metrics-6, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:24,837] INFO [MergedLog partition=_confluent-telemetry-metrics-7, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:24,838] INFO Created log for partition _confluent-telemetry-metrics-7 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-7 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-11 10:42:24,838] INFO [Partition _confluent-telemetry-metrics-7 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-7 (kafka.cluster.Partition)
[2023-08-11 10:42:24,843] INFO [Partition _confluent-telemetry-metrics-7 broker=1] Log loaded for partition _confluent-telemetry-metrics-7 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:24,843] INFO Setting topicIdPartition pOivpVk8S9yMe4qcDbzGKw:_confluent-telemetry-metrics-7 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:24,843] INFO [MergedLog partition=_confluent-telemetry-metrics-7, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:24,846] INFO [MergedLog partition=_confluent-telemetry-metrics-8, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:24,847] INFO Created log for partition _confluent-telemetry-metrics-8 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-8 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-11 10:42:24,847] INFO [Partition _confluent-telemetry-metrics-8 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-8 (kafka.cluster.Partition)
[2023-08-11 10:42:24,848] INFO [Partition _confluent-telemetry-metrics-8 broker=1] Log loaded for partition _confluent-telemetry-metrics-8 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:24,848] INFO Setting topicIdPartition pOivpVk8S9yMe4qcDbzGKw:_confluent-telemetry-metrics-8 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:24,848] INFO [MergedLog partition=_confluent-telemetry-metrics-8, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:24,851] INFO [MergedLog partition=_confluent-telemetry-metrics-9, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:24,855] INFO Created log for partition _confluent-telemetry-metrics-9 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-9 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-11 10:42:24,856] INFO [Partition _confluent-telemetry-metrics-9 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-9 (kafka.cluster.Partition)
[2023-08-11 10:42:24,856] INFO [Partition _confluent-telemetry-metrics-9 broker=1] Log loaded for partition _confluent-telemetry-metrics-9 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:24,856] INFO Setting topicIdPartition pOivpVk8S9yMe4qcDbzGKw:_confluent-telemetry-metrics-9 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:24,892] INFO [MergedLog partition=_confluent-telemetry-metrics-9, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:24,896] INFO [MergedLog partition=_confluent-telemetry-metrics-10, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:24,904] INFO Created log for partition _confluent-telemetry-metrics-10 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-10 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-11 10:42:24,905] INFO [Partition _confluent-telemetry-metrics-10 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-10 (kafka.cluster.Partition)
[2023-08-11 10:42:24,905] INFO [Partition _confluent-telemetry-metrics-10 broker=1] Log loaded for partition _confluent-telemetry-metrics-10 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:24,905] INFO Setting topicIdPartition pOivpVk8S9yMe4qcDbzGKw:_confluent-telemetry-metrics-10 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:24,905] INFO [MergedLog partition=_confluent-telemetry-metrics-10, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:24,910] INFO [MergedLog partition=_confluent-telemetry-metrics-11, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:24,910] INFO Created log for partition _confluent-telemetry-metrics-11 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-11 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-11 10:42:24,911] INFO [Partition _confluent-telemetry-metrics-11 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-11 (kafka.cluster.Partition)
[2023-08-11 10:42:24,911] INFO [Partition _confluent-telemetry-metrics-11 broker=1] Log loaded for partition _confluent-telemetry-metrics-11 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:24,911] INFO Setting topicIdPartition pOivpVk8S9yMe4qcDbzGKw:_confluent-telemetry-metrics-11 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:24,911] INFO [MergedLog partition=_confluent-telemetry-metrics-11, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:24,935] INFO [MergedLog partition=_confluent-telemetry-metrics-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:24,936] INFO Created log for partition _confluent-telemetry-metrics-0 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-0 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-11 10:42:24,951] INFO [Partition _confluent-telemetry-metrics-0 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-0 (kafka.cluster.Partition)
[2023-08-11 10:42:24,951] INFO [Partition _confluent-telemetry-metrics-0 broker=1] Log loaded for partition _confluent-telemetry-metrics-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:24,951] INFO Setting topicIdPartition pOivpVk8S9yMe4qcDbzGKw:_confluent-telemetry-metrics-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:24,951] INFO [MergedLog partition=_confluent-telemetry-metrics-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:24,954] INFO [MergedLog partition=_confluent-telemetry-metrics-1, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:24,955] INFO Created log for partition _confluent-telemetry-metrics-1 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-1 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-11 10:42:24,955] INFO [Partition _confluent-telemetry-metrics-1 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-1 (kafka.cluster.Partition)
[2023-08-11 10:42:24,955] INFO [Partition _confluent-telemetry-metrics-1 broker=1] Log loaded for partition _confluent-telemetry-metrics-1 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:24,956] INFO Setting topicIdPartition pOivpVk8S9yMe4qcDbzGKw:_confluent-telemetry-metrics-1 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:24,958] INFO [MergedLog partition=_confluent-telemetry-metrics-1, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:24,961] INFO [MergedLog partition=_confluent-telemetry-metrics-2, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:24,961] INFO Created log for partition _confluent-telemetry-metrics-2 in /etc/confluent/kraft-combined-logs/_confluent-telemetry-metrics-2 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
[2023-08-11 10:42:24,965] INFO [Partition _confluent-telemetry-metrics-2 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-2 (kafka.cluster.Partition)
[2023-08-11 10:42:24,965] INFO [Partition _confluent-telemetry-metrics-2 broker=1] Log loaded for partition _confluent-telemetry-metrics-2 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:24,965] INFO Setting topicIdPartition pOivpVk8S9yMe4qcDbzGKw:_confluent-telemetry-metrics-2 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:24,966] INFO [MergedLog partition=_confluent-telemetry-metrics-2, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:24,966] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent-telemetry-metrics with new configuration : max.message.bytes -> 10485760,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,retention.ms -> 259200000,segment.ms -> 14400000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:31,533] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent_balancer_api_state-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:31,549] INFO [MergedLog partition=_confluent_balancer_api_state-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:31,550] INFO Created log for partition _confluent_balancer_api_state-0 in /etc/confluent/kraft-combined-logs/_confluent_balancer_api_state-0 with properties {cleanup.policy=compact, retention.ms=-1} (kafka.log.LogManager)
[2023-08-11 10:42:31,552] INFO [Partition _confluent_balancer_api_state-0 broker=1] No checkpointed highwatermark is found for partition _confluent_balancer_api_state-0 (kafka.cluster.Partition)
[2023-08-11 10:42:31,552] INFO [Partition _confluent_balancer_api_state-0 broker=1] Log loaded for partition _confluent_balancer_api_state-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:31,552] INFO Setting topicIdPartition uVVkBoe2S8Ktx5BeEe0Tyw:_confluent_balancer_api_state-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:31,552] INFO [MergedLog partition=_confluent_balancer_api_state-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for _confluent_balancer_api_state-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:31,553] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic _confluent_balancer_api_state with new configuration : cleanup.policy -> compact,retention.ms -> -1 (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:32,914] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group ConfluentTelemetryReporterSampler-4143341910604163517 in Empty state. Created a new member id kafka-cruise-control-a1126faa-8eab-42eb-a737-329d00abcaa7 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:32,916] INFO [GroupCoordinator 1]: Preparing to rebalance group ConfluentTelemetryReporterSampler-4143341910604163517 in state PreparingRebalance with old generation 0 (__consumer_offsets-32) (reason: Adding new member kafka-cruise-control-a1126faa-8eab-42eb-a737-329d00abcaa7 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:32,917] INFO [GroupCoordinator 1]: Stabilized group ConfluentTelemetryReporterSampler-4143341910604163517 generation 1 (__consumer_offsets-32) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:32,924] INFO [GroupCoordinator 1]: Assignment received from leader kafka-cruise-control-a1126faa-8eab-42eb-a737-329d00abcaa7 for group ConfluentTelemetryReporterSampler-4143341910604163517 for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:55,512] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(connect-offsets-10, connect-offsets-8, connect-offsets-14, connect-offsets-12, connect-offsets-2, connect-offsets-0, connect-offsets-6, connect-offsets-4, connect-offsets-24, connect-offsets-18, connect-offsets-16, connect-offsets-22, connect-offsets-20, connect-offsets-9, connect-offsets-7, connect-offsets-13, connect-offsets-11, connect-offsets-1, connect-offsets-5, connect-offsets-3, connect-offsets-23, connect-offsets-17, connect-offsets-15, connect-offsets-21, connect-offsets-19) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:55,528] INFO [MergedLog partition=connect-offsets-10, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,529] INFO Created log for partition connect-offsets-10 in /etc/confluent/kraft-combined-logs/connect-offsets-10 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,529] INFO [Partition connect-offsets-10 broker=1] No checkpointed highwatermark is found for partition connect-offsets-10 (kafka.cluster.Partition)
[2023-08-11 10:42:55,530] INFO [Partition connect-offsets-10 broker=1] Log loaded for partition connect-offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,530] INFO Setting topicIdPartition 07PxZl1ARv2f013KJ54i7g:connect-offsets-10 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,530] INFO [MergedLog partition=connect-offsets-10, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,532] INFO [MergedLog partition=connect-offsets-8, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,533] INFO Created log for partition connect-offsets-8 in /etc/confluent/kraft-combined-logs/connect-offsets-8 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,533] INFO [Partition connect-offsets-8 broker=1] No checkpointed highwatermark is found for partition connect-offsets-8 (kafka.cluster.Partition)
[2023-08-11 10:42:55,533] INFO [Partition connect-offsets-8 broker=1] Log loaded for partition connect-offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,533] INFO Setting topicIdPartition 07PxZl1ARv2f013KJ54i7g:connect-offsets-8 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,533] INFO [MergedLog partition=connect-offsets-8, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,536] INFO [MergedLog partition=connect-offsets-14, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,537] INFO Created log for partition connect-offsets-14 in /etc/confluent/kraft-combined-logs/connect-offsets-14 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,538] INFO [Partition connect-offsets-14 broker=1] No checkpointed highwatermark is found for partition connect-offsets-14 (kafka.cluster.Partition)
[2023-08-11 10:42:55,538] INFO [Partition connect-offsets-14 broker=1] Log loaded for partition connect-offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,539] INFO Setting topicIdPartition 07PxZl1ARv2f013KJ54i7g:connect-offsets-14 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,539] INFO [MergedLog partition=connect-offsets-14, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-14 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,540] INFO [MergedLog partition=connect-offsets-12, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,541] INFO Created log for partition connect-offsets-12 in /etc/confluent/kraft-combined-logs/connect-offsets-12 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,541] INFO [Partition connect-offsets-12 broker=1] No checkpointed highwatermark is found for partition connect-offsets-12 (kafka.cluster.Partition)
[2023-08-11 10:42:55,541] INFO [Partition connect-offsets-12 broker=1] Log loaded for partition connect-offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,541] INFO Setting topicIdPartition 07PxZl1ARv2f013KJ54i7g:connect-offsets-12 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,541] INFO [MergedLog partition=connect-offsets-12, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-12 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,544] INFO [MergedLog partition=connect-offsets-2, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,544] INFO Created log for partition connect-offsets-2 in /etc/confluent/kraft-combined-logs/connect-offsets-2 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,544] INFO [Partition connect-offsets-2 broker=1] No checkpointed highwatermark is found for partition connect-offsets-2 (kafka.cluster.Partition)
[2023-08-11 10:42:55,544] INFO [Partition connect-offsets-2 broker=1] Log loaded for partition connect-offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,544] INFO Setting topicIdPartition 07PxZl1ARv2f013KJ54i7g:connect-offsets-2 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,544] INFO [MergedLog partition=connect-offsets-2, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,546] INFO [MergedLog partition=connect-offsets-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,547] INFO Created log for partition connect-offsets-0 in /etc/confluent/kraft-combined-logs/connect-offsets-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,547] INFO [Partition connect-offsets-0 broker=1] No checkpointed highwatermark is found for partition connect-offsets-0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,547] INFO [Partition connect-offsets-0 broker=1] Log loaded for partition connect-offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,547] INFO Setting topicIdPartition 07PxZl1ARv2f013KJ54i7g:connect-offsets-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,547] INFO [MergedLog partition=connect-offsets-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,549] INFO [MergedLog partition=connect-offsets-6, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,550] INFO Created log for partition connect-offsets-6 in /etc/confluent/kraft-combined-logs/connect-offsets-6 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,550] INFO [Partition connect-offsets-6 broker=1] No checkpointed highwatermark is found for partition connect-offsets-6 (kafka.cluster.Partition)
[2023-08-11 10:42:55,550] INFO [Partition connect-offsets-6 broker=1] Log loaded for partition connect-offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,550] INFO Setting topicIdPartition 07PxZl1ARv2f013KJ54i7g:connect-offsets-6 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,550] INFO [MergedLog partition=connect-offsets-6, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,552] INFO [MergedLog partition=connect-offsets-4, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,553] INFO Created log for partition connect-offsets-4 in /etc/confluent/kraft-combined-logs/connect-offsets-4 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,553] INFO [Partition connect-offsets-4 broker=1] No checkpointed highwatermark is found for partition connect-offsets-4 (kafka.cluster.Partition)
[2023-08-11 10:42:55,553] INFO [Partition connect-offsets-4 broker=1] Log loaded for partition connect-offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,553] INFO Setting topicIdPartition 07PxZl1ARv2f013KJ54i7g:connect-offsets-4 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,553] INFO [MergedLog partition=connect-offsets-4, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,555] INFO [MergedLog partition=connect-offsets-24, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,555] INFO Created log for partition connect-offsets-24 in /etc/confluent/kraft-combined-logs/connect-offsets-24 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,555] INFO [Partition connect-offsets-24 broker=1] No checkpointed highwatermark is found for partition connect-offsets-24 (kafka.cluster.Partition)
[2023-08-11 10:42:55,556] INFO [Partition connect-offsets-24 broker=1] Log loaded for partition connect-offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,556] INFO Setting topicIdPartition 07PxZl1ARv2f013KJ54i7g:connect-offsets-24 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,556] INFO [MergedLog partition=connect-offsets-24, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-24 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,558] INFO [MergedLog partition=connect-offsets-18, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,558] INFO Created log for partition connect-offsets-18 in /etc/confluent/kraft-combined-logs/connect-offsets-18 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,558] INFO [Partition connect-offsets-18 broker=1] No checkpointed highwatermark is found for partition connect-offsets-18 (kafka.cluster.Partition)
[2023-08-11 10:42:55,558] INFO [Partition connect-offsets-18 broker=1] Log loaded for partition connect-offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,559] INFO Setting topicIdPartition 07PxZl1ARv2f013KJ54i7g:connect-offsets-18 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,559] INFO [MergedLog partition=connect-offsets-18, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-18 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,561] INFO [MergedLog partition=connect-offsets-16, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,562] INFO Created log for partition connect-offsets-16 in /etc/confluent/kraft-combined-logs/connect-offsets-16 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,562] INFO [Partition connect-offsets-16 broker=1] No checkpointed highwatermark is found for partition connect-offsets-16 (kafka.cluster.Partition)
[2023-08-11 10:42:55,562] INFO [Partition connect-offsets-16 broker=1] Log loaded for partition connect-offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,563] INFO Setting topicIdPartition 07PxZl1ARv2f013KJ54i7g:connect-offsets-16 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,563] INFO [MergedLog partition=connect-offsets-16, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-16 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,565] INFO [MergedLog partition=connect-offsets-22, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,565] INFO Created log for partition connect-offsets-22 in /etc/confluent/kraft-combined-logs/connect-offsets-22 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,565] INFO [Partition connect-offsets-22 broker=1] No checkpointed highwatermark is found for partition connect-offsets-22 (kafka.cluster.Partition)
[2023-08-11 10:42:55,565] INFO [Partition connect-offsets-22 broker=1] Log loaded for partition connect-offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,566] INFO Setting topicIdPartition 07PxZl1ARv2f013KJ54i7g:connect-offsets-22 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,566] INFO [MergedLog partition=connect-offsets-22, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-22 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,568] INFO [MergedLog partition=connect-offsets-20, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,569] INFO Created log for partition connect-offsets-20 in /etc/confluent/kraft-combined-logs/connect-offsets-20 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,569] INFO [Partition connect-offsets-20 broker=1] No checkpointed highwatermark is found for partition connect-offsets-20 (kafka.cluster.Partition)
[2023-08-11 10:42:55,569] INFO [Partition connect-offsets-20 broker=1] Log loaded for partition connect-offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,570] INFO Setting topicIdPartition 07PxZl1ARv2f013KJ54i7g:connect-offsets-20 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,570] INFO [MergedLog partition=connect-offsets-20, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-20 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,572] INFO [MergedLog partition=connect-offsets-9, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,573] INFO Created log for partition connect-offsets-9 in /etc/confluent/kraft-combined-logs/connect-offsets-9 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,573] INFO [Partition connect-offsets-9 broker=1] No checkpointed highwatermark is found for partition connect-offsets-9 (kafka.cluster.Partition)
[2023-08-11 10:42:55,574] INFO [Partition connect-offsets-9 broker=1] Log loaded for partition connect-offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,575] INFO Setting topicIdPartition 07PxZl1ARv2f013KJ54i7g:connect-offsets-9 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,575] INFO [MergedLog partition=connect-offsets-9, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,577] INFO [MergedLog partition=connect-offsets-7, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,578] INFO Created log for partition connect-offsets-7 in /etc/confluent/kraft-combined-logs/connect-offsets-7 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,578] INFO [Partition connect-offsets-7 broker=1] No checkpointed highwatermark is found for partition connect-offsets-7 (kafka.cluster.Partition)
[2023-08-11 10:42:55,581] INFO [Partition connect-offsets-7 broker=1] Log loaded for partition connect-offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,581] INFO Setting topicIdPartition 07PxZl1ARv2f013KJ54i7g:connect-offsets-7 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,581] INFO [MergedLog partition=connect-offsets-7, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,584] INFO [MergedLog partition=connect-offsets-13, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,585] INFO Created log for partition connect-offsets-13 in /etc/confluent/kraft-combined-logs/connect-offsets-13 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,585] INFO [Partition connect-offsets-13 broker=1] No checkpointed highwatermark is found for partition connect-offsets-13 (kafka.cluster.Partition)
[2023-08-11 10:42:55,586] INFO [Partition connect-offsets-13 broker=1] Log loaded for partition connect-offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,586] INFO Setting topicIdPartition 07PxZl1ARv2f013KJ54i7g:connect-offsets-13 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,587] INFO [MergedLog partition=connect-offsets-13, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-13 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,589] INFO [MergedLog partition=connect-offsets-11, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,590] INFO Created log for partition connect-offsets-11 in /etc/confluent/kraft-combined-logs/connect-offsets-11 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,590] INFO [Partition connect-offsets-11 broker=1] No checkpointed highwatermark is found for partition connect-offsets-11 (kafka.cluster.Partition)
[2023-08-11 10:42:55,591] INFO [Partition connect-offsets-11 broker=1] Log loaded for partition connect-offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,592] INFO Setting topicIdPartition 07PxZl1ARv2f013KJ54i7g:connect-offsets-11 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,592] INFO [MergedLog partition=connect-offsets-11, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,593] INFO [MergedLog partition=connect-offsets-1, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,594] INFO Created log for partition connect-offsets-1 in /etc/confluent/kraft-combined-logs/connect-offsets-1 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,594] INFO [Partition connect-offsets-1 broker=1] No checkpointed highwatermark is found for partition connect-offsets-1 (kafka.cluster.Partition)
[2023-08-11 10:42:55,594] INFO [Partition connect-offsets-1 broker=1] Log loaded for partition connect-offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,594] INFO Setting topicIdPartition 07PxZl1ARv2f013KJ54i7g:connect-offsets-1 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,594] INFO [MergedLog partition=connect-offsets-1, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,597] INFO [MergedLog partition=connect-offsets-5, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,597] INFO Created log for partition connect-offsets-5 in /etc/confluent/kraft-combined-logs/connect-offsets-5 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,597] INFO [Partition connect-offsets-5 broker=1] No checkpointed highwatermark is found for partition connect-offsets-5 (kafka.cluster.Partition)
[2023-08-11 10:42:55,597] INFO [Partition connect-offsets-5 broker=1] Log loaded for partition connect-offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,597] INFO Setting topicIdPartition 07PxZl1ARv2f013KJ54i7g:connect-offsets-5 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,598] INFO [MergedLog partition=connect-offsets-5, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,600] INFO [MergedLog partition=connect-offsets-3, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,601] INFO Created log for partition connect-offsets-3 in /etc/confluent/kraft-combined-logs/connect-offsets-3 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,601] INFO [Partition connect-offsets-3 broker=1] No checkpointed highwatermark is found for partition connect-offsets-3 (kafka.cluster.Partition)
[2023-08-11 10:42:55,601] INFO [Partition connect-offsets-3 broker=1] Log loaded for partition connect-offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,601] INFO Setting topicIdPartition 07PxZl1ARv2f013KJ54i7g:connect-offsets-3 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,601] INFO [MergedLog partition=connect-offsets-3, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,604] INFO [MergedLog partition=connect-offsets-23, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,604] INFO Created log for partition connect-offsets-23 in /etc/confluent/kraft-combined-logs/connect-offsets-23 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,604] INFO [Partition connect-offsets-23 broker=1] No checkpointed highwatermark is found for partition connect-offsets-23 (kafka.cluster.Partition)
[2023-08-11 10:42:55,604] INFO [Partition connect-offsets-23 broker=1] Log loaded for partition connect-offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,604] INFO Setting topicIdPartition 07PxZl1ARv2f013KJ54i7g:connect-offsets-23 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,604] INFO [MergedLog partition=connect-offsets-23, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-23 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,607] INFO [MergedLog partition=connect-offsets-17, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,607] INFO Created log for partition connect-offsets-17 in /etc/confluent/kraft-combined-logs/connect-offsets-17 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,607] INFO [Partition connect-offsets-17 broker=1] No checkpointed highwatermark is found for partition connect-offsets-17 (kafka.cluster.Partition)
[2023-08-11 10:42:55,608] INFO [Partition connect-offsets-17 broker=1] Log loaded for partition connect-offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,608] INFO Setting topicIdPartition 07PxZl1ARv2f013KJ54i7g:connect-offsets-17 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,608] INFO [MergedLog partition=connect-offsets-17, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-17 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,610] INFO [MergedLog partition=connect-offsets-15, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,610] INFO Created log for partition connect-offsets-15 in /etc/confluent/kraft-combined-logs/connect-offsets-15 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,610] INFO [Partition connect-offsets-15 broker=1] No checkpointed highwatermark is found for partition connect-offsets-15 (kafka.cluster.Partition)
[2023-08-11 10:42:55,610] INFO [Partition connect-offsets-15 broker=1] Log loaded for partition connect-offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,610] INFO Setting topicIdPartition 07PxZl1ARv2f013KJ54i7g:connect-offsets-15 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,611] INFO [MergedLog partition=connect-offsets-15, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-15 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,613] INFO [MergedLog partition=connect-offsets-21, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,613] INFO Created log for partition connect-offsets-21 in /etc/confluent/kraft-combined-logs/connect-offsets-21 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,613] INFO [Partition connect-offsets-21 broker=1] No checkpointed highwatermark is found for partition connect-offsets-21 (kafka.cluster.Partition)
[2023-08-11 10:42:55,613] INFO [Partition connect-offsets-21 broker=1] Log loaded for partition connect-offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,613] INFO Setting topicIdPartition 07PxZl1ARv2f013KJ54i7g:connect-offsets-21 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,613] INFO [MergedLog partition=connect-offsets-21, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-21 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,617] INFO [MergedLog partition=connect-offsets-19, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,617] INFO Created log for partition connect-offsets-19 in /etc/confluent/kraft-combined-logs/connect-offsets-19 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,618] INFO [Partition connect-offsets-19 broker=1] No checkpointed highwatermark is found for partition connect-offsets-19 (kafka.cluster.Partition)
[2023-08-11 10:42:55,618] INFO [Partition connect-offsets-19 broker=1] Log loaded for partition connect-offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,619] INFO Setting topicIdPartition 07PxZl1ARv2f013KJ54i7g:connect-offsets-19 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,619] INFO [MergedLog partition=connect-offsets-19, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-offsets-19 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,619] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic connect-offsets with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:55,735] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(connect-statuses-0, connect-statuses-3, connect-statuses-4, connect-statuses-1, connect-statuses-2) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:55,742] INFO [MergedLog partition=connect-statuses-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,742] INFO Created log for partition connect-statuses-0 in /etc/confluent/kraft-combined-logs/connect-statuses-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,743] INFO [Partition connect-statuses-0 broker=1] No checkpointed highwatermark is found for partition connect-statuses-0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,744] INFO [Partition connect-statuses-0 broker=1] Log loaded for partition connect-statuses-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,744] INFO Setting topicIdPartition 5CmOyuoBQFqQ3cu8mVpROA:connect-statuses-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,744] INFO [MergedLog partition=connect-statuses-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-statuses-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,747] INFO [MergedLog partition=connect-statuses-3, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,749] INFO Created log for partition connect-statuses-3 in /etc/confluent/kraft-combined-logs/connect-statuses-3 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,749] INFO [Partition connect-statuses-3 broker=1] No checkpointed highwatermark is found for partition connect-statuses-3 (kafka.cluster.Partition)
[2023-08-11 10:42:55,749] INFO [Partition connect-statuses-3 broker=1] Log loaded for partition connect-statuses-3 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,749] INFO Setting topicIdPartition 5CmOyuoBQFqQ3cu8mVpROA:connect-statuses-3 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,750] INFO [MergedLog partition=connect-statuses-3, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-statuses-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,752] INFO [MergedLog partition=connect-statuses-4, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,752] INFO Created log for partition connect-statuses-4 in /etc/confluent/kraft-combined-logs/connect-statuses-4 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,752] INFO [Partition connect-statuses-4 broker=1] No checkpointed highwatermark is found for partition connect-statuses-4 (kafka.cluster.Partition)
[2023-08-11 10:42:55,753] INFO [Partition connect-statuses-4 broker=1] Log loaded for partition connect-statuses-4 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,753] INFO Setting topicIdPartition 5CmOyuoBQFqQ3cu8mVpROA:connect-statuses-4 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,753] INFO [MergedLog partition=connect-statuses-4, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-statuses-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,756] INFO [MergedLog partition=connect-statuses-1, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,757] INFO Created log for partition connect-statuses-1 in /etc/confluent/kraft-combined-logs/connect-statuses-1 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,757] INFO [Partition connect-statuses-1 broker=1] No checkpointed highwatermark is found for partition connect-statuses-1 (kafka.cluster.Partition)
[2023-08-11 10:42:55,757] INFO [Partition connect-statuses-1 broker=1] Log loaded for partition connect-statuses-1 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,757] INFO Setting topicIdPartition 5CmOyuoBQFqQ3cu8mVpROA:connect-statuses-1 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,757] INFO [MergedLog partition=connect-statuses-1, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-statuses-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,760] INFO [MergedLog partition=connect-statuses-2, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,761] INFO Created log for partition connect-statuses-2 in /etc/confluent/kraft-combined-logs/connect-statuses-2 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,761] INFO [Partition connect-statuses-2 broker=1] No checkpointed highwatermark is found for partition connect-statuses-2 (kafka.cluster.Partition)
[2023-08-11 10:42:55,761] INFO [Partition connect-statuses-2 broker=1] Log loaded for partition connect-statuses-2 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,761] INFO Setting topicIdPartition 5CmOyuoBQFqQ3cu8mVpROA:connect-statuses-2 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,761] INFO [MergedLog partition=connect-statuses-2, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-statuses-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,763] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic connect-statuses with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:55,805] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(connect-configs-0) (kafka.server.ReplicaFetcherManager)
[2023-08-11 10:42:55,812] INFO [MergedLog partition=connect-configs-0, dir=/etc/confluent/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
[2023-08-11 10:42:55,813] INFO Created log for partition connect-configs-0 in /etc/confluent/kraft-combined-logs/connect-configs-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
[2023-08-11 10:42:55,814] INFO [Partition connect-configs-0 broker=1] No checkpointed highwatermark is found for partition connect-configs-0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,815] INFO [Partition connect-configs-0 broker=1] Log loaded for partition connect-configs-0 with initial high watermark 0 (kafka.cluster.Partition)
[2023-08-11 10:42:55,815] INFO Setting topicIdPartition efdzgKVBRMGB65AYMBTFLw:connect-configs-0 (kafka.tier.state.FileTierPartitionState)
[2023-08-11 10:42:55,815] INFO [MergedLog partition=connect-configs-0, dir=/etc/confluent/kraft-combined-logs] Initializing tier metadata without recovery for connect-configs-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
[2023-08-11 10:42:55,816] INFO [DynamicConfigPublisher nodeType=broker id=1] Updating topic connect-configs with new configuration : cleanup.policy -> compact (kafka.server.metadata.DynamicConfigPublisher)
[2023-08-11 10:42:55,872] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group kafka-connect in Empty state. Created a new member id connect-1-b1556aea-f2c2-43a8-9abe-92d5525ff4bc and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:55,874] INFO [GroupCoordinator 1]: Preparing to rebalance group kafka-connect in state PreparingRebalance with old generation 0 (__consumer_offsets-11) (reason: Adding new member connect-1-b1556aea-f2c2-43a8-9abe-92d5525ff4bc with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:55,875] INFO [GroupCoordinator 1]: Stabilized group kafka-connect generation 1 (__consumer_offsets-11) with 1 members (kafka.coordinator.group.GroupCoordinator)
[2023-08-11 10:42:55,896] INFO [GroupCoordinator 1]: Assignment received from leader connect-1-b1556aea-f2c2-43a8-9abe-92d5525ff4bc for group kafka-connect for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
